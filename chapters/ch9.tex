\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{html}\usepackage{html}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

%\section{Overfitting}

%http://read.pudn.com/downloads157/ebook/699944/Pattern%20Recognition%20and%20Machine%20Learning%20(Solution%20Manual)%20-%20Bishop.pdf

%https://github.com/GoldenCheese/PRML-Solution-Manual/blob/master/Solution%20Manual%20For%20PRML.pdf

%https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-web-sol-2009-09-08.pdf

%\DeclareMathOperator{tr}{tr}
%\DeclareMathOperator{diag}{diag}
%\DeclareMathOperator{\sign}{sign}

%http://cs229.stanford.edu/
%http://cs229.stanford.edu/materials.html
%http://cs229.stanford.edu/syllabus.html
%http://cs229.stanford.edu/notes/
%http://cs229.stanford.edu/notes/cs229-notes1.pdf
%http://cs229.stanford.edu/notes/cs229-notes2.pdf
%http://cs229.stanford.edu/notes/cs229-notes3.pdf
%http://cs229.stanford.edu/notes/cs229-notes4.pdf
%http://cs229.stanford.edu/notes/cs229-notes5.pdf
%http://cs229.stanford.edu/notes/cs229-notes6.pdf

As one of the most important tasks in {\em machine learning}, 
{\em pattern classification} is to classify some objects of interest, 
generically referred to as {\em patterns} and described by a set of $d$ 
{\em features} or {\em attributes} that characterizes the patterns, 
to one of some $K$ {\em classes} or {\em categories}. Each pattern is
represented by a vector (or a point) ${\bf x}=[x_1,\cdots,x_d]^T$ in 
a d-dimensional {\em feature space}, where $x_i\;(i=1,\cdots,d)$ is a 
variable for the measurement of the ith feature. Symbolically, the $K$ 
classes can be denoted $\{C_1,\cdots,C_K\}$, and a pattern ${\bf x}$ 
belonging to the kth class is denoted by ${\bf x}\in C_k$. Pattern 
classification can therefore be considered as the process by which 
the d-dimensional feature space is partitioned into $K$ regions each 
corresponding to one of the $K$ classes. The boundaries between these 
regions, called {\em decision boundaries}, are to be determined by the 
specific algorithm, called a classifier, used for the classification.

Pattern classification can be carried out as either a {\em supervised}
or {\em unsupervised learning} process, depending on the availability
of a {\em training set} containing patterns of known class identities.
Specifically, the training set contains a set of $N$ patterns in 
${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$, labeled respectively by the
corresponding component in ${\bf y}=[ y_1,\cdots,y_N]^T$ representing 
the class identities of the corresponding patterns in some way. For
example, we can use $y_k \in \{1,\cdots,K\}$ to indicate ${\bf x}_k
\in C_{y_k}$. Alternatively, in the special case when $K=2$, there are 
only two classes $C_+$ and $C_-$, and the classifier becomes binary 
based on training pattern ${\bf x}_j\;(j=1,\cdots,N)$, each labeled by 
$y_j=1$ if ${\bf x}_j\in C+$ or $y_j=-1$ if ${\bf x}_j\in C_-$.

We assume there are $N_k$ training samples 
$\{{\bf x}_1^{(k)},\cdots,{\bf x}_{N_k}^{(k)}\}$ all labeled to belong 
to $C_k,\;\;(k=1,\cdots,K)$, and in total $N=\sum_{k=1}^K N_k$ samples in 
the training set. If the training set is a fair representation of all 
patterns of different classes in the entire dataset, then $P_k=N_k/N$ can 
be treated as an estimate of the {\em a priori} probability that any 
randomly selected pattern ${\bf x}$ happens to belong to class $C_k$, 
without any prior knowledge of the pattern.

Once a classifier is properly trained according to a specific algorithm 
based on the traning set, the feature space is partitioned into regions 
corresponding to the different classes and any unlabeled pattern of unknown 
class as a vector ${\bf x}$ in the feature space can be classified into 
one of the $K$ classes. 

Supervised classification can be considered as a process of 
establishing the corresponding relationship between the patterns 
${\bf x}_1,\cdots, {\bf x}_N$ treated as the independent or input 
variables to the classifier, and the classes $C_1,\cdots,C_K$ the 
input patterns belong, treated as the dependent or output variables.
Therefore regression and classification can be considered as the
same supervised learning process: modeling the relationship between 
the data points in $\{ {\bf x}_1,\cdots,{\bf x}_N\}$ and their 
corresponding labelings (or targets) in $\{ y_1,\cdots,y_N\}$. This
process is regression when the labelings take continous real values, 
but it is classification when they are discrete categorical 
representing different classes. Some methods in the previous chapter
on regression analysis are actually used as classifiers, such as 
logistic and solfmax regressions, and the method of Gaussian process 
can also be used for classification.

The model output error, bias vs variance...


If the training data of labeled patterns are unavailable, various
{\em unsupervised learning} methods can be used to assign each unlabeled 
patterns into one of the $K$ different groups, called {\em clusters}, 
according to its position in the feature space, based on the overall
spatial structure and distribution of the data set in the feature
space. This process is called {\em clustering analysis} or simply 
{\em clustering}. 

In the following, methods for both supervised classification and 
unsupervised clustering will be discussed.

\section{K Nearest Neighbor and Minimum Distance Classifiers}

Here we first consider a set of simple supervised classification
algorithms that assign an unlabeled sample ${\bf x}$ to one of 
the $K$ known classes based on set $N$ of {\em training samples} 
${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$, where each sample ${\bf x}_n$ 
is labeled by $y_n=k\in\{1,\cdots,K\}$, indicating it belongs to 
class $C_k$. 

\begin{itemize}

\item {\bf k Nearest neighbors (k-NN) Classifier}

  Given an unlabeled pattern ${\bf x}$, we first find its $k$
  {\em nearest neighbors} in the training dataset, and then assign 
  ${\bf x}$ to one of the $K$ classes by a majority vote of the $k$
  neighbors based on their class labelings. The voting can be weighted
  so that closer neighbors are more heavily weighted than those that 
  are farther away. In particular, when $k=1$, ${\bf x}$ is assigned 
  to the class of its closest neighbor.

  While the k-NN method is simple and straight forward, its computational
  cost is high as classifying any unlabeled pattern ${\bf x}$ requires 
  computing distances to all data points in the training set.

\item {\bf Minimum Distance Classifier}

  Given a set of training data points $\{{\bf x}_1,\cdots,{\bf x}_{N_k}\}$ 
  all belonging to the kth class $C_k$ ($k=1,\cdots,K$), we can find their 
  mean and covariance to represent the class:
  \begin{equation}
    {\bf m}_k=\frac{1}{N_k}\sum_{n=1}^{N_k} {\bf x}_n,\;\;\;\;\;\;\;
    {\bf\Sigma}_k=\frac{1}{N_k}\sum_{n=1}^{N_k} ({\bf x}_n-{\bf m})^T
    ({\bf x}_n-{\bf m})
  \end{equation}
  Any unlabeled data point ${\bf x}$ can be classified to one of the
  $K$ classes based on certain distance $d({\bf x},\,C_k)$ between
  ${\bf x}$ and each of class $C_k$:
  \begin{equation}
    \mbox{if}\;\;d({\bf x},C_k)=min \{d({\bf x},C_i)\;\;i=1,\cdots,C \} 
    \;\;\;\;\mbox{then} \;\;{\bf x}\in C_k
  \end{equation}
  
  We could simply use the Euclidean distance $d_E({\bf x},{\bf m}_k)$
  between ${\bf x}$ and ${\bf m}_k$. But such a classification may not 
  reliable as Euclidean distance does not take into consideration the
  covariance ${\bf\Sigma}_k$ representing how the $N_k$ samples are 
  distributed in the feature space, as illustrated by the following
  example.

  {\bf Example 1:} As illustrated in the figure below (left plot), a
  point $x=1$ in 1-D space is to be classified into one of the two 
  classes represented by their corresponding Gaussian pdfs:
  \begin{equation}
    C_1\sim{\cal N}( 5, 1.2^2),\;\;\;\;\;\;\;\;\;\;\;\;\;
    C_2\sim{\cal N}(-5, 3^2)
  \end{equation}
  If only the two means $m_1=5$ and $m_2=-5$ are considered, we have
  $d_E(x,\,m_1)=4\;<\;d_E(x,\,m_2)=6$, i.e., $x=1$ is closer to $m_1$ 
  than $m_2$ and therefore should be classified to class $C_1$. However,
  as shown in the plot, $x$ should be classified to class $C_2$, if the 
  variances $\sigma_1=1.2$ and $\sigma_2=3$ are also taken into consideration. 
  
%  \htmladdimg{../figures/MahalanobisDist.png}
  \htmladdimg{../figures/GaussianPlot1.png}

  We see the distance $d({\bf x},\,C_k)$ should be positively related 
  to $d_E(x,\,m_k)$, but inversely related to $\sigma_k$, i.e., we can
  define $d(x,C_k)=(x-m_k)^2/\sigma_k^2$. Based on this distance, we find
  $d(x_1,\,m_1)=11.11 \;>\;d(x_1,\,m_2)=4.0$, i.e., $x=1$ should be
  classified to $C_2$.

  In a higher dimensional feature space, we can carry out classification 
  based on the more generally defined {\em Mahalanobis distance} between 
  a point ${\bf x}$ and a distribution represented by ${\bf m}_k$ and 
  ${\bf\Sigma}_k$:
  \begin{equation}
    d_M({\bf x},C_k)=({\bf x}-{\bf m}_k)^T{\bf \Sigma}_k^{-1}({\bf x}-{\bf m}_k)
  \end{equation}

  {\bf Example 2:} As illustrated in the above figure (right plot), two
  samples $x_1=1$ and $x_2=3$ are to be classified into either of two 
  classes:  
  \begin{equation}
    C_1\sim{\cal N}(0, 1.2^2),\;\;\;\;\;\;\;\;\;\;\;\;\;
    C_2\sim{\cal N}(0, 3^2)
  \end{equation}
  As the two means $m_1=m_2=0$ are the same, $|x_i-m_1|^2=|x_i-m_2|^2$ 
  for both samples $x_1$ and $x_2$, they are both classified into $C_2$ 
  with a greater variance $\sigma_2\;> \;\sigma_2$ therefore smaller
  Mahalanobis distances:
  \begin{equation}
    \left\{
    \begin{array}{l}
    d_M(x_1,\,C_1)=\frac{(x_1-m_1)^2}{\sigma_1^2}=\frac{1}{1.2^2}=0.69,
    \;\;\;\;\;\;\;\;\;\;\;\;\;\;
    d_M(x_1,\,C_2)=\frac{(x_1-m_1)^2}{\sigma_2^2}=\frac{1}{9}=0.11\\
    d_M(x_2,\,C_1)=\frac{(x_1-m_1)^2}{\sigma_1^2}=\frac{3}{1.2^2}=6.25,
    \;\;\;\;\;\;\;\;\;\;\;\;\;\;
    d_M(x_1,\,C_2)=\frac{(x_1-m_1)^2}{\sigma_2^2}=\frac{3^2}{9}=1
    \end{array}\right.
  \end{equation}
  However, as shown in the plot, $x_1=1$ should be classified to $C_1$.
  We therefore see that sometimes the Mahalanobis distance is not reliable
  for classification, and some better method need to be considered, as 
  discussed later.

\end{itemize}


\section{Naive Bayes Classification}

The method of naive Bayes classification is a classical supervised
classification algorithm, which is first trained by a training set 
of $N$ samples ${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$ and their 
corresponding labelings ${\bf y}=[y_1,\cdots,y_N]^T$, and then used 
to classify any unlabeled sample ${\bf x}$ into class $C_k$ with the
maximumm {\em posterior probability}. As indicated by the name, naive
Bayes classification is based on \htmladdnormallink{Bayes' theorem}
{../probability/node8.html}:
\begin{equation}
  P(C_k|{\bf x})=\frac{p({\bf x},C_k)}{p({\bf x})}  
  =\frac{p({\bf x}|C_k)P(C_k)}{p({\bf x})}  
  \label{posteriorNB}
\end{equation}
where 
\begin{itemize}
\item $P(C_k)$ is the {\em prior probability} that any data sample
  belongs to class $C_k$ without observing its values, more briefly 
  denoted by $P_k$, which can be estimated by
  \begin{equation}
    P_k=\frac{N_k}{N}=\frac{N_k}{\sum_{l=1}^K N_l},
    \;\;\;\;\;\;\;(k=1,\cdots,K)
    \label{priorNB}
  \end{equation}
  where $N_k$ is the number of data samples in the training set 
  labeled to belong to class $C_k$. This estimation is based on 
  the assumption that the training samples are evenly drawn from 
  the entire population, and are therefore a fair representation 
  of all $K$ classes.

\item 
  $p({\bf x}|C_k)=L(C_k|{\bf x})$ is the {\em likelihood} for 
  any observed ${\bf x}$ to belong to class $C_k$, which is the 
  conditional probability of ${\bf x}$ given that ${\bf x}\in C_k$, 
  assumed to be a 
  \htmladdnormallink{Gaussian}{../probability/node4.html} in 
  terms of the mean vector ${\bf m}_k$ and covariance matrix 
  ${\bf\Sigma}_k$:
  \begin{equation}
    p({\bf x}|C_k)=N({\bf x},{\bf m}_k,{\bf \Sigma}_k)
    =\frac{1}{(2\pi)^{d/2} \left| {\bf \Sigma}_k\right|^{1/2}}
    \exp\left[-\frac{1}{2}({\bf x}-{\bf m}_k)^T{\bf \Sigma}_k^{-1}
      ({\bf x}-{\bf m}_k)\right]
    \label{likelihoodNB}
  \end{equation}
  This assumption is based on the fact that the Gaussian distribution
  has the maximum entropy (uncertainty) among all probability density 
  functions with the same covariance, i.e., it imposes the least amount 
  of unsupported constraint on the model for the dataset.

\item $p({\bf x},C_k)$ is the joint probability of ${\bf x}$ and $C_k$:
  \begin{equation}
    p({\bf x},C_k)=p({\bf x}|C_k)P(C_k)=P(C_k|{\bf x})p({\bf x})
    \label{jointprobNB}
  \end{equation}

\item 
  $p({\bf x})$ is the distribution of any data sample in the dataset, 
  independent of its classe identity, the weighted sum of all 
  likelihood $p({\bf x}|C_k)$ for $k=1,\cdots,K$, i.e., the joint
  probability $p({\bf x},C_k)$ marginalized over all $K$ classes:
  \begin{equation} 
    p({\bf x})=\sum_{k=1}^K p({\bf x},C_k)=\sum_{k=1}^K P_k \,p({\bf x}|C_k)
    =\sum_{k=1}^K P_k \,
    \frac{1}{(2\pi)^{d/2} \left| {\bf \Sigma}_k\right|^{1/2}}
    \exp\left[-\frac{1}{2}({\bf x}-{\bf m}_k)^T{\bf \Sigma}_k^{-1}
      ({\bf x}-{\bf m}_k)\right]
    \label{pofxNB}
  \end{equation}

\item 
  $P(C_k|{\bf x})$ is the posterior probability that a data sample 
  ${\bf x}$ belongs to class $C_k$ based on the observed values in 
  ${\bf x}=[x_1,\cdots,x_d]^T$.
\end{itemize}

Based on Bayes' theorem discussed above, the naive Bayes method 
classifies an unlabeled data sample ${\bf x}$ to class $C_k$ with 
the maximum posterior probability ({\em maximum a posteriori (MAP)}):
\begin{equation}
  \mbox{if}\;\;P(C_k|{\bf x})=\max_l\{ P(C_l|{\bf x}),\;(l=1,\cdots,K)\},
  \;\;\;\;\mbox{then}\;\;{\bf x}\in C_k
  \label{BayesClassifier}
\end{equation}
As $p({\bf x})$ is common to all $K$ classes (independent of 
$k$), it plays no role in the relative comparison among the $K$ 
classes, and can therefore be dropped, i.e., ${\bf x}$ is classified
to $C_k$ with maximal $p({\bf x}|C_k)\,P_k$.

The naive Bayes classifier is an optimal classifier in the sense 
that the classification error is minimum. To illustrate this,
consider an arbitrary boundary between two classes $C_1$ and $C_2$ 
that partitions the 1-D feature space into two regions $R_1$ and
$R_2$, as shown below. The probability of a misclassification is
the joint probability $P( {\bf x}\in C_i \cap {\bf x}\in R_j)$
for a sample ${\bf x}\in C_i$ but falling in $R_j$. The total 
probability of error due to misclassification can be expressed as:
\begin{eqnarray}
  P(error) & = & P(({\bf x}\in R_2) \cap ({\bf x}\in C_1)) 
  +P(({\bf x}\in R_1) \cap ({\bf x}\in C_2)) \nonumber \\
  & = & P({\bf x} \in R_2/C_1)\,P_1
  +P({\bf x} \in R_1/C_2)\,P_2 \nonumber \\
  &=& P_1 \int_{R_2}p({\bf x}/C_1)d{\bf x}
  +P_2\int_{R_1}p({\bf x}/C_2)d{\bf x}
\end{eqnarray}

\htmladdimg{../figures/MLerror1.png}

It is obvious that the Bayes classifier is indeed optimal, due to 
the fact that its boundary $p({\bf x}|C_i)P_i=p({\bf x}|C_j)P_j$
between classes $C_i$ and $C_j$ (the bottom plot) guarantees the 
classification error (shaded area) to be minimum.

To find the likelihood function
$p({\bf x}|C_k)={\cal N}({\bf x},{\bf m}_k,{\bf\Sigma}_k)$, we need
to find ${\bf m}_k$ and ${\bf\Sigma}_k$ based on the training set 
${\cal D}=\{{\bf X},\,{\bf y}\}$, based on the $N_k$ training samples 
in class $C_k$ by the method of 
\htmladdnormallink{maximum likelihood estimation}{../probability/node11.html}:
\begin{equation}
  {\bf m}_k=\frac{1}{N_k}\sum_{i=1}^{N_k} {\bf x}_i,
  \;\;\;\;\;\;
  {\bf\Sigma}_k=\frac{1}{N_k}\sum_{i=1}^{N_k} 
  ({\bf x}_i-{\bf m}_k)({\bf x}_i-{\bf m}_k)^T,
  \;\;\;\;\;\;({\bf x}_i\in C_k)
\end{equation}

Once both $P_k=N_k/N$ and 
$p({\bf x}|C_k)={\cal N}({\bf x},{\bf m}_k,{\bf\Sigma}_k)$
are available, the classifier is trained, and the posterior 
$P(C_k|{\bf x})$ can be calculated for the classification in 
Eq. (\ref{BayesClassifier}).

As the classification is based on the relative comparison of the 
posterior $P(C_k|{\bf x})$, any monotonic mapping of the posterior 
can be equivalently used to simplify the computation, such as the 
logarithmic function:
\begin{eqnarray}
  \log P(C_k|{\bf x})&=&\log \left[ p({\bf x}|C_k) P_k/p({\bf x}) \right]
  =\log p({\bf x}|C_k) +\log P_k-\log p({\bf x})
  \nonumber\\
  &=&-\frac{1}{2}({\bf x}-{\bf m}_k)^T{\bf \Sigma}_k^{-1}({\bf x}-{\bf m}_k)
  -\frac{N}{2}\log(2\pi)-\frac{1}{2}\log|{\bf\Sigma}_k|
  +\log P_k-\log p({\bf x})
\end{eqnarray}
Dropping the constant terms $\log p({\bf x})$ and $N \log(2\pi)/2$ 
that are independent of the index $k$ and therefore play no role in
the comparison above, we get the {\em quadratic discriminant function}:
\begin{equation}
  D_k({\bf x})=-\frac{1}{2}({\bf x}-{\bf m}_k)^T{\bf \Sigma}_k^{-1}({\bf x}-{\bf m}_k)
  -\frac{1}{2}\log|{\bf\Sigma}_k|+\log P_k
\end{equation}
Now the classification can be represented by
\begin{equation}
  \mbox{if}\;\; D_l({\bf x})=\max \{ D_k({\bf x}),\;(k=1,\cdots,K)\},
  \;\;\;\;\;\mbox{then}\;\;{\bf x}\in C_l
\end{equation}
Equivalently, we can also treat the negative of this discriminant 
function as a distance measurement:
\begin{equation}
  d_k({\bf x})=({\bf x}-{\bf m}_k)^T{\bf \Sigma}_k^{-1}({\bf x}-{\bf m}_k)
  +\log|{\bf\Sigma}_k|-2\,\log P_k
\end{equation}
for minimum distance classification. We note that the first term is the
Mohalanobis distance defined previously. However, the additonal terms in 
the expression contribute to better classification performance.

Geometrically, the feature space is partitioned into regions corresponding 
to the $K$ classes by the {\em decision boundaries} between every pair of 
classes $C_i$ and $C_j$, represented by the equation $D_i({\bf x})=D_j({\bf x})$,
i.e.,
\begin{eqnarray}
  &&-\frac{1}{2}({\bf x}-{\bf m}_i)^T{\bf \Sigma}_i^{-1}({\bf x}-{\bf m}_i)
  -\frac{1}{2}\;\log\,|{\bf \Sigma}_i | + \log\, P_i 	
  \nonumber\\
  &=&-\frac{1}{2}({\bf x}-{\bf m}_j)^T{\bf \Sigma}_j^{-1}({\bf x}-{\bf m}_j)
  -\frac{1}{2}\;\log\,|{\bf \Sigma}_j | + \log\, P_j 
\end{eqnarray}
which can be written as
\begin{equation}
  {\bf x}^T{\bf W}{\bf x}+{\bf w}^T{\bf x}+w=0
\end{equation}
where
\begin{eqnarray}
  {\bf W}&=&-\frac{1}{2}({\bf \Sigma}_i^{-1}-{\bf \Sigma}_j^{-1})	
  \nonumber\\
  {\bf w}&=&{\bf \Sigma}_i^{-1}{\bf m}_i-{\bf \Sigma}_j^{-1}{\bf m}_j
  \nonumber\\	
  w&=&-\frac{1}{2}({\bf m}_i^T{\bf \Sigma}_i^{-1}{\bf m}_i-{\bf m}_j^T{\bf \Sigma}_j^{-1}{\bf m}_j)
  -\frac{1}{2}\log\,\frac{|{\bf \Sigma}_i|}{|{\bf \Sigma}_j|}
  +\log\,\frac{P_i}{P_j}
  \label{Www2}
\end{eqnarray}
In general, this decision boundary is a quadratic hypersurface
(hypersphere, hyperellipsoid, hyperparabola, or hyperhyperbola), by 
which an unlabeled data sample ${\bf x}$ is classified into either of 
the two classes $C_i$ and $C_j$ based on whether it is on the positive
or negative side of the surface::
\begin{equation}
  \mbox{if}\;\;{\bf x}^T{\bf W}{\bf x}+{\bf w}^T{\bf x}+w
  \left\{\begin{array}{l}>0\\<0\end{array}\right.,\;\;\;\;\;
  \mbox{then}\;\;\; {\bf x}\in \left\{\begin{array}{c}
  C_i\\ C_j  \end{array}\right.
  \label{MLdiscriminant}
\end{equation}

We further consider several special cases:
\begin{itemize}
\item All classes have the same prior:
  \begin{equation}
    P_i=P_j\;\;\;\;(i,j=1,\cdots,K)
  \end{equation}
  then the last term of $D_i({\bf x})$ is zero, and we have
  \begin{equation}
    D_i({\bf x})=-\frac{1}{2}({\bf x}-{\bf m}_i)^T{\bf \Sigma}_i^{-1}({\bf x}-{\bf m}_i)
    -\frac{1}{2}\;\log\,|{\bf \Sigma}_i |
  \end{equation}

  We can reconsider Example 2 in the previous section but now based on 
  the negative of the discriminant function above treated as a distance
  $d({\bf x},\,C_i)=({\bf x}-{\bf m}_i)^T{\bf \Sigma}_i^{-1}({\bf x}-{\bf m}_i)
  +\log\,|{\bf \Sigma}_i |$, and get
  \begin{equation}
    \left\{ \begin{array}{l}
      d(x_1,\,C_1)=0.69+\log(1.2^2)=1.06,   \;\;\;\;\;\;\;\;\;\;\;\;\;\;
      d_M(x_1,\,C_2)=0.11+\log(3^2)=2.31\\
      d_M(x_2,\,C_1)=6.25+\log(1.2^2)=6.61,  \;\;\;\;\;\;\;\;\;\;\;\;\;\;
      d_M(x_1,\,C_2)=1+\log(3^2)=3.20
    \end{array}\right.
  \end{equation}
  Now we see that $x_1$ is classified into class $C_1$, while $x_2$ into $C_2$,
  as desired. The misclassification of $x_1$ to $C_2$ by the Mahalanobis 
  distance, the first term, is corrected, due to the addition of the second 
  term $\log(\sigma^2)$. The plot below shows the partitioning of the 1-D
  space for the two classes $C_1$ and $C_2$:

  \htmladdimg{../figures/GaussianPlot2.png}


\item All classes have the same covariance matrix ${\bf\Sigma}_i={\bf\Sigma}$,
  the discriminant function becomes:
  \begin{equation} 
    D_i({\bf x})=-\frac{1}{2}({\bf x}-{\bf m}_i)^T{\bf \Sigma}^{-1}({\bf x}-{\bf m}_i)
    +\log\,P_i 
  \end{equation}
  Moreover, if all $P_i$ are the same and the second term is dropped,
  $D_i({\bf x})$ becomes the negative {\em Mahalanobis distance}.

  The boundary equation $D_i({\bf x})=D_j({\bf x})$ between $C_i$ and $C_j$
  becomes
  \begin{equation} 
    -\frac{1}{2}({\bf x}-{\bf m}_i)^T{\bf\Sigma}^{-1}({\bf x}-{\bf m}_i)+\log\,P_i
    =-\frac{1}{2}({\bf x}-{\bf m}_j)^T{\bf\Sigma}^{-1}({\bf x}-{\bf m}_j)+\log\,P_j 
  \end{equation}
  As the quadratic terms ${\bf x}^T{\bf\Sigma}^{-1}{\bf x}$ on both sides
  of the equation are the same, it becomes a linear equation:
  \begin{equation}
    {\bf w}^T{\bf x}+w=0	
    \label{LDF}
  \end{equation}
  where
  \begin{equation}
    {\bf w}={\bf \Sigma}^{-1}({\bf m}_i-{\bf m}_j)	
  \end{equation}
  and 
  \begin{equation}
    w=-\frac{1}{2}({\bf m}_i^T{\bf \Sigma}^{-1}{\bf m}_i
    -{\bf m}_j^T{\bf \Sigma}^{-1}{\bf m}_j)+\log\,\frac{P_i}{P_j}	
  \end{equation}
  This linear equation represents a hyperplane between the two points
  ${\bf m}_i$ and ${\bf m}_j$ and perpendicular to the straight line
  ${\bf \Sigma}^{-1}({\bf m}_i-{\bf m}_j)$
  (the straight line ${\bf m}_i-{\bf m}_j$ rotated by matrix ${\bf \Sigma}^{-1}$).

\item All classes have the same isotropic distribution:
  \begin{equation}
    {\bf \Sigma}_i={\bf \sigma}^2\,{\bf I}=diag[\sigma^2,\cdots,\sigma^2]	
  \end{equation}
  then $|{\bf \Sigma}_i|=\sigma^{2d}$ and $D_i({\bf x})$ becomes
  \begin{equation}
    D_i({\bf x})=-\frac{|| {\bf x}-{\bf m}_i ||^2}{2\sigma^2}+\log\,P_i 
  \end{equation}
  Note that the term $\log\,|{\bf \Sigma}_i|$ has been dropped from the original 
  expression of $D_i({\bf x})$ as it is now the same for all classes.

  The boundary equation $D_i({\bf x})=D_j({\bf x})$ becomes:
  \begin{equation}
    \frac{|| {\bf x}-{\bf m}_i ||^2}{2\sigma^2}-\log\,P_i
    =\frac{|| {\bf x}-{\bf m}_j ||^2}{2\sigma^2}-\log\,P_j 
  \end{equation}
  which can be simplified to a linear equation:
  \begin{equation}
    {\bf w}^T{\bf x}+w=0
  \end{equation}
  where
  \begin{equation}
    {\bf w}={\bf m}_i-{\bf m}_j,\;\;\;\;\;
    w=-({\bf m}_i^T{\bf m}_i-{\bf m}_j^T{\bf m}_j)+2\sigma^2\;\log\,\frac{P_i}{P_j}
  \end{equation}
  This linear equation represents a hyperplane between the two points ${\bf m}_i$ 
  and ${\bf m}_j$ and perpendicular to the straight line passing through these
  points.

\item Further, if all classes have the same $P_i$, $D_i({\bf x})$ becomes
  \begin{equation}	
    D_i({\bf x})=-({\bf x}-{\bf m}_i)^T({\bf x}-{\bf m}_i)=-||{\bf x}-{\bf m}_i||^2
  \end{equation}
  and the Bayes classifier becomes minimum distance classifier based on
  Euclidean distance (maximizing $D_i({\bf x})$ is equivalent to minimizing
  $||{\bf x}-{\bf m}_i||^2$.

\end{itemize}

{\bf Example 1:}

This example shows the classification of two cocentric classes with one
surrounding the other. They are separated by an ellipse with 9 out of 
200 samples misclassified, i.e., the error rate of $9/200=4.5\%$. The 
confusion matrix is shown below:
\begin{equation}
\left[\begin{array}{rrr}
    92 &   8   \\
     1 &  99   \\
\end{array}\right]
\end{equation}
indicating 8 of the 100 samples belonging to class 1 are misclassified
to class 2, and 1 of the 100 samples belonging to class 2 is misclassified
to class 1.

\htmladdimg{../figures/MLexample3.png}

{\bf Example 2:}

This example shows the classification of a 2-class exclusive-or (XOR)
data set with significant overlap, in terms of the confusion matrix and 
the error rate of $61/400=15.25\%$. Note that the decision boundaries
are a pair of hyperbolas. The confusion matrix is 
\begin{equation}
\left[\begin{array}{rrr}
 175 &   25   \\
  36 &  164   \\
\end{array}\right]
\end{equation}

\htmladdimg{../figures/MLexample2.png}

{\bf Example 3:}

The first two panels in the figure below show three classes in the 2-D
space, while the third one shows the partitioning of the space corresponding
to the three classes. Note that the boundaries between the classes are all 
quadratic. The confusion matrix of the classification result is shown below, 
with the error rate $49/600=8.17\%$.
\begin{equation}
\left[\begin{array}{rrr}
    196 & 3 & 1   \\
    0 & 191 & 9   \\
    3 & 33 & 164  \\
\end{array}\right]
\end{equation}

\htmladdimg{../figures/MLexample1.png}

{\bf Example 4}

The figure below shows some sub-samples of ten sets digits from 0 to 9, 
each is hand-written 224 times by different students. Each hand-written
digit is represented as an 16 by 16 image containing 256 pixels, treated
as an $N=256$ dimensional vector. 

\htmladdimg{../figures/Data123Samples.png}

The dataset can be visualized by applying the KLT transform to map all
data points in the 256-D space into a 3-D space spanned by the three 
eigenvectors corresponding to the three greatest eigenvalues of the 
covariance matrix of the dataset, as shown below:

\htmladdimg{../figures/Data123_3D.png}

Then a Bayes classifier trained on the dataset is used to classify the 
same dataset into ten classes. The resulting confusion matrix as shown 
below, indicating 158 out of the 2240 samples are misclassified with an
error rate of 5.63\%.

\begin{equation}
\left[ \begin{array}{rrrrrrrrrr}
   215 &   0 &   0 &   0 &   0 &   2 &   4 &   0 &   3  &  0 \\
     0 & 216 &   0 &   0 &   1 &   0 &   1 &   3 &   1  &  2 \\
     2 &   0 & 219 &   0 &   0 &   0 &   0 &   1 &   2  &  0 \\
     1 &   0 &   0 & 212 &   0 &   1 &   0 &   3 &   4  &  3 \\
     1 &   0 &   1 &   0 & 213 &   0 &   0 &   0 &   4  &  5 \\
     1 &   0 &   2 &   1 &   0 & 211 &   0 &   0 &   9  &  0 \\
     2 &   6 &   0 &   0 &   0 &   0 & 213 &   0 &   3  &  0 \\
     2 &   0 &   3 &   0 &   0 &   0 &   1 & 205 &   3  & 10 \\
     1 &   0 &   2 &   4 &   0 &   8 &   1 &   4 & 200  &  4 \\
     0 &   0 &   1 &   1 &   8 &   0 &   0 &   2 &   2  &210 \\
\end{array} \right]
\end{equation}

\section{AdaBoost}

%http://119.90.25.43/web.eecs.umich.edu/~jjcorso//t/598F14/files/lecture_1117_boosting.pdf
%https://mitpress.mit.edu/sites/default/files/titles/content/boosting_foundations_algorithms/titlepage.html
%http://rob.schapire.net/papers/explaining-adaboost.pdf
%http://rob.schapire.net/papers/conv-rate-adaboost-journal.pdf
%http://119.90.25.31/www.inf.fu-berlin.de/inst/ag-ki/adaboost4.pdf
%http://cs.nyu.edu/~dsontag/courses/ml12/slides/lecture13.pdf
%http://www.robots.ox.ac.uk/~az/lectures/cv/adaboost_matas.pdf
%http://www.jmlr.org/papers/volume14/mukherjee13b/mukherjee13b.pdf
%https://arxiv.org/abs/1106.6024
%http://mipal.snu.ac.kr/images/2/29/ISCAS_Boosted_pca_final.pdf

The {\em Adaptive boosting (AdaBoost)} is a supervised binary 
classification algorithm based on a training set 
$\{ ({\bf x}_i,\,y_i)\;|\;i=1,\cdots,N\}$, where each sample 
${\bf x}_i$ is labeled by $y_i\in\{-1,\,+1\}$, indicating to which 
of the two classes $C_-$ and $C_+$ it belongs. 

AdaBoost is an iterative algorithm. In the t-th iteration, each of 
the $N$ training samples is classified into one of the two classes
by a {\em weak classifier} $h_t({\bf x}_n)\in\{-1,\,+1\}$, which can
be considered as a hypothesis. The classifier is weak in the sense 
that its performance only needs to be better than chance, i.e., the 
error rate is less than $50\%$. If a sample ${\bf x}_n$ is correctly 
classified, $h_t({\bf x}_n)=y_n=\pm 1$, i.e., $y_n\,h_t({\bf x}_n)=1$;
if it is misclassified, $h_t({\bf x}_n)=-y_n=\pm 1$, i.e., 
$y_n\,h_t({\bf x}_n)=-1$. We need to find the best weak classifier
that minimizes the weighted error rate defined as:
\begin{equation}
  \varepsilon_t=\frac{\sum_{y_n\,h_t({\bf x}_n)=-1} w_t(n)}{\sum_{n=1}^N w_t(n)}
\end{equation}
where $w_t(n)$ is the weight of the nth training samples at the t-th 
iteration. We can also get the correct rate:
\begin{equation}
  1-\varepsilon_t=1-\frac{\sum_{y_n\,h_t({\bf x}_n)=-1} w_t(n)}{\sum_{n=1}^N w_t(n)}
  =\frac{\sum_{y_n\,h_t({\bf x}_n)=1} w_t(n)}{\sum_{n=1}^N w_t(n)}
\end{equation}
At the initial stage with $t=0$, all $N$ samples are equally 
weighted by $w_0(1)=\cdots=w_0(N)=1$, and the weighted error 
\begin{equation}
  \varepsilon_0=\frac{1}{N} \sum_{y_n\,h_0({\bf x}_n)=-1} w_0(n)
  =\frac{\mbox{number of misclassified samples}}
  {\mbox{total number of samples}}
\end{equation}
is actually the probability for any sample ${\bf x}$ in the training
set to be misclassified by $h_0$. For the weak classifier the error 
rate defined above only needs to be lower than 50 percent, i.e., 
$\varepsilon_t<1/2$. When $t>0$, this weight $w_t(n)$ will be modified
based on whether ${\bf x}_n$ is classified correctly in the subsequent
iterations, as we will see below.

At each iteration, a {\em strong} or {\em boosted classifier} 
$H_t({\bf x}_n)$ is also constructed based on the linear combination 
(boosting) of all previous weak classifiers $h_1({\bf x}_n),\cdots,
h_t({\bf x}_n)$ for all $n=1,\cdots,N$:
\begin{eqnarray}
  F_t({\bf x}_n)&=&\alpha_th_t({\bf x}_n)+F_{t-1}({\bf x}_n)
  =\alpha_th_t({\bf x}_n)+\alpha_{t-1}h_{t-1}({\bf x}_n)+F_{t-2}({\bf x}_n)
  \nonumber\\
  &=&\cdots=\sum_{i=1}^t\alpha_ih_i({\bf x}_n)
  \label{StrongClassifierF}
\end{eqnarray}
where the coefficient $\alpha_i$ for the weak classifier 
$h_i({\bf x}_n)$ in the i-th iteration is obtained as discussed 
below. Taking the sign function of $F_t({\bf x})$ we get the strong 
classifier:
\begin{equation}
  H_t({\bf x}_n)=sign [F_t({\bf x}_n)]=\left\{\begin{array}{ll}
  +1 & F_t({\bf x}_n)>0\\-1 & F_t({\bf x}_n)<0\end{array}\right.
\end{equation}
which classifies a training sample ${\bf x}_n$ into one of the two 
classes, while the magnitude $|F_t({\bf x}_n)|$ represents the confidence 
of the decision.

The weight $w_t(n)$ of ${\bf x}_n$ in the t-th iteration $(t>0)$ is
defined as 
\begin{equation}
  w_t(n)=e^{-y_n F_{t-1}({\bf x}_n)}\left\{\begin{array}{ll}
  >1 & \mbox{if $y_n\,F_{t-1}({\bf x}_n)<0$ (misclassification)}\\
  <1 & \mbox{if $y_n\,F_{t-1}({\bf x}_n)>0$ (correct classification)}
  \end{array}\right.
\end{equation}
The weight $w_t(n)$ of the t-th iteration can be obtained recursively 
from $w_{t-1}(n)$ of the previous iteration:
\begin{eqnarray}
  w_t(n)&=&e^{-y_n\,F_{t-1}({\bf x}_n)}
  =e^{-y_n\,[\alpha_{t-1}h_{t-1}({\bf x}_n)+F_{t-2}({\bf x}_n)]}
  \nonumber\\
  &=&e^{-\alpha_{t-1}\,y_nh_{t-1}({\bf x}_n)}\;e^{-y_n\,F_{t-2}({\bf x}_n)}
  =e^{-\alpha_{t-1}\,y_nh_{t-1}({\bf x}_n)}\;w_{t-1}(n)
  \label{weightupdate}
\end{eqnarray}
and this recursion can be carried out all the way back to the first 
iteration with $w_1(k)=e^{-\alpha_0 \,y_nh_0({\bf x}_n)}w_0(n)$. 

The performance of the strong classifier can be measured by the 
{\em exponential loss}, defined as the sume of all $N$ weights:
\begin{equation}
  E_{t+1}=\sum_{n=1}^N w_{t+1}(n)=\sum_{n=1}^N e^{-y_n\,F_t({\bf x}_n)}
  =\sum_{n=1}^N e^{-\alpha_t\,y_n\, h_t({\bf x}_n)} \;w_t(n)
  \;\;\;\;\;\;(t>1)
\end{equation}

The coefficient $\alpha_t$ in Eq. (\ref{StrongClassifierF}) can now
be found as the one that minimizes the exponential loss $E_{t+1}$. 
To do so, we first separate the terms in the summation for $E_{t+1}$
into two parts, corresponding to the samples that are classified by
$h_t$ correctly and incorrectly:
\begin{equation}
  E_{t+1}=\sum_{n=1}^N w_t(n)e^{-\alpha_t \,y_n\,h_t({\bf x}_n)} 
  =\sum_{y_n h_t({\bf x}_n)=-1} w_t(n)e^{\alpha_t}
  +\sum_{y_n h_t({\bf x}_n)=1} w_t(n)e^{-\alpha_t}
\end{equation}
and then set its derivative with respect to $\alpha_t$ to zero:
\begin{equation}
  \frac{dE_{t+1}}{d\alpha_t}
  =\sum_{y_n h_t({\bf x}_n)=-1} w_t(n) e^{\alpha_t}
  -\sum_{y_n h_t({\bf x}_n)=1} w_t(n) e^{-\alpha_t}=0
\end{equation}
Solving this equation we get $\alpha_t$ as the optimal coefficient 
that minimizes $E_{t+1}$:
\begin{equation}
  \alpha_t=\frac{1}{2}\,\ln \left(
  \frac{\sum_{y_n h_t({\bf x}_n)=1} w_t(n)}{\sum_{y_n h_t({\bf x}_n)=-1} w_t(n)}\right)
  =\ln\,\sqrt{\frac{1-\varepsilon_t}{\varepsilon_t}} > 0
\end{equation}
which is greater than zero becauses $\varepsilon_t<1/2$,
$1-\varepsilon_t>1/2$, and 
\begin{equation}
  e^{\alpha_t}=\sqrt{\frac{1-\varepsilon_t}{\varepsilon_t}}>1,\;\;\;\;
  e^{-\alpha_t}=\sqrt{\frac{\varepsilon_t}{1-\varepsilon_t}}<1
\end{equation}

We now see that if a weak classifier $h_t$ has a small error 
$\varepsilon_t$, it will be weighted by a large $\alpha_t$ and therefore
contributes more to the strong classifier $H_t$; on the other hand, if 
$h_t$ has a large error $\varepsilon_t$, it will be weighted by a small
$\alpha_t$ and contributes less to $H_t$. As the strong classifier $H_t$ 
takes advantage of all previous weak classifiers $h_1,\cdots,h_t$, each 
of which may be more effective in a certain region of the N-D feature 
space than others, $H_t$ can be expected to be a much more accurate 
classifier than any of the weak classifiers.

Replacing $t$ by $t+1$ in Eq. (\ref{weightupdate}), we now get
\begin{equation}
  w_{t+1}(n)=w_t(n)\,e^{-\alpha_t\,y_nh_t({\bf x}_n)}
  =\left\{\begin{array}{ll} 
  w_t(n) \sqrt{\frac{\varepsilon_t}{1-\varepsilon_t}}<w_t(n) & 
  \mbox{if }y_n h_t({\bf x}_n)=1 \\
  w_t(n) \sqrt{\frac{1-\varepsilon_t}{\varepsilon_t}}>w_t(n) & 
  \mbox{if }y_n h_t({\bf x}_n)=-1 
  \end{array}\right.
\end{equation}
We see that if $y_nh_t({\bf x}_n)=1$, i.e., ${\bf x}_n$ is classified
correctly by $h_t$, it will be weighted more lightly by $w_{t+1}(n)<w_t(n)$
in the next iteration, but if $y_nh_t({\bf x}_n)=-1$, i.e., ${\bf x}_n$ 
is classified incorrectly by $h_t$, it will be weighted more heavily by 
$w_{t+1}(k)>w_t(k)$ in the next iteration, thereby it will be emphasized 
and have a better chance to be corrected to have more accurately 
classified by $h_{t+1}$ in the next iteration.

We further consider the ratio of the exponential losses of two 
consecutive iterations:
\begin{eqnarray}
  \frac{E_{t+1}}{E_t}&=&\frac{\sum_{n=1}^N w_{t+1}(n)}{\sum_{n=1}^N w_t(n)}
    =\frac{\sum_{n=1}^N w_t(n)e^{-\alpha_t\,y_n\,h_t({\bf x}_n)}}{\sum_{n=1}^N w_t(k)}
    \nonumber\\
    &=&\frac{\sum_{y_n h_t({\bf x}_n)=-1} w_t(n)}{\sum_{n=1}^N w_t(n)}e^{\alpha_t}
    +\frac{\sum_{y_n h_t({\bf x}_n)=1} w_t(n)}{\sum_{n=1}^N w_t(n)}e^{-\alpha_t}
    \nonumber\\
    &=&\varepsilon_t\;\sqrt{\frac{1-\varepsilon_t}{\varepsilon_t}}
    +(1-\varepsilon_t)\;\sqrt{\frac{\varepsilon_t}{1-\varepsilon_t}}
    =2\sqrt{\varepsilon_t(1-\varepsilon_t)}\le 1
\end{eqnarray}
This ratio is twice the geometric average 
$\sqrt{\varepsilon_t(1-\varepsilon_t)}\le 1$ of $\varepsilon_t$ 
and $1-\varepsilon_t$, which reaches its maximum when 
$\varepsilon_t=1-\varepsilon_t=1/2$. However, as $\varepsilon_t<1/2$, 
the ratio is always smaller than 1. We see that the exponential cost 
can be approximated as an exponentially decaying function from its
initial value $E_0=\sum_{n=1}^N w_0(n)=N$ for some $\varepsilon<1/2$:
\begin{equation}
  \lim_{t\rightarrow\infty}E_t\approx 
  \lim_{t\rightarrow\infty}\left(2\sqrt{\varepsilon(1-\varepsilon)}\right)^t E_0=0
\end{equation}
We can therefore conclude that the exponential loss will always decrease, 
i.e., the error of AdaBoost will always converge to zero. 

The weak classifier used in each iteration is typically implemented
as a {\em decision stump}, a binary classifier that partitions the 
N-D feature space into two regions. Specifically, as a simple example, 
a coordinate descent method can be used by which all training samples 
are projected onto the ith dimension of the feature space ${\bf d}_i$:
\begin{equation}
  x_n=P_{{\bf d}_i}({\bf x}_n)=\frac{{\bf x}_n^T{\bf d}_i}{||{\bf d}_i||},
\;\;\;\;\;\;(n=1,\cdots,N)
\end{equation}
and then classified into two classes by a threshold $T$ along the 
direction of ${\bf d}_i$:
\begin{equation}
  h(x_n)=\left\{\begin{array}{ll}+1& \mbox{if }x_n<T\\-1 & \mbox{if }x_n>T
  \end{array}\right.
\end{equation}
The optimal decision stump is obtained when the following weighted error 
is minimized with respect to the threshold $T$ along that direction of 
${\bf d}_i$:
\begin{equation}
  \varepsilon_t=\sum_{n=1}^N w_t(n)\; \delta(h_t(x_n)-y_n)
  =\sum_{h_t(x_n)\ne y_n} w_t(n)
\end{equation}

Alternatively, the weak classifier can also be obtained based on the
{\em principal component analysis (PCA)} by partitioning the N-D feature
space along the directions of the eigenvectors of the between-class 
scatter matrix
\begin{equation}
  {\bf S}_b=\frac{1}{N}\left[N_-({\bf m}_{-1}-{\bf m})({\bf m}_{-1}-{\bf m})^T
    +N_+({\bf m}_{+1}-{\bf m})({\bf m}_{+1}-{\bf m})^T\right]
\end{equation}
where $N_-$ and $N_+$ are the numbers of samples in the two classes
$(N_-+N_+=N)$, and 
\begin{equation}
  {\bf m}_{\pm}=\frac{1}{N_{\pm}}\sum_{y_n=\pm 1} w(n)\,{\bf x}_n,
  \;\;\;\;\; {\bf m}=\frac{1}{N}\sum_{n=1}^N w(n)\,{\bf x}_n
\end{equation}
are the weighted mean vectors of the two classes and the total 
weighted mean vector of all $N$ samples in the training set. 
The training samples are likely to be better separated along these 
directions of the eigenvectors of ${\bf S}_b$, which measures the 
separability of the two classes. The eigenequations of ${\bf S}_b$
is:
\begin{equation}
  {\bf S}_b={\bf\Phi\Lambda\Phi}^{-1}={\bf\Phi\Lambda\Phi}^T
\end{equation}
As ${\bf S}_b$ is symmetric, the eigenvector matrix ${\bf\Phi}$
is orthonormal and its columns can be used as an orthogonal basis
that spans the feature space as well as the standard basis. The 
same binary threshold classification considered above for the weak
classifiers can be readily applied along these PCA bases.

The Matlab code of the main iteration loop of the algorithm is 
listed below, followed by the essential functions called by the 
main loop. Here \verb|T| is the maximum number of iteration, and
\verb|N| is the total number of training samples. 

The algorithm can be carried out in the feature space based on
either the PCA basis (columns of the eigenvector matrix ${\bf\Phi}$),
or the original standard basis (columns of the identity matrix 
${\bf I}$). 

The {\em decision stump} is implemented by a binary classifier, 
which partitions all training samples projected onto a 1-D space 
into two groups by a threshold value, which needs to be optimal in
terms of the number of misclassification. A parameter \verb|plt| is
used to indicate the polarity of the binary classification, i.e.,
which of the two class $C_1$ and $C_0$ is on the lower (or higher)
side of the threshold.

\begin{verbatim}
    Plt=[];              % polarity of binary classification
    Tr=[];               % transformation of basis vectors
    Th=[];               % threshold of binary classification
    Alpha=[];            % alpha values 
    h=zeros(T,N);        % h functions 
    F=zeros(T,N);        % F functions
    w=ones(T,N);         % weights for N training samples
    Er=N;                % error initialized to N
    t=0;                 % iteration index
    while t<T  & Er>0                 % the main iteration
        t=t+1;                        
        [tr th plt er]=WeakClassifier(X,y,w(t,:),PCA); % weak classifier
        alpha=log(sqrt((1-er)/er));   % update alpha   
        Alpha=cat(1,Alpha,alpha);     % record alpha
        Tr=cat(2,Tr,tr');             % record transform vector
        Th=cat(1,Th,th);              % record threshold 
        Plt=cat(1,Plt,plt);           % record polarity
        x=tr*X;                       % carry out transform
        c=sqrt(er/(1-er));
        for n=1:N                     % update weights
            h(t,n)=h_function(x(n),th,plt);     % find h function
           if  h(t,n)*y(n)<0
               w(t+1,n)=w(t,n)/c;     % update weights
           else
               w(t+1,n)=w(t,n)*c;
           end            
        end      
        F(t,:)=Alpha(t)*h(t,:);       % get F functions
        if t>1
            F(t,:)=F(t,:)+F(t-1,:);
        end
        Er=sum(sign(F(t,:).*y)==-1);  % error of strong classifier, number of misclassifications
        fprintf('%d: %d/%d=%f\n',t,Er,N,Er/N)
    end
\end{verbatim}

Here are the functions called by the main iteration loop above:

\begin{verbatim}
function [Tr,Th Plt Er]=WeakClassifier(X,y,w,pca)
    % X: N columns each for one of the N training samples
    % y: labeling of X
    % w: weights for N training samples
    % pca: use PCA dimension if pca~=0
    % Er: minimum error among all D dimensions
    % Tr: transform vector (standard or PCA basis)
    % Th: threshold value 
    % Plt: polarity 
    [D N]=size(X);
    n0=sum(y>0);            % number of samples in class C+
    n1=sum(y<0);            % number of samples in class C-
    if pca                  % find PCA basis
        for i=1:D
            Y(i,:)=w.*X(i,:);
        end
        X0=Y(:,find(y>0));  % all samples in class C+
        X1=Y(:,find(y<0));  % all samples in class C-
        m0=mean(X0')';      % mean of C+
        m1=mean(X1')';      % mean of C-
        mu=(n0*m0+n1*m1)/N; % over all mean
        Sb=(n0*(m0-mu)*(m0-mu)'+n1*(m1-mu)*(m1-mu)')/N; % between-class scatter matrix
        [v d]=eig(Sb);      % eigenvector and eigenvalue matrices of Sb
    else
        v=eye(N);           % standard basis
    end  
    Er=9e9;
    for i=1:D               % for all D dimensions
        tr=v(:,i)';         % get transform vector from identity or PCA matrix
        x=tr*X;             % rotate the vector
        [th plt er]=BinaryClassifier(x,y,w); % binary classify N samples in 1-D
        er=0;
        for n=1:N
            h(n)=h_function(x(n),th,plt);  % h-function of nth sample 
            if h(n)*y(n)<0                 % if misclassified
                er=er+w(n);                % add error
            end
        end   
        er=er/sum(w);                      % total error of dimension d
        if Er>er                           % record info corresponding to min error
            Er=er;                         % min error
            Plt=plt;                       % polarity
            Th=th;                         % threshold
            Tr=tr;                         % transform vector
        end
    end 
end

function h=h_function(x, th, plt)
    if xor(x>th, plt)
        h=1;
    else
        h=-1;
    end
end

function [Th Plt Er]=BinaryClassifier(x,y,w)
    N=length(x); 
    [x1 i]=sort(x);            % sort 1-D data x
    y1=y(i);                   % reorder the targets 
    w1=w(i);                   % reorder the weights 
    Er=9e9;
    for n=1:N-1                % for N-1 ways of binary classification       
        e0=sum(w1(find(y1(1:n)==1)))+sum(w1(n+find(y1(n+1:N)==-1)));
        e1=sum(w1(find(y1(1:n)~=1)))+sum(w1(n+find(y1(n+1:N)~=-1)));
        if e1 > e0                 % polarity: left -1, right +1
            plt=0; er=e0; 
        else                       % polarity: left +1, right -1
            plt=1; er=e1;
        end
        if Er > er                 % update minimum error for kth dimension
            Er=er;                 % minimum error
            Plt=plt;               % polarity
            Th=(x1(k)+x1(k+1))/2;  % threshold
        end        
    end
end
\end{verbatim}

{\bf Example 0} This example shows the classification of an XOR data
set (used previously the test the naive Bayes mehtod), containing two 
classes of $N=200$ simples each in 2-D space, represented by the red 
and blue dots in the figure below. The intermediate results after 4, 8,
16 and 32 iterations are shown in to illustrate the progress of the 
iteration, when the error rate continuously reduces from 134/400, 85/400, 
81/400, 58/100. 

\htmladdimg{../figures/AdaBoostEx0a.png}

After over 350 iterations the error rate eventually reduced to zero
(guaranteed by the AdaBoost method). The sequence of binary participation
Although all training samples are correctly classified eventually, the 
result suffers the problem of overfitting.

\htmladdimg{../figures/AdaBoostEx0.png}

{\bf Example 1} This example shows the classification of two classes 
of $N=500$ simples in 2-D space, which is partitioned along both the
standard basis vectors (left) and the PCA directions (right). The PCA 
method performances significantly better in this example as its error 
reduces faster and it converges to zero after 61 iterations, while the 
error of the method based on the standard axes (in vertical and horizontal
directions) does not go down to zero even after 120 iterations. This 
faster reduction of error of the PCA method can be easily understood 
as many different directions are used to partition the space into two
regions, while in the coordinate descent method the partitioning of the 
space is limited to either of the two directions.

\htmladdimg{../figures/AdaBoostCompare1.png}

How the AdaBoost is iteratively trained can be seen in the figure below. 
First, the error rate, the ratio between the number of misclassified 
samples and the total number of samples, for both the training and 
testing samples, are plotted as the iteration progresses (top). Also, 
the weighted error $\varepsilon_t$, the exponential costs $E_t$, and
the ratio $E_{t+1}/E_t=2\sqrt{\varepsilon_t(1-\varepsilon_t)}$ are all
plotted (bottom). We see that $\varepsilon_t<0.5$ and $E_{t+1}/E_t<1$ 
for all steps, and the exponential cost $E_t$ attenuates exponentially
towards zero.

\htmladdimg{../figures/AdaBoostErrorRate1.png}


{\bf Example 2} In this dataset, 100 training samples of two classes 
(50 each) forms four clusters arranged in the 2-D space as an XOR pattern
as shown figure (top-left), and 100 testing samples of the same distribution
are classified by the AbaBoost algorithm trained by both the coordinate 
descent and PCA methods.

We see that the coordinate descent method performs very poorly in both
the slow convergence (more than 400 iterations) during training and high 
classification error rate (more than 1/3) during testing. The partitioning 
of the 2-D space is an obvious over fitting of the training set instead of
reflecting the actual distribution of the two classes (four clusters). On 
the other hand, the PCA method converges quickly (10 iterations) and 
classifies the testing samples with very low error rate. The space is 
clearly partitioned into two regions corresponding to the two classes.

\htmladdimg{../figures/AdaBoostEx2b.png}
\htmladdimg{../figures/AdaBoostEx2a.png}


%over-fitting: the training samples are well classified with very low
%error but the testing samples are not well classified.


\section{Support Vector machine}

%http://cs229.stanford.edu/notes/cs229-notes3.pdf
%https://www.csie.ntu.edu.tw/~cjlin/papers/bottou_lin.pdf


%This is a variation of the perceptron learning algorithm.

\subsection{Maximal Margin and Support Vectors}

% http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf


\htmladdnormallink{MIT video}{https://www.youtube.com/watch?v=_PwhiWxHK8o}

\htmladdnormallink{Stanford video}{https://www.youtube.com/watch?v=v7H5ks5iDEQ}

The support vector machine (SVM) is a supervised binary classifier 
trained by a dsta set $\{ ({\bf x}_n, y_n),\;\;n=1,\cdots,N\}$ of 
which data sample ${\bf x}_n$, a vector in the d-dimensional feature 
space, belongs to either class $C_+$ if labeled by $y_n=1$ or class 
$C_-$ if labeled by $y_n=-1$. The result of the training process is 
the {\em decision plane} $H_0$ in the feature space, described by the
linear {\em decision equation}:
\begin{equation}
  f({\bf x})={\bf w}^T{\bf x}+b=\sum_{i=1}^d x_i w_i+b=0
\end{equation}
in terms of its normal vector ${\bf w}$ and intercept $b$. When these
two parameters are determined in the training process, any unlabeled
sample ${\bf x}$ is classified into either $C_+$ if it is the positive 
side of $H_0$, or $C_-$ if it is on the negative side of $H_0$:
\begin{equation} 
  \mbox{if} \;\;f({\bf x})={\bf w}^T{\bf x}+b\;
  \left\{ \begin{array}{l} >0 \\ =0\\ <0\end{array} \right.,
  \;\;\;\mbox{then}\;\;
  \left\{ \begin{array}{l} {\bf x}\in C_+ \\ {\bf x}\in H_0\\
    {\bf x}\in C_-\end{array} \right.
\end{equation}

We note that the initial setup of the SVM is actually the same as the
method of \htmladdnormallink{linear regression }{../ch7/node10.html},
which can be treated as a binary classifier if the regression function
$f({\bf x})={\bf w}^T{\bf x}+b$ is thresholded by constant zero 
$f({\bf x})=0$. However, the SVM is actually a much more sophisticated 
and powerful binary classifier as we will see later.

%\htmladdimg{../figures/SVMplot.png}
\htmladdimg{../figures/LinearFunction.png}
%\htmladdimg{../figures/perceptron.png}

The decision equation $f({\bf x})={\bf w}^T{\bf x}+b=0$ can be 
rewritten as 
\begin{equation} 
  p_{\bf w}({\bf x})=\frac{{\bf w}^T{\bf x}}{||{\bf w}||}
  =-\frac{b}{||{\bf w}||}
\end{equation}
representing the projection of any point ${\bf x}\in H_0$ on the decision
plane $H_0$ onto its normal direction ${\bf w}$. The absolute value of
this projection is the distance between $H_0$ and the origin:
\begin{equation} 
  d(H_0,\,{\bf 0})=|p_{\bf w}({\bf x})|=\frac{|b|}{||{\bf w}||}
  \label{DistToOrigin}
\end{equation}
Similarly, the projection of any point ${\bf x}'\notin H_0$ 
off the decision plane $H_0$ onto ${\bf w}$ is 
$p_{\bf w}({\bf x}')={\bf w}^T{\bf x}'/||{\bf w}||$, and the absolute 
value of the difference between these two projections is the distances
from ${\bf x}'$ to $H_0$:
\begin{equation}
  d(H_0,\,{\bf x}')=\bigg| p_{\bf w}({\bf x}')-p_{\bf w}({\bf x}) \bigg|
  =\bigg|\frac{{\bf w}^T{\bf x}'}{||{\bf w}||}-\left(-\frac{b}{||{\bf w}||}\right)\bigg|
  =\frac{|{\bf w}^T{\bf x}'+b|}{||{\bf w}||}
  =\frac{|f({\bf x}')|}{||{\bf w}||}
\end{equation}

\htmladdimg{../figures/SVMprojections.png}

We desire to find the optimal decision plane $H_0$ that separates the 
training samples belonging to the two different classes, assumed to be 
linearly separable, in such a way that the following distance between 
$H_0$ and closest training samples on either sides, called the 
{\em support vectors} and denoted by ${\bf x}_{sv}$, is maximized:
\begin{equation}
  d(H_0,\,{\bf x}_{sv}\in C_+)=d(H_0,\,{\bf x}_{sv}\in C_-)
  =d(H_0,\,{\bf x}_{sv})=\frac{|f({\bf x}_{sv})|}{||{\bf w}||}
\end{equation}
We denote by $H_+$ and $H_-$ the two planes that are parelle to $H_0$ 
and pass through the support vectors ${\bf x}_{sv}$ on either side 
of $H_0$, and assume their corresponding equations to be:
\begin{equation}
  \left\{\begin{array}{ll}
  H_+: & f_+({\bf x})=f({\bf x})-c={\bf w}^T {\bf x}+b-c=0 \\ 
  H_-: & f_-({\bf x})=f({\bf x})+c={\bf w}^T {\bf x}+b+c=0
  \end{array}\right.
  \label{SVMplanes}
\end{equation}
As these equations can be arbitrarily scaled, we can let $c=1$ for 
convenience. Based on Eq. (\ref{DistToOrigin}), the distances from 
$H_+$ and $H_-$ to the origin can be written as 
\begin{equation}
  d(H_{\pm},{\bf 0})=\frac{|b\pm 1|}{||{\bf w}||}
\end{equation}
and their distances to the decision plane $H_0$ is
\begin{equation}
  \bigg| d(H_\pm,{\bf 0})-d(H_0,{\bf 0}) \bigg|
  =\bigg| \frac{|b\pm 1|}{||{\bf w}||}-\frac{|b|}{||{\bf w}||} \bigg|
      =\frac{1}{||{\bf w}||}
\end{equation}
This is called the {\em margin}, which is maximized if $||{\bf w}||$ 
is minimized. 

For these planes to correctly separate all samples in the training set 
$\{({\bf x}_n,\,y_n)\;\;(n=1,\cdots,N)\}$, they have to satisfy the 
following two conditions:
\begin{equation}
  \left\{\begin{array}{ll}
  f({\bf x}_n)-1={\bf w}^T {\bf x}_n+b-1 \ge 0 & \mbox{if $y_n=1$}\\
  f({\bf x}_n)+1={\bf w}^T {\bf x}_n+b+1 \le 0 & \mbox{if $y_n=-1$}
  \end{array}\right.
\end{equation}
which can be combined to become:
\begin{equation}
  y_n\,f({\bf x}_n)=y_n ({\bf w}^T{\bf x}_n +b) \ge 1
  \label{SVMcondition}
\end{equation}
Note that the equality is satisfied by the support vectors on $H_+$ 
or $H_-$, while the inequalities are satisfied by all other samples 
farther away from $H_0$ behind $H_+$ or $H_-$. 

Now the task of finding the optimal decision plane $H_0$ can be 
formulated as a constrained minimization problem:
\begin{eqnarray}
  \mbox{minimize:       } \;\;\;& &
  \frac{1}{2}||{\bf w}||^2 =\frac{1}{2}{\bf w}^T {\bf w}  \nonumber \\
  \mbox{subject to:     } \;\;\; & & 
  y_n ({\bf x}_n^T {\bf w}+b) \ge 1,\;\;\;\;(n=1,\cdots,N)
  \label{SVMprimal}
\end{eqnarray}


{\bf Example:}

\htmladdimg{../figures/svm1.png}

The straight line in 2D space shown above, denoted by $H_0$, 
is described by the following linear equation 
\begin{equation}
  f({\bf x})={\bf w}^T{\bf x}+b=[w_1,w_2]
  \left[ \begin{array}{c} x_1 \\ x_2 \end{array} \right]+b
  =[1, 2]\left[ \begin{array}{c} x_1 \\ x_2 \end{array} \right]-1
  =x_1+2x_2-1=0 
\end{equation}
The distance from $H_0$ to the origin is:
\begin{equation}
  d(H_0,{\bf 0})=\frac{|b|}{||{\bf w}||}=\frac{1}{\sqrt{w_1^2+w_2^2}}
  =\frac{1}{\sqrt{1^2+2^2}}=\frac{1}{\sqrt{5}}=0.447
\end{equation}
which is invariant with respect to any scaling of the equation. 
Consider three points in the space:
\begin{itemize}
\item ${\bf x}_0=[0.5,\;0.25]^T$, $f({\bf x}_0)=0.5+2\times 0.25-1=0$, 
  i.e., ${\bf x}_0$ is on the plane. Its distance to the plane is
  $d({\bf x}_0,H)=f({\bf x}_0)/||{\bf w}||=0$.
\item ${\bf x}_1=[1,\;0.25]^T$, $f({\bf x}_1)=1+2\times 0.25-1=0.5>0$, 
  i.e., ${\bf x}_1$ is above the straight line, its distance to the plane
  is $d({\bf x}_1,H)=f({\bf x}_1)/||{\bf w}||=|0.5|/\sqrt{5}=0.2235$.
\item ${\bf x}_2=[0.5,\;0]^T$, $f({\bf x}_2)=0.5+2\times 0-1=-0.5<0$, i.e., 
  ${\bf x}_2$ is below the straight line, its distance to the plane is
  $d({\bf x}_2,H)=f({\bf x}_2)/||{\bf w}||=|-0.5|/\sqrt{5}=0.2235$.
\end{itemize}

As the two points ${\bf x}_1=[1,\,0.25]^T$ and ${\bf x}_2=[0.5,\,0]^T$
have equal distance to the straight line $f({\bf x})=x_1+2x_2-1=0$, 
for $H_0$, they can be treated as two support vectors ${\bf x}_{sv}$ 
on either side of $H_0$, and 
\begin{equation}
  |f({\bf x}_{sv})|=|{\bf w}^T{\bf x}_1+b|=|{\bf w}^T{\bf x}_2+b|=0.5
\end{equation}
Dividing by $0.5$ on both sides of the decision equation $f({\bf x})=0$,
it is scaled to become
\begin{equation}
  f({\bf x})={\bf w}^T{\bf x}+b=2x_1+4x_2-2=0
\end{equation}
and the two equations diescribing $H_+$ parallel to $H_0$ and passing
through ${\bf x}_1$ and $H_-$ passing through ${\bf x}_2$ are
\begin{eqnarray}
  H_+: && 2x_1+4x_2-2-1=2x_1+4x_2-3=0\nonumber\\
  H_-: && 2x_1+4x_2-2+1=2x_1+4x_2-1=0\nonumber
  \nonumber
\end{eqnarray}
Their distances to the origin are
\begin{equation}
  d(H_+,\,{\bf 0})=\frac{|-3|}{||{\bf w}||}=1.341,\;\;\;\;
  d(H_-,\,{\bf 0})=\frac{|-1|}{||{\bf w}||}=0.447
\end{equation}
and the distance between $H_-$ and $H_+$ is indeed $2/||{\bf w}||$:
\begin{equation}
  d(H_-,\,H_+)=d(H_+,\,{\bf 0})-d(H_-,\,{\bf 0})=\frac{2}{||{\bf w}||}
  =0.894
\end{equation}
twice the distance between $H_0$ and the origin $d(H_0,{\bf 0})=0.447$
found previously.

\htmladdimg{../figures/svm2b.png}

For reasons to be discussed later, instead of directly solving the 
constrained minimization problem in Eq. (\ref{SVMprimal}), now called 
the {\em primal problem}, we actually solve the
\htmladdnormallink{\em dual problem}{../ch3/node13.html}.
Specifically, we first construct the {\em Lagrangian function} of the
primal problem:
\begin{equation}
  L_p({\bf w},b,{\bf \alpha})=\frac{1}{2}{\bf w}^T{\bf w}
  +\sum_{n=1}^N \alpha_n(1-y_n( {\bf w}^T {\bf x}_n+b))
  \label{SVMLagrange}
\end{equation}
where $\alpha_1,\cdots,\alpha_N$ are the {\em Lagrange multipliers},
which is called the {\em primal function}. Here for this minimization 
problem with non-positive constraints, the Lagrangian multipliers are
required to be negative, $\alpha_n$, according to Table \ref{PolarityTable} 
\htmladdnormallink{here}{../ch3/node13.html}, if a minus sign is used 
for the second term. However, to be consistent with most SVM literatures,
we use the positive sign for the second term and require $\alpha_n\ge 0$.
Note that if ${\bf x}_n$ is a support vector on either $H_+$ or $H_-$, 
i.e., the equality constraint $y_n({\bf w}^T{\bf x}_n+b)=1$ holds, then 
$\alpha_n>0$; but if ${\bf x}_n$ is not a support vector, the equality
constraint does not hold, and $\alpha_n=0$.

We next find the minimum (or infimum) as the lower bound of the primal 
function $L_p({\bf w},b)$ in Eq. (\ref{SVMLagrange}), by setting to 
zero its partial derivatives with respect to both $b$ and ${\bf w}$:
\begin{equation}
  \frac{\partial}{\partial b}L_p({\bf w},b)
  =\frac{\partial}{\partial b}  \left[ \frac{1}{2}{\bf w}^T{\bf w}
  +\sum_{n=1}^N \alpha_n(1-y_n( {\bf w}^T {\bf x}_n+b))\right]
  =\sum_{n=1}^N \alpha_n y_n=0	
  \label{constraintAlpha}
\end{equation}
and
\begin{equation}
  \frac{\partial}{\partial {\bf w}}L_p({\bf w},b)
  =\frac{\partial}{\partial {\bf w}} 
  \left[ \frac{1}{2}{\bf w}^T{\bf w}
  +\sum_{n=1}^N \alpha_n(1-y_n( {\bf w}^T {\bf x}_n+b))\right]
  ={\bf w}-\sum_{n=1}^N\alpha_ny_n{\bf x}_n=0,
\end{equation}
i.e.,
\begin{equation}
  {\bf w}=\sum_{n=1}^N \alpha_n y_n {\bf x}_n
  \label{SVMweightvector}
\end{equation}
Substituting these back into the primal function 
$L_p({\bf w},b,{\bf\alpha})$, we get its lower bound as a function
of the Lagrange multipliers $\{\alpha_1,\cdots,\alpha_N\}$, called 
the {\em dual function}:
\begin{eqnarray}
  L_d({\bf \alpha})&=&\inf_{{\bf w},b} L_p({\bf w},b,{\bf \alpha})
    =\inf_{{\bf w},b}\left[\frac{1}{2}{\bf w}^T{\bf w}
  +\sum_{n=1}^N \alpha_n(1-y_n( {\bf w}^T {\bf x}_n+b)) \right]
  \nonumber\\
  &=&\inf_{{\bf w},b}\left[\frac{1}{2}{\bf w}^T{\bf w}
  +\sum_{n=1}^N \alpha_n -{\bf w}^T \sum_{n=1}^N \alpha_ny_n{\bf x}_n
  -b\;\sum_{n=1}^N \alpha_ny_n \right]
  \nonumber\\
  &=&\frac{1}{2} \left(\sum_{m=1}^N \alpha_m y_m {\bf x}_m\right)^T
  \left(\sum_{n=1}^N \alpha_n y_n {\bf x}_n\right)+\sum_{n=1}^N \alpha_n
  -\left(\sum_{m=1}^N \alpha_m y_m {\bf x}_m\right)^T
  \left(\sum_{n=1}^N \alpha_n y_n {\bf x}_n\right)
  \nonumber\\
  &=&\sum_{n=1}^N\alpha_n -\frac{1}{2}
  \sum_{n=1}^N \sum_{m=1}^N \alpha_n \alpha_m y_n y_m
  \left({\bf x}_n^T{\bf x}_m\right)
\end{eqnarray}
To further find the greatest or tightest lower bound of the primal 
function, we need to maximize this dual function $L_d({\bf\alpha})$ 
with respect to the Lagrange multipliers, subject to the constraint 
imposed by Eq. (\ref{constraintAlpha}):
\begin{eqnarray}
  \mbox{maximize:} \;\;\; && L_d({\bf\alpha})
  =\sum_{n=1}^N\alpha_n -\frac{1}{2}
  \sum_{n=1}^N \sum_{m=1}^N \alpha_n \alpha_m y_n y_m
  \left({\bf x}_n^T{\bf x}_m\right)
  ={\bf 1}^T{\bf\alpha}-\frac{1}{2}{\bf\alpha}^T{\bf Q}{\bf\alpha}
  \nonumber \\
  \mbox{subject to:} \;\;\; && \sum_{n=1}^N \alpha_n y_n
  ={\bf y}^T{\bf\alpha}=0, \;\;\;\; \alpha_n\ge 0\;\;\;(n=1,\cdots,N)
  \label{dualProblem}
\end{eqnarray}
where ${\bf Q}$ is an $N$ by $N$ symmetric matrix of which the component
in the mth row and nth column is $Q(m,n)=y_my_n{\bf x}_m^T{\bf x}_n$ 
($m,n=1,\cdots,N$). Now we have converted the primal problem of 
constrained minimization of the primal function $L_p({\bf w},b,{\bf\alpha})$ 
with respect to ${\bf w}$ and $b$ to its dual problem of linearly
constrained maximization of the dual function $L_d({\bf \alpha})$ 
with respect to $\{\alpha_1,\cdots,\alpha_N\}$. This is 
\htmladdnormallink{quadratic programming (QP) problem}{../ch3/node18.html},
solving which we get all Lagrange multipliers 
$\alpha_n\ge 0,\;(n=1,\cdots,N)$.
All training samples ${\bf x}_n$ corresponding to positive Lagrange 
multipliers $\alpha_n>0$ are support vectors, while others corresponding
to $\alpha_n=0$ are not support vectors.

We can now find the normal direction ${\bf w}$ of the optimal decision
plane based on Eq. (\ref{SVMweightvector}):
\begin{equation}
  {\bf w}=\sum_{n=1}^N \alpha_n y_n {\bf x}_n
  =\sum_{n\in sv} \alpha_ny_n{\bf x}_n
  =\sum_{x_n\in C_+,\;n\in sv} \alpha_n{\bf x}_n
  -\sum_{x_n\in C_-,\;n\in sv} \alpha_n{\bf x}_n
  \label{FindW}
\end{equation}
Note that the normal vector ${\bf w}$ is the difference between the
weighted means of the support vectors in classes $C_+$ and $C_-$, and
it determined only by the support vectors.

We can also find $b$ based on the equality of Eq. (\ref{SVMcondition}) 
for any of the support vectors:
\begin{equation}
  y_n \left( {\bf w}^T {\bf x}_n+b\right) = 1\;\;\;\;\;\mbox{or}\;\;\;\;
  {\bf w}^T {\bf x}_n+b = y_n,\;\;\;\;\;\;(n\in sv)
\end{equation}
(recall $y_n^2=1$). Solving the equation for $b$ we get:
\begin{equation}
  b = y_n-{\bf w}^T {\bf x}_n
  =y_n-\sum_{m \in sv} \alpha_m y_m( {\bf x}_m^T{\bf x}_n),\;\;\;\;\;(n\in sv)
  \label{FindB}
\end{equation}
All support vectors should yield the same result. Computationally we 
simply get $b$ as the average of the above for all support vectors.

We note that both ${\bf w}$ and $b$ depend only on the support vectors 
on planes $H_+$ and $H_-$, corresponding to a positive $\alpha_n>0$, 
while all remaining samples corresponding to $\alpha_n=0$ are farther 
away from the decision plane $H_0$, behind either $H_+$ or $H_-$. We 
see that the SVM is solely based on those support vectors in the
training set, once they are identified during the training process, 
all other non-support vectors are irrelevant.

%in the following general form:
%\begin{equation}
%\begin{array}{ll}
%  \mbox{minimize:} && L({\bf \alpha})
%  =\frac{1}{2}{\bf\alpha}^T{\bf Q}{\bf\alpha}+{\bf c}^T{\bf\alpha}\\
%  \mbox{subject to:} && {\bf h}({\bf\alpha})={\bf A\alpha}-{\bf b}={\bf 0},
%  \;\;\;\; {\bf\alpha}\ge {\bf 0}
%  \end{array}
%\end{equation}
%with ${\bf c}=-{\bf 1}$, ${\bf A}={\bf y}^T$ and ${\bf b}={\bf 0}$.

Having obtained ${\bf w}$ and $b$ as shown above as the training process,
any unlabeled sample ${\bf x}$ can be classified into either of the two 
classes depending on whether the following decision function is greater 
or smaller than zero:
\begin{equation}
  f({\bf x})={\bf w}^T{\bf x}+b
  =\left(\sum_{n\in sv}\alpha_ny_n{\bf x}_n\right)^T{\bf x}+b
  =\sum_{n\in sv}\alpha_ny_n\,({\bf x}^T_n{\bf x})+b\;\;\;\;
  \left\{\begin{array}{ll}>0 & {\bf x}\in C_+\\<0 & {\bf x}\in C_-
    \end{array}\right.
  \label{SVMclassification}
\end{equation}
We also get:
\begin{eqnarray}
  ||{\bf w}||^2&=&{\bf w}^T {\bf w}
  =\left(\sum_{n\in sv} \alpha_n y_n {\bf x}^T_n\right)\,
  \left(\sum_{m\in sv} \alpha_m y_m {\bf x}_m\right)
  =\sum_{n\in sv} \alpha_n y_n \sum_{m\in sv} \alpha_m y_m ({\bf x}^T_n{\bf x}_m)
  \nonumber \\
  &=&\sum_{n\in sv} \alpha_n y_n (y_n-b)=\sum_{n\in sv} \alpha_n (1-y_n b)
  =\sum_{n\in sv} \alpha_n - b\sum_{n\in sv} \alpha_n y_n=\sum_{n\in sv} \alpha_n 
\end{eqnarray}
The last equality is due to equality constraints of the dual problem
$\sum_{n=1}^N \alpha_n y_n=0$. Now the margin can be written as:
\begin{equation}
  \frac{1}{||{\bf w}||}=\left(\sum_{n\in sv} \alpha_n\right)^{-1/2}	
\end{equation}

The SVM algorithm for binary classification can now by summarized 
as the following steps:
\begin{itemize}
\item Find the Lagrange multipliers $\{\alpha_1,\cdots,\alpha_N\}$
  by solving the QP problem in Eq. (\ref{dualProblem});  
\item Find the normal vector ${\bf w}$ by Eq. (\ref{FindW});
\item Find the bias $b$ by Eq. (\ref{FindB});
\item Classify unlabeled ${\bf x}$ by Eq. (\ref{SVMclassification}).
\end{itemize}
However, we note that as the last expression of the decision function 
in Eq. (\ref{SVMclassification}) depends only on $\alpha_n$ as well
as ${\bf x}_n$ and $y_n$ in the training set, while the normal vector
${\bf w}$ is never needed. The second step above for calculating 
${\bf w}$ by Eq. (\ref{FindW}) can therefore be dropped.


We prefer to solve this dual problem not only because it is easier 
than the original primal problem, but also, more importantly, for 
the reason that all data points appear only in the form of an inner 
product $ {\bf x}_n^T{\bf x}_m$ in the dual problem, allowing the 
{\em kernel method} to be used, as discussed later. 


The Matlab code for the essential part of the algorithm is listed 
below, where \verb|X| and \verb|y| are respectively the data array 
composed of $N$ training vectors $[{\bf x}_1,\cdots,{\bf x}_N]$ and 
their corresponding labelings $y_1,\cdots,y_N$, and \verb|IPmethod|
is a function that implements the 
\htmladdnormallink{interior point method}{../ch3/node18.html} 
for solving a general QP problem of minimizing 
${\bf x}^T{\bf Qx}/2+{\bf c}^T{\bf x}$ subject to the linear 
equality constraints ${\bf Ax}={\bf b}$ and ${\bf x}\ge {\bf 0}$.

\begin{verbatim}
    [X y]=getData;               % get the training data
    [m n]=size(X);   
    Q=(y'*y).*(X'*X);            % compute the quadratic matrix
    c=-ones(n,1);                % coefficients of linear term
    A=y;                         % coefficient matrix and 
    b=0;                         % constants of linear equality constraints
    alpha=0.1*ones(n,1);         % initial guess of solution

    [alpha mu lambda]=IPmethod(Q,A,c,b,alpha);  % solve QP to find alphas
                                                % by interior point method
    I=find(abs(alpha)>10^(-3));  % indecies of non-zero alphas
    asv=alpha(I);                % non-zero alphas
    Xsv=X(:,I);                  % support vectors
    ysv=y(:,I);                  % their labels
    w=sum(repmat(ysv.*asv,m,1).*Xsv,2);  % normal vector (not needed)
    bias=mean(ysv-w'*Xsv);               % bias
\end{verbatim}

{\bf Example: } The training set contains two classess of 2-D points 
with Gaussian distributions generated based on the following mean vectors 
and covariance matrices:
\begin{equation}
  {\bf m}_-=\left[\begin{array}{c}3.5\\0\end{array}\right],\;\;\;\;
  {\bf S}_-=\left[\begin{array}{cc}1&0.5\\0.5&1\end{array}\right],\;\;\;\;
  {\bf m}_+=\left[\begin{array}{c}0\\3.5\end{array}\right],\;\;\;\;
  {\bf S}_+=\left[\begin{array}{cc}1&0\\0&1\end{array}\right]
\end{equation}
The SVM algorithm finds the weight vector ${\bf w}$ and constant $b$ 
of the optimal boundary between the two classes based on three support 
vectors, all listed below. Note that decision boundary is completely 
dictated by the three support vectors and the margion distance between
them is maximized. 
\begin{equation}
  \begin{array}{c|c|l|r}\hline
    n & \alpha_n & {\bf x}_n=[x_1,\,x_2] & y_n \\\hline\hline
    %  9 &  6.47 & x=[2.19,\, 2.03]  &	-1 \\
    % 20 &  4.67 & x=[0.25,\,-1.43]  &	-1 \\
    % 53 & 11.40 & x=[1.03,\, 0.82]  &	 1 \\\hline
    40 &  0.52 & x=[4.43,\, 2.44]  &	-1 \\
    52 &  3.11 & x=[1.17,\, 0.74]  &	-1 \\
    103 &  3.64 & x=[1.30,\, 1.64]  &	 1 \\\hline
  \end{array},\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
  %{\bf w}=\left[\begin{array}{c}-0.35\\0.40\end{array}\right],\;\;\;\;b=0.21
  %{\bf w}=\left[\begin{array}{c}-4.17\\2.33\end{array}\right],\;\;\;\;b=3.40
  {\bf w}=\left[\begin{array}{r}-1.25\\2.39\end{array}\right],\;\;\;\;\;\;\;\;
  b=-1.31
\end{equation}
\htmladdimg{../figures/SVMexample.png}
%\htmladdimg{../figures/SVMexample1.png}
%\htmladdimg{../figures/SVMexample2.png}

\subsection{Kernel Mapping}

The SVM algorithm above converges only if the data points of the
two classes in the training set are linearly separable. If this is
not the case, we can use the {\em kernel method} to map all data 
points ${\bf x}=[x_1,\cdots,x_d]^T$ from the original d-dimensional 
space into a feature space of higher dimensions:
\begin{equation}
  {\bf x} \Longrightarrow {\bf z}=\phi({\bf x})	
\end{equation}
in which the two classes become linearly separable. This idea is
illustrated by the following simple examples.

{\bf Example 1} 

In 1-D space, classes $C_-=\{x\big|(\alpha\le x\le\beta)\}$ 
and $C_+=\{x\big|(x\le \alpha)\;\mbox{or}\;(x\ge\beta)\}$ are not
linearly separable. By the following mappping from 1-D space to 2-D 
space:
\begin{equation}
  {\bf z}=\phi(x)=\left[\begin{array}{l}z_1\\z_2\end{array}\right]
  =\left[ \begin{array}{c} x \\ (x-(\alpha+\beta)/2)^2 \end{array}\right]
\end{equation}
the two classes can be separated by a threshold in the second
dimension of the 2-D space. 

\htmladdimg{../figures/KernelExamples.png}

{\bf Example 2:} 

The method above can be generalized to higher dimensional space 
such as mapping from 2-D to 3-D space. Consider two classes in
a 2-D space that are not linearly separable:
$C_-=\{{\bf x},\; ||{\bf x}||<d\}$ and 
$C_+=\{{\bf x},\; ||{\bf x}||>d\}$. However, by the 
following mappping from 2-D space to 3-D space:
\begin{equation}
  {\bf z}=\phi({\bf x})=\left[\begin{array}{l}z_1\\z_2\\z_3\end{array}\right]
  =\left[\begin{array}{c}x_1\\x_2\\x_1^2+x_2^2\end{array}\right]
\end{equation}
the two classes can be linearly separated by thresholding in the 
third dimension of the 3-D space.

{\bf Example 3:}

In 2-D space, in the exclusive OR data set, the two classes $C_-$ 
containing points in quadrants I and III, and $C_+$ containing
points in quadrants II and IV are not linearly separable. By the
following mapping from 2-D space to 3-D space:
\begin{equation}
  {\bf z}=\phi({\bf x})=\left[\begin{array}{l}z_1\\z_2\\z_3\end{array}\right]
  =\left[\begin{array}{c}x_1\\x_2\\x_1x_2\end{array}\right]
\end{equation}
the two classes can be separated by a threshold in the third 
dimension of the 3-D space.


{\bf Definition: } A kernel is a function that takes two vectors 
${\bf x}_m$ and ${\bf x}_n$ as arguments and returns the inner product 
of their images ${\bf z}_m=\phi({\bf x}_m)$ and ${\bf z}_n=\phi({\bf x}_n)$:
\begin{equation}	
  K({\bf x}_m,{\bf x}_n)=\phi({\bf x}_m)^T\phi({\bf x}_n) 
  ={\bf z}_m^T{\bf z}_n
\end{equation}

The kernel function takes as input some two vectors ${\bf x}_m$
and ${\bf x}_n$ in the original feature space, and returns the inner 
product ${\bf z}_m$ and ${\bf z}_n$ in the higher dimensional space. 
If the data points in the original space only appear in the form of 
inner product, then the kernel function ${\bf z}=\phi({\bf x})$,
called the kernel-induced {\em implicit} mapping, does not need to
be explicitly specified, and the dimension of the new does not even 
need to be known. 

The following is a set of commonly used kernel functions 
$K({\bf x},\,{\bf x}')$, which can be represented as an 
inner product of two vectors ${\bf z}=\phi({\bf x})$ and 
${\bf z}'=\phi({\bf x}')$ in a higher dimensional space.

\begin{itemize}
\item {\bf linear kernel (no kernel mapping)}

  Assume ${\bf x}=[x_1,\cdots,x_d]^T$, ${\bf x}'=[x'_1,\cdots,x'_d]^T$, 

  \begin{equation}	
    K({\bf x},{\bf x}')={\bf x}^T {\bf x}'=\sum_{i=1}^d x_ix'_i
  \end{equation}

\item {\bf polynomial kernels}

  The binomial theorem states:
  \begin{equation}
  (x+y)^n=\sum_{k=0}^n\left(\begin{array}{c}n\\k\end{array}\right) x^{n-k} y^k
  =\sum_{k=0}^n \frac{n!}{k!(n-k)!}\; x^{n-k} y^k
  \end{equation}
  where the binomial coefficient
  \begin{equation}
  \left(\begin{array}{c}n\\k\end{array}\right)=\frac{n!}{k!(n-k)!}
  \end{equation}
  is the number of ways to distribute $n$ items into two bins ($k$ in one
  and $n-k$ in the other). This result can be generalized to the multinormial
  case:  %  https://arxiv.org/pdf/0904.3664.pdf?
  \begin{equation}
  (x_1+\cdots+x_d)^n=\sum_{\sum_{i=1}^d k_i=n}
  \left(\begin{array}{c}n\\k_1,\cdots,k_d\end{array}\right)x_1^{k_1}\cdots x_d^{k_d}
  =\sum_{\sum_{i=1}^d k_i=n}\frac{n!}{k_1!\cdots k_d!}x_1^{k_1}\cdots x_d^{k_d}
  \end{equation}
  where the multinomial coefficient 
  \begin{equation}
    \left(\begin{array}{c}n\\k_1,\cdots,k_d\end{array}\right)
      =\frac{n!}{k_1!\cdots k_d!}
%      (x_1^{k_1}\cdots x_d^{k_d})
  \end{equation}
  is the number of ways to distribute $n$ balls into $d$ bins with 
  $k_i$ balls in the ith bin (see \htmladdnormallink{here}
  {https://www.statlect.com/mathematical-tools/partitions}), and the 
  summation is over all possible ways to get $d$ non-negative integers 
  $k_1,\cdots,k_d$ that add up to $n$.

  Now consider the homogeneous polynomial kernel for d-dimensional vectors 
  ${\bf x}=[x_1,\cdots,x_d]^T$ defined as
  \begin{eqnarray}
    K({\bf x},{\bf x}')&=&({\bf x}^T{\bf x}')^n=(x_1x'_1+\cdots+x_dx'_d)^n
    \nonumber\\
    &=&\sum_{\sum_{i=1}^d k_i=n}\frac{n!}{k_1!\cdots k_d!}\;
    \left( (x_1x'_1)^{k_1}\cdots (x_dx'_d)^{k_d} \right)
    =\phi({\bf x})^T \phi({\bf x}')={\bf z}^T{\bf z}'
  \end{eqnarray}
  where 
  \begin{equation}
    {\bf z}=\phi({\bf x})=\left[\sqrt{ \frac{n!}{k_1!\cdots k_d!} }
    \left(x_1^{k_1}\cdots x_d^{k_d}\right),\;\left(k_i\ge 0,\;\sum_{i=1}^d k_i=n\right)\right]^T
  \end{equation}

  In particular, when $d=2$ and $n=2$, the polynomial kernel defined over
  2-D vectors ${\bf x}=[x_1,x_2]^T$ is:
  \begin{equation}
    K({\bf x},{\bf x}')=({\bf x}^T{\bf x}')^2=(x_1x'_1+x_2x'_2)^2
    =(x_1 x'_1)^2+2x_1 x'_1 x_2 x'_2+(x_2 x'_2)^2=\phi({\bf x})^T \phi({\bf x}')
    ={\bf z}^T{\bf z}'
  \end{equation}
  where ${\bf z}=\phi({\bf x})=[x_1^2,\,\sqrt{2}x_1x_2,\,x_2^2]$ is a mapping 
  from ${\bf x}$ in 2-D space to ${\bf z}$ in 3-D space.

  A non-homogeneous polynomial kernel is defined as
  \begin{equation}
    K({\bf x},{\bf x}')=(1+{\bf x}^T{\bf x}')^n
  \end{equation}

\item {\bf The radial basis function (RBF) kernel}

  The RBF kernel is defined as
  \begin{equation}
    K({\bf x},{\bf x}')=e^{-||{\bf x}-{\bf x}'||^2/2\sigma^2}	
    =e^{-\gamma||{\bf x}-{\bf x}'||^2}	
  \end{equation}
  where $\gamma=1/2\sigma^2$ is a parameter that can be adjusted to fit
  each specific dataset. This kernel can be wriiten as the inner product of 
  two infinite dimensional vectors (for simplicity, we assume $\sigma=1$):
  \begin{eqnarray}
    K({\bf x},\,{\bf x}')&=&e^{-||{\bf x}-{\bf x}'||^2/2}
    =e^{-||{\bf x}||^2/2}\; e^{-||{\bf x}'||^2/2}\; e^{{\bf x}^T{\bf x}'}
    =e^{-||{\bf x}||^2/2}\, e^{-||{\bf x}'||^2/2}
    \sum_{n=0}^\infty \frac{({\bf x}^T{\bf x}')^n}{n!}
    \nonumber\\
    &=&e^{-||{\bf x}||^2/2} \; e^{-||{\bf x}'||^2/2}\;\sum_{n=0}^\infty \left[ 
    \frac{1}{n!}\sum_{\sum_{i=1}^d k_i=n} \frac{n!}{k_1!\cdots k_d! }
    \left((x_1x'_1)^{k_1}\cdots (x_dx'_d)^{k_d}\right) \right]
    \nonumber\\
    &=&\sum_{n=0}^\infty \sum_{\sum_{i=1}^d k_i=n}
    \left(e^{-||{\bf x} ||^2/2}\; \frac{x _1^{k_1}\cdots x _d^{k_d}}{\sqrt{k_1!\cdots k_d!}}\right)
    \left(e^{-||{\bf x}'||^2/2}\; \frac{x_1^{\prime k_1}\cdots x_d^{\prime k_d}}{\sqrt{k_1!\cdots k_d!}}\right)
    \nonumber\\
    &=&\phi({\bf x})^T \phi({\bf x}')={\bf z}^T{\bf z}'
  \end{eqnarray}
  where
  \begin{equation}
    {\bf z}=\phi({\bf x})=\left[ e^{-||{\bf x}||^2/2}\;\frac{x_1^{k_1}\cdots x_d^{k_d}}{\sqrt{ k_1!\cdots k_d!}},
      \;\left(n=0,\cdots,\infty,\;\sum_{k=1}^nk_i=n\right) \right]^T
  \end{equation}
  is an infinite dimensional vector. In particular, when $d=1$ we have
  \begin{eqnarray}
    K(x,\,x')&=&e^{-(x-x')^2/2}=e^{-x^2/2}\, e^{-x'^2/2}\, e^{xx'}
    =e^{-x^2/2}\, e^{-x'^2/2} \sum_{n=0}^\infty \frac{(xx')^n}{n!}
    \nonumber\\
    &=&\sum_{n=0}^\infty (e^{-x^2/2}\,x^n/\sqrt{n!})\;(e^{-x'^2/2}\,x'^n/\sqrt{n!})
  \end{eqnarray}
  where ${\bf z}=\phi(x)=\left[ e^{-x^2/2}\,x^n/\sqrt{n!},\;(n=0,\cdots,\infty)\right]^T$
  is a mapping from a 1-D space to an infinite dimensional space.


%{\bf Example 3:} 
%\begin{equation}
%  K({\bf x},{\bf z})=K({\bf x},{\bf z})K({\bf x},{\bf x})^{-1/2}K({\bf z},{\bf z})^{-1/2}
%  =\phi({\bf x})^T \phi({\bf z})	
%\end{equation}

\end{itemize}

The method of kernel mapping can be applied to the SVM algorithm 
consider previously as all data points appear in the algorithm are
in the form of an inner product. Specifically, during the training
process, we replace the inner product ${\bf x}_m^T{\bf x}_n$ in both 
Eq. (\ref{dualProblem}) and Eq. (\ref{FindB}) by the kernel functioin 
$K({\bf x}_m,\,{\bf x}_n)$ to get
\begin{eqnarray}
  \mbox{maximize:} \;\;\; && L_d({\bf\alpha})
  =\sum_{n=1}^N\alpha_n -\frac{1}{2}
  \sum_{n=1}^N \sum_{m=1}^N \alpha_n \alpha_m y_n y_m K({\bf x}_n,{\bf x}_m)
  \nonumber \\
  \mbox{subject to:} \;\;\; && \sum_{n=1}^N \alpha_n y_n
  ={\bf y}^T{\bf\alpha}=0, \;\;\;\; \alpha_n\ge 0\;\;\;(n=1,\cdots,N)
  \label{dualProblemKernel}
\end{eqnarray}
and
\begin{equation}
  b=y_n-\sum_{m \in sv} \alpha_m y_m K({\bf x}_m,{\bf x}_n),\;\;\;\;\;(n\in sv)
  \label{FindBKernel}
\end{equation}
We also replace the inner product ${\bf x}^T_n{\bf x}$ in 
Eq. (\ref{SVMclassification}) by $K({\bf x}_n,\,{\bf x})$ for the 
classification of any unlabeled point ${\bf z}=\phi({\bf x})$:
\begin{equation}
  f({\bf z})={\bf w}^T{\bf z}+b
  =\sum_{n\in sv}\alpha_ny_n\,({\bf z}_n^T{\bf z})+b
  =\sum_{n\in sv}\alpha_ny_n\,K({\bf x}_n,{\bf x})+b\;\;\;\;
  \left\{\begin{array}{ll}>0 & {\bf x}\in C_+\\<0 & {\bf x}\in C_-
  \end{array}\right.
  \label{SVMclassificationKernel}
\end{equation}
Again, we note that the normal vector ${\bf w}$ in Eq. (\ref{FindW}) 
never needs to be explicitely calculated. As now both the training
and classification are carried out in some higher dimensional space 
${\bf z}=\phi({\bf x})$, in which the classes are more likely linearly
separable, the classification can be more effectively. More generally, 
the kernel method can be applied to any algorithm, so long as the data
always appear in the form of an inner product.


\subsection{Soft Margin SVM}

When the two classes $C_-$ and $C_+$ are not linearly separable, the 
condition for the optimal hyperplane in Eq. (\ref{SVMcondition}) can 
be relaxed by including an extra error term $\zeta_n \ge 0$:
\begin{equation}
  y_n ({\bf x}_n^T{\bf w} +b) \ge 1-\xi_n,\;\;\;(n=1,\cdots,N)	
\end{equation}
For better classification result, this error $\xi_n$ needs to be 
minimized as well as $||{\bf w}||$. Now the primal problem in 
Eq. (\ref{SVMprimal}) can be modified to the following minimization 
problem with an objective function and two sets of inequality 
(non-negative) constraints:
\begin{eqnarray}
  \mbox{minimize} \;\;\; && {\bf w}^T {\bf w}+C\sum_{n=1}^N \xi_n
  \nonumber \\
  \mbox{subject to} \;\;\; && y_n ({\bf x}_n^T {\bf w}+b)+\xi_n-1 \ge 0,
  \;\;\;\mbox{and}\;\;\;\xi_n \ge 0;\;\;\;(n=1,\cdots,N)
\end{eqnarray}

Here $C$ is a parameter controlling the trade-off between the global 
distribution of the data points of the two classes and the local 
points close to the class boundary. A large $C$ value emphasizes the
minimization of the error term in the objective function, so that the 
decision boundary $H_0$ that maximizes the margin is mostly dictated 
by a small number of local support vectors in the region between the 
two class, including possibly some outliers. The corresponding result 
is similar to that of a hard margin SVM considered previously, with 
the potential problem of overfitting the data. On the other hand, a 
small $C$ value deemphasizes the error term, allowing greater errors
and more data points to become support vectors based on which $H_0$
is determined. The resulting desicion boundary is a better reflection
of the overall distribution of the data points of the two classes. 

\begin{comment}
When $p=2$, the problem is formulated as
\begin{eqnarray}
  &\mbox{minimize} & {\bf w}^T {\bf w}+C\sum_{n=1}^N \xi_n^2
  \nonumber \\
  &\mbox{subject to} & y_n ({\bf x}_n^T {\bf w}+b)+\xi_n-1 \ge 0,
  \;\;\;(n=1,\cdots,N)
  \nonumber
\end{eqnarray}
Note that the condition $\xi_n \ge 0$ can be dropped, as any $\xi_n<0$, 
can be set to zero and the objective function is further reduced.

When $p=1$, the problem is formulated as
\begin{eqnarray}
  &\mbox{minimize} & {\bf w}^T {\bf w}+C\sum_{n=1}^N \xi_n
  \nonumber \\
  &\mbox{subject to}& y_n ({\bf x}_n^T {\bf w}+b) +\xi_n-1\ge 0
  \;\;\;\;\xi_n \ge 0\;\;\;(n=1,\cdots,N)
  \nonumber
\end{eqnarray}
This is called 1-norm soft margin problem. The algorithm based on 1-norm
setup, when compared to 2-norm algorithm, is less sensitive to outliers in 
training data. When the data is noisy, 1-norm method should be used to 
ignore the outliers.

{\bf 2-Norm Soft Margin}

The primal Lagrangian to be minimized is
\begin{equation}
  L_p({\bf w},b,\xi,\alpha)=\frac{1}{2}{\bf w}^T{\bf w}+\frac{C}{2}\sum_{n=1}^N\xi_n^2
  -\sum_{n=1}^N \alpha_n[y_n({\bf w}^T{\bf x}+b)-1+\xi_n]
\end{equation}
Substituting
\begin{equation}
  \frac{\partial L}{\partial {\bf w}}={\bf w}-\sum_{n=1}^N y_n\alpha_n{\bf x}_n=0;
  \;\;\;
  \frac{\partial L}{\partial b}=\sum_{n=1}^N y_n\alpha_n=0;
  \;\;\;
  \frac{\partial L}{\partial \xi}_n=C\xi_n-\alpha_n=0;
\end{equation}
into the primal Lagrangian, we get the dual problem 
\begin{eqnarray}
  & \mbox{maximize} & L_d(\alpha)=\sum_{n=1}^N\alpha_n
  -\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N y_ny_m\alpha_n\alpha_m {\bf x}_m^T{\bf x}_n
  -\frac{1}{2C}\sum_{n=1}^N \alpha_n^2
  \nonumber \\
  && =\sum_{n=1}^N\alpha_n
  -\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N y_ny_m\alpha_n\alpha_m({\bf x}_m^T{\bf x}_n
  +\frac{1}{C}\delta_{mn})
  \nonumber \\
  & \mbox{subject to}	& \sum_{n=1}^N \alpha_n y_n=0,\;\;\;\;
  \alpha_n \ge 0,\;\;\;(n=1,\cdots,N)
  \nonumber
\end{eqnarray}
where $\delta_{mn}$ is the Kronecker delta which is 1 only if $m=n$ but 0 otherwise.

This QP program problem which can be written as
\begin{eqnarray}
  &\mbox{minimize} & L({\bf \alpha})
  =\frac{1}{2}\sum_{n=1}^N \sum_{m=1}^N \alpha_n \alpha_m y_n y_m
  \left({\bf x}_n^T,{\bf x}_m\right)-\sum_{n=1}^N\alpha_n
  =\frac{1}{2}{\bf\alpha}^T{\bf Q}{\bf\alpha}-{\bf 1}^T{\bf\alpha}
  \nonumber \\
  & \mbox{subject to}	& \sum_{n=1}^N \alpha_n y_n=0,\;\;\;\;
  \alpha_n \ge 0,\;\;\;(n=1,\cdots,N)
  \nonumber
\end{eqnarray}
where ${\bf Q}$ is a symmetric matrix with 
$Q(m,n)=y_my_n{\bf x}_m^T{\bf x}_n+\delta_{mn}/C$.

Solving this QP problem for $\alpha_n$, we get all support vectors
${\bf x}_n$ corresponding to $\alpha_n > 0$ that satisfy:
\begin{equation}
  y_n({\bf x}_n^T{\bf w}+b)=1-\xi_n
\end{equation}
Substituting ${\bf w}=\sum_{m\in sv} y_m\alpha_m{\bf x}_m$ into this equation, 
we get
\begin{equation}
  y_n\left(\sum_{m\in sv} y_m\alpha_m({\bf x}_n^T {\bf x}_m)+b\right)=1-\xi_n,
  \;\;\;\;\mbox{i.e.,}\;\;\;\;
  y_n\sum_{m\in sv} y_m\alpha_m({\bf x}_n^T {\bf x}_m)=1-\xi_n-y_nb	
\end{equation}
For the optimal weight ${\bf w}$, we have
\begin{eqnarray}
  ||{\bf w}||^2&=&{\bf w}^T {\bf w}
  =\left(\sum_{n\in sv} \alpha_n y_n {\bf x}^T_n\right)\,
  \left(\sum_{m\in sv} \alpha_m y_m {\bf x}_m\right)
  =\sum_{n\in sv} \alpha_n y_n \sum_{m\in sv} \alpha_m y_m {\bf x}^T_n{\bf x}_m
  \nonumber \\
  &=&\sum_{n\in sv} \alpha_n (1-\xi_n-y_n b)
  =\sum_{n\in sv} \alpha_n-\sum_{n\in sv}\alpha_n \xi_n
  -b\sum_{n\in sv} y_n \alpha_n 	
  \nonumber \\
  &=&\sum_{n\in sv} \alpha_n-\sum_{n\in sv}\alpha_n \xi_n
  =\sum_{n\in sv} \alpha_n -\frac{1}{C}\sum_{n\in sv}\alpha_n^2
  \nonumber
\end{eqnarray}	
The last equation is due to $\xi_n=\alpha_n/C$. The optimal margin is 
\begin{equation}
  \frac{1}{||{\bf w}||}=\left(\sum_{n\in sv} \alpha_n -\frac{1}{C}\sum_{n\in sv}\alpha_n^2\right)^{-1/2} 
\end{equation}

\end{comment}

The constrained minimization problem can be converted into the 
minimization of the corresponding Lagrangian:
\begin{equation}
  L_p({\bf w},b,{\bf\xi},{\bf\alpha},{\bf\mu})
  =\frac{1}{2}{\bf w}^T{\bf w}+C\sum_{n=1}^N\xi_n
  -\sum_{n=1}^N \alpha_n[y_n({\bf w}^T{\bf x}+b)+\xi_n-1]-\sum_{n=1}^N\mu_n\xi_n
  \label{1NormSoftMargin}
\end{equation}
where $\mu_n$ as well as $\alpha_n$ are the Lagrange multipliers for 
the two sets of constraints. As they are non-negative (instead of 
non-positive as in previous section), minus sign is used in front of 
the last two terms for the constraints, so that the Lagrange multipliers 
$\alpha_n$ and $\mu_n$ are still required to be positive (in consistent 
with Table \ref{PolarityTable}). The 
\htmladdnormallink{KKT conditions}{../ch3/node15.html}
(Eq. (\ref{KKTConditions})) of 
this minimization problem are listed below (for all $n=1,\cdots,N$):
\begin{itemize}
\item Stationarity:
  \begin{eqnarray}
    &&\frac{\partial L_p}{\partial {\bf w}}={\bf w}-\sum_{n=1}^N y_n\alpha_n{\bf x}_n=0,
    \;\;\;\;\mbox{i.e.}\;\;\;\;{\bf w}=\sum_{n=1}^N y_n\alpha_n{\bf x}_n;
    \label{KKT1}\\
    &&\frac{\partial L_p}{\partial b}=\sum_{n=1}^N y_n\alpha_n=0;\\
    &&\frac{\partial L_p}{\partial \xi_n}=C-\alpha_n-\mu_n=0;
  \end{eqnarray}

\item Primal feasibility:
  \begin{equation}
    y_n({\bf w}^T{\bf x}+b)+\xi_n-1\ge 0,\;\;\;\;\xi_n\ge 0,
    \label{KKT2}
  \end{equation}
\item Complementarity:
  \begin{equation}
    \alpha_n[y_n({\bf w}^T{\bf x}+b)+\xi_n-1]=0,\;\;\;\;\mu_n\xi_n=0,
    \label{KKT3}
  \end{equation}
\item Dual feasibility:
  \begin{equation}
    \alpha_n\ge 0,\;\;\;\;\;\mu_n\ge 0
    \label{KKT4}
  \end{equation}
\end{itemize}
Depending on the value of $\alpha_n$, there exist three possible cases, 
due to the complementarity (Eq. (\ref{KKT3})):
\begin{itemize}
\item If $\alpha_n=0$, then $\mu_n=C-\alpha_n=C>0$, $\xi_n=0$, and 
  we have
  \begin{equation}
  y_n({\bf w}^T{\bf x}_n+b)-1\ge 0
  \end{equation}
\item If $0<\alpha_n<C$, i.e., $\alpha_n\ne 0$, then we have
  \begin{equation}
  y_n({\bf w}^T{\bf x}_n+b)+\xi_n-1=0
  \end{equation}
  But as $\mu_n=C-\alpha_n>0$, $\xi_n=0$ and the equation above 
  becomes
  \begin{equation}
  y_n({\bf w}^T{\bf x}_n+b)-1=0
  \end{equation}
\item If $\alpha_n=C\ne 0$, then we have 
  \begin{equation}
  y_n({\bf w}^T{\bf x}_n+b)+\xi_n-1=0
  \end{equation}
  But as $\mu_n=C-\alpha_n=0$, $\xi_n\ge 0$ and the equation above
  becomes
  \begin{equation}
  y_n({\bf w}^T{\bf x}_n+b)-1\le 0
  \end{equation}
\end{itemize}
The expression $y_n({\bf w}^T{\bf x}_n+b)-1$ in all three cases
above can be rewritten as
\begin{equation}
  y_n({\bf w}^T{\bf x}_n+b)-1=y_n({\bf w}^T{\bf x}_n+b)-y_n^2
  =y_n[({\bf w}^T{\bf x}_n+b)-y_n]=y_n\,E_n
\end{equation}
where $E_n=({\bf w}^T{\bf x}_n+b)-y_n$ is the error, or the 
difference between the actual and desired output defined in 
Eq. (\ref{SVMerror}), when ${\bf x}_n$ is the input. Now we
can summarize all three cases above to get:
\begin{equation}
  y_n\,E_n \left\{\begin{array}{ll}\ge 0 & \mbox{if }\alpha_n=0\\
  = 0 & \mbox{if }0<\alpha_n<C\\\le 0 & \mbox{if }\alpha_n=C
  \end{array}\right.
  \label{KKTsoftmargin}
\end{equation}
These results derived from the KKT conditions in Eqs. (\ref{KKT1}) 
through (\ref{KKT4}) are an alternative form of the KKT conditions,
which will be used in SMO algorithm to check if the KKT conditions
are violated by any $\alpha_n$.

The dual of the primal problem in Eq. (\ref{1NormSoftMargin}) can be 
obtained by substituting the first set of equations (stationarity) 
into the primal Lagrangian:
\begin{eqnarray}
  & \mbox{maximize} & L_d({\bf\alpha})=\sum_{n=1}^N\alpha_n
  -\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N \alpha_n\alpha_m y_ny_m{\bf x}_m^T{\bf x}_n
  +C\sum_{n=1}^N \xi_n-\sum_{n=1}^N \alpha_n\xi_n-\sum_{n=1}^N  \mu_n\xi_n 
  \nonumber \\
  && =\sum_{n=1}^N \alpha_n
  -\frac{1}{2}\sum_{n=1}^N \sum_{m=1}^N \alpha_n\alpha_my_ny_m {\bf x}_m^T{\bf x}_n
  \nonumber \\
  & \mbox{subject to} & 0 \le \alpha_n \le C,\;\;\;\;
  \sum_{n=1}^N \alpha_n y_n=0
  \label{SVMSM}
\end{eqnarray}
Note that due to the condition $C=\alpha_n+\mu_n$, the Lagrangian 
multipliers $\xi_n$ and $\mu_n$ no longer appear in the Lagrangian 
$L_d$ of the dual problem, which happens to be identical to that of 
the linearly separable problem discussed in previous section. Also, 
since $\alpha_n \ge 0,\;\mu_n \ge 0$ and $C=\alpha_n+\mu_n$, we have 
$0 \le \alpha_n =C-\mu_n \le C$. Having solved this QP problem for 
$\alpha_1,\cdots,\alpha_N$, we get the optimal decision plane in
terms of the normal vector of the decision plane
\begin{equation}
  {\bf w}=\sum_{n=1}^Ny_n\alpha_n{\bf x}_n
  =\sum_{n\in sv} y_n\alpha_n{\bf x}_n
\end{equation}
based on Eq. (\ref{FindW}), and the bais term $b$ based on
Eq. (\ref{FindB}):
\begin{equation}
  b=y_n-\sum_{i\in sv} y_i\alpha_i{\bf x}_i^T{\bf x}_n
  =y_n-\sum_{i\in sv} y_i\alpha_iK({\bf x}_i,{\bf x}_n),
  \;\;\;\;\;(n\in sv)
  \label{FindBkernel}
\end{equation}
where the inner product ${\bf x}_k^T{\bf x}$ is replaced by the 
kernel function $K({\bf x}_k,{\bf x})$ if it is nonlinear. 
Computationally, we take the average of such $b$ values of all 
support vectors as the offset term.

Given ${\bf w}$ and $b$, any unlabeled point ${\bf x}$ can be
classified into either of the two classes depending on whether it
is on the positive or negative side of the decision plane:
\begin{equation}
  {\bf w}^T{\bf x}+b=\sum_{i\in sv} y_i\alpha_i {\bf x}_i^T{\bf x}+b
  =\sum_{i\in sv} y_i\alpha_i K({\bf x}_i,{\bf x})+b\;
  \left\{ \begin{array}{ll}>0, & {\bf x}\in C_+\\
    <0, & {\bf x}\in C_-\end{array}\right.
    \label{ClassifyKernel}
\end{equation}
where the inner product is replaced by a kernel function if it is
nonlinear.


\subsection{Sequential Minimal Optimization (SMO) Algorithm}

% Theory:
%http://www.jmlr.org/papers/volume6/fan05a/fan05a.pdf
%https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf
%http://cs229.stanford.edu/notes/cs229-notes3.pdf
%http://cs229.stanford.edu/materials/smo.pdf
%http://web.cs.iastate.edu/~honavar/smo-svm.pdf
%https://pdfs.semanticscholar.org/8b5e/ab2c9fefe2fb1cc15e755cf7382ffc638f7c.pdf
%http://www.iis.sinica.edu.tw/~kmchung/download/linear.pdf
%http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html

% Code:
%https://www.mathworks.com/matlabcentral/fileexchange/63100-smo--sequential-minimal-optimization-
%https://www.mathworks.com/examples/matlab/community/22570-svm-using-sequential-minimal-optimization-simplified-smo
%https://www.mathworks.com/matlabcentral/fileexchange/63100-smo--sequential-minimal-optimization-?focused=7703443&tab=function
%http://www.cnblogs.com/huadongw/p/4994657.htmla

The sequential minimal optimization (SMO, due to
\htmladdnormallink{John Platt 1998}{https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf}, also see notes
\htmladdnormallink{here}{http://web.cs.iastate.edu/~honavar/smo-svm.pdf})
is a more efficient algorithm for solving the SVM problem, compared with the
generic QP algorithms such as the internal-point method. The SMO algorithm 
can be considered as a method of decomposition, by which an optimization 
problem of multiple variables is decomposed into a series of subproblems 
each optimizing an objective function of a small number of variables,
typically only one, while all other variables are treated as constants 
that remain unchanged in the subproblem. For example, the coordinate 
descent algorithm discussed previously is just such a decomposition method,
which solves a problem in multi-dimensional space by converting it into a 
sequence of subproblems each in a one-dimensional space.


When applying the decomposition method to the soft margin SVM problem, we
could consider optimizing only one variable $\alpha_i$, while treating all
remaining variables $\alpha_1,\cdots,\alpha_{i-1},\alpha_{i+1},\cdots,\alpha_N$
as constants. However, due to the constraint $\sum_{n=1}^N\alpha_ny_n=0$, 
$\alpha_i$ is a linear combination of $N-1$ constants and is therefore 
also a constant. We therefore need to consider two variables at a time, 
here assumed to be $\alpha_i$ and $\alpha_j$, while treating the remaining
$N-2$ variables as constants. The objective function of such a subproblem 
can be obtained by dropping all constant terms independent of the two 
selected variables $\alpha_i$ and $\alpha_j$ in the original objective
function in Eq. (\ref{SVMSM}):
\begin{eqnarray}
  & \mbox{maximize} & L(\alpha_i,\alpha_j)
  =\alpha_i+\alpha_j-\frac{1}{2}\left(
    \alpha_i^2{\bf x}_i^T{\bf x}_i+\alpha_j^2{\bf x}_j^T{\bf x}_j
  +2\alpha_i\alpha_jy_iy_j{\bf x}_i^T{\bf x}_j\right)
  \nonumber\\
  &&-\alpha_iy_i\left(\sum_{n\ne i}\alpha_ny_n{\bf x}_n^T\right){\bf x}_i
    -\alpha_jy_j\left(\sum_{n\ne j}\alpha_ny_n{\bf x}_n^T\right){\bf x}_j
  \nonumber\\
  &=&\alpha_i+\alpha_j-\frac{1}{2}\left(\alpha_1^2 K_{ii}+\alpha_2^2K_{jj}
  +2\alpha_i\alpha_jy_iy_jK_{ij}\right)
  \nonumber\\
  &&-\alpha_iy_i\sum_{n\ne i}\alpha_ny_nK_{ni}
    -\alpha_jy_j\sum_{n\ne j}\alpha_ny_nK_{nj}
  \nonumber\\
  & \mbox{subject to} & 0\le\alpha_i,\alpha_j\le C,\;\;\;\;\;\;
  \sum_{n=1}^N \alpha_ny_n=0
  \label{objective2}
\end{eqnarray}
where all inner products ${\bf x}_m^T{\bf x}_n$ have been replaced 
by the corresponding kernel function 
$K_{mn}=K({\bf x}_m,{\bf x}_n)={\bf z}_m^T{\bf z}_n$ with
${\bf z}_n=\phi({\bf x}_n)$, so that the kernel method can be used.

This maximization problem can be solved iteratively. Given the 
previous values $\alpha_n^{old}\,(n=1,\cdots,N)$, we update the two
selected variables by finding the new values $\alpha_i^{new}$ and
$\alpha_j^{new}$ that maximize $L(\alpha_i,\alpha_j)$, subject to 
the two constraints. 

We rewrite the second constraint as
\begin{equation}
  \alpha_iy_i+\alpha_jy_j=-\sum_{n\ne i,j} \alpha_ny_n
\end{equation}
and multiply both sides by $y_i$ to get
\begin{equation}
  y_i^2\alpha_i+y_iy_j\alpha_j=\alpha_i+s\alpha_j
  =\left(-\sum_{n\ne i,j} \alpha_ny_n\right)y_i
  =\delta,\;\;\;\;\;\mbox{i.e.}\;\;\;\;\;\alpha_i=\delta-s\alpha_j
  \label{2ndConstraint}
\end{equation}
where we have defined $s=y_iy_j$ and 
$\delta=(-\sum_{n\ne i,j} \alpha_ny_n)y_i$, which remains the 
same before and after $\alpha_i$ and $\alpha_j$ are updated:
\begin{equation}
  \alpha_i^{new}+s\alpha_j^{new}=\alpha_i^{old}+s\alpha_j^{old}=\delta,
  \;\;\;\;\;\mbox{i.e.,}\;\;\;\;\;
  \Delta\alpha_i=\alpha_i^{new}-\alpha_i^{old}
  =-s(\alpha_j^{new}-\alpha_j^{old})=-s\Delta\alpha_j
  \label{a1a2const}
\end{equation}

We now consider a closed-form solution for updating $\alpha_i$ and
$\alpha_j$ in each iteration of this two-variable optimization problem.

We rewrite Eq. (\ref{FindW}) as
\begin{equation}
  \sum_{n\ne i,j}\alpha_ny_n{\bf x}_n
  ={\bf w}-\alpha_iy_i{\bf x}_i-\alpha_jy_j{\bf x}_j
\end{equation}
so that the two summations in $L(\alpha_i,\alpha_j)$, now denoted by
$v_k\;(k=1,2)$, can be written as
\begin{eqnarray}
  v_k&=&\sum_{n\ne i,j}\alpha_ny_nK_{nk}
  =\sum_{n\ne i,j}\alpha_ny_nK_{nk}-\alpha_iy_iK_{ik}-\alpha_jy_jK_{jk}
  \nonumber\\
  &=&\left(\sum_{n\ne i,j}\alpha_ny_nK_{nk}+b\right)
  -b-\alpha_iy_iK_{ik}-\alpha_jy_jK_{jk}
  \nonumber\\
  &=&u_k-b-\alpha_iy_iK_{ik}-\alpha_jy_jK_{jk}
\end{eqnarray}
where 
\begin{equation}
  u_k=\sum_{n=1}^N\alpha_ny_nK_{nk}+b
\end{equation}
is the output (Eq. (\ref{SVMclassificationKernel}) corresponding to
${\bf x}_k$ as the input. Now the objective function above can be 
written as:
\begin{eqnarray}
  L(\alpha_i,\alpha_j)&=&\alpha_i+\alpha_j-\frac{1}{2}
  (\alpha_i^2 K_{ii}+\alpha_j^2K_{jj}
  +2s\alpha_i\alpha_jK_{ij})-\alpha_iy_iv_i-\alpha_jy_jv_j
\end{eqnarray}
Our goal is to find the optimal values $\alpha_i^{new}$ and $\alpha_j^{new}$ 
that maximize the objective function $L(\alpha_i,\alpha_j)$. To do so, 
we first express it as a function of $\alpha_j$ alone by substituting
$\alpha_i=\delta-s\alpha_j$ (Eq. (\ref{a1a2const})):
\begin{eqnarray}
  L(\alpha_j)&=&\delta+(1-s)\alpha_j-\frac{1}{2}(\delta-s\alpha_j)^2K_{ii}
  -\frac{1}{2}\alpha_2^2K_{jj}
  \nonumber\\
  &&-s(\delta-s\alpha_2)\alpha_2K_{ij}
  -(\delta-s\alpha_j)y_iv_i-\alpha_jy_jv_j
  \nonumber\\
  &=&\frac{1}{2}(2K_{ij}-K_{ii}-K_{jj})\alpha_j^2
  +[1-s+s\delta(K_{ii}-K_{ij})+y_j(v_i-v_j)]\alpha_j+c
\end{eqnarray}
where scalar $c$ represents all constant terms independent of $\alpha_i$
and $\alpha_j$ and are therefore dropped. Now the objective function 
becomes a quadratic function of the single variable $\alpha_j$. Consider 
the coefficients of this quadratic function:
\begin{itemize}
\item The coefficient of $\alpha_j^2$:
  \begin{equation}
    \frac{1}{2}(2K_{ij}-K_{ii}-K_{jj})=\frac{1}{2}\eta
  \end{equation}
  where $\eta$ is defined as
  \begin{eqnarray}
    \eta&=&2K_{ij}-K_{ii}-K_{jj}
    =2{\bf z}_i^T{\bf z}_j-{\bf z}_i^T{\bf z}_i-{\bf z}_j^T{\bf z}_j
    =-({\bf z}_i-{\bf z}_j)^T({\bf z}_i-{\bf z}_j)
    \nonumber\\
    &=&-||{\bf z}_i-{\bf z}_j||^2\le 0
  \end{eqnarray}
\item The coefficient of $\alpha_j$:
  \begin{eqnarray}
    &&1-s+s\delta(K_{ii}-K_{ij})+y_j(v_i-v_j)
    \nonumber\\
    &=&1-s+(s\alpha_i+\alpha_j)(K_{ii}-K_{ij})
    \nonumber\\
    &&+y_j[(u_i-b-\alpha_iy_iK_{ii}-\alpha_jy_jK_{ij})-
      (u_j-b-\alpha_iy_iK_{ij}-\alpha_jy_jK_{jj})]
    \nonumber\\
    &=&1-s+s\alpha_1(K_{ii}-K_{ij})+\alpha_2(K_{ii}-K_{ij})
    \nonumber\\
    &&+y_j(u_i-u_j)-\alpha_isK_{ii}-\alpha_jK_{ij}
    +\alpha_isK_{ij}+\alpha_jK_{jj}
    \nonumber\\
    &=&y_j^2-s-\alpha_j(2K_{ij}-K_{ii}-2K_{jj})+y_j(u_i-u_j)
    \nonumber\\
    &=&y_j(u_i-y_j-u_j+y_j)-\alpha_j(2K_{ij}-K_{ii}-K_{jj})
    \nonumber\\
    &=&y_j(E_i-E_j)-\alpha_j\eta
  \end{eqnarray}
  where we have defined 
  \begin{equation}
    E_k=u_k-y_k=\sum_{n=1}^N\alpha^{old}_ny_nK_{nk}+b-y_k
    \;\;\;\;\;(k=1,2)
  \end{equation}
  as the difference between the desired output $y_k$ and the 
  actual output based on the previous values of the variables 
  $\alpha_n^{old}\;(n=1,\cdots,N)$.
\end{itemize}
Now the quadratic objective function can be rewritten as:
\begin{equation}
  L(\alpha_j)=\frac{1}{2}\eta\alpha_j^2
  +\left[y_j(E_i-E_j)-\alpha_j^{old}\eta\right]\alpha_j
\end{equation}
Given the previous values $\alpha_n^{old}\;(n=1,\cdots,N)$ in 
the coefficients, we will find $\alpha_j^{new}$ that maximizes
$L(\alpha_j)$. Consider its first and second order derivatives:
\begin{eqnarray}
  \frac{d}{d\alpha_j}L(\alpha_j)&=&\eta\alpha_j+y_j(E_i-E_j)-\alpha_j^{old}\eta
  \nonumber\\
  \frac{d^2}{d\alpha_j^2}L(\alpha_j)&=&\eta\le 0
\end{eqnarray}
As the second order derivative of $L(\alpha_j)$ is $\eta\le 0$, it 
has a maximum. Solving the equation $d\,L(\alpha_j)/d\alpha_j=0$, we 
get $\alpha_j^{new}$ that maximizes $L(\alpha_j)$ based on the old 
value $\alpha_j^{old}$:
\begin{equation}
  \alpha_j^{new}=\alpha_j^{old}+\frac{y_j(E_j-E_i)}{\eta}
  =\alpha_j^{old}+\Delta\alpha_j,
  \;\;\;\;\;\;\;\;
  \Delta\alpha_j=\alpha_j^{new}-\alpha_j^{old}=\frac{y_j(E_j-E_i)}{\eta},
\end{equation}

Note that this is an unconstrained solution, which needs to be modified
if the constraints in Eq.\,(\ref{objective2}) are violated. As shown in
the figure below, $\alpha_i$ and $\alpha_j$ are inside the square of 
size $C$, due to the first constraint $0\le \alpha_i,\;\alpha_j\le C$;
also, due to the second constraint $\alpha_i+s\alpha_j=\delta$
(Eq. (\ref{2ndConstraint})), they have to be on the diagonal line
segment inside the square.

\htmladdimg{../figures/SMOfig1a.png}

The four panels correspond to the four possible cases, depending 
on whether $s=1$ ($y_i=y_j=\pm 1$, panels 1 and 2), or $s=-1$
($y_i=-y_j=\pm 1$, panels 3 and 4), and whether $\delta<C$ (panels
1 and 3) or $\delta>C$ (panels 2 and 4), which can be further 
summarized below in terms of the lower bound $L$ and upper bound 
$H$ of $\alpha_2$: 
\begin{itemize}
\item If $s=1$, then   $\alpha_i+s\alpha_j=\alpha_i+\alpha_j=\delta$,
  \begin{itemize}
  \item if $\delta<C$, then $\min(\alpha_j)=0,\;\max(\alpha_j)=\delta$ 
    (first panel)
  \item if $\delta>C$, then $\min(\alpha_j)=\delta-C,\;\max(\alpha_j)=C$
    (second panel)
  \end{itemize}
  Combining these two cases, we have
  \begin{equation}
    \left\{\begin{array}{l}
    L=\max(0,\;\delta-C)=\max(0,\;\alpha_i+\alpha_j-C)\\
    H=\min(\delta,\;C)=\min(\alpha_i+\alpha_j,\;C)
    \end{array}\right.
  \end{equation}
\item If $s=-1$, then $\alpha_i+s\alpha_j=\alpha_i-\alpha_j=\delta$,
  \begin{itemize}
  \item if $\delta>0$, then $\min(\alpha_j)=0,\;\max(\alpha_j)=C-\delta$
    (third panel)
  \item if $\delta<0$, then $\min(\alpha_j)=-\delta,\;\max(\alpha_j)=C$
    (fourth panel)
  \end{itemize}
  Combining these two cases, we have
  \begin{equation}
    \left\{\begin{array}{l}
    L=\max(0,\;-\delta)=\max(0,\;\alpha_j-\alpha_i)\\
    H=\min(C-\delta,\;C)=\min(C+\alpha_j-\alpha_i,\;C)
    \end{array}\right.
  \end{equation}
\end{itemize}

Now we apply the constraints in terms of $L$ and $H$ to the optimial
solution $\alpha_j^{new}$ obtained previously to get $\alpha_j$ that 
is feasible as well as optmal:
\begin{equation}
  \alpha_j^{new}\Longleftarrow \left\{\begin{array}{cl}
  H & \mbox{if }\alpha_j^{new}\ge H\\ 
  \alpha_j^{new} & \mbox{if } L<\alpha_2^{new} < H\\
  L & \mbox{if } \alpha_j^{new}\le L\end{array}\right.
\end{equation}
We can further find $\alpha_i$ based on Eq. (\ref{a1a2const}):
\begin{equation}
  \alpha_i^{new}=\alpha_i^{old}-s\Delta\alpha_j
  =\alpha_i^{old}-s(\alpha_j^{new}-\alpha_j^{old})
\end{equation}
and update the weight vector based on Eq. (\ref{KKT1}):
\begin{eqnarray}
  {\bf w}^{new}&=&\sum_{n\ne i,j} y_n\alpha_n^{old}{\bf x}_n
  +y_i\alpha_i^{new}{\bf x}_i+y_j\alpha_j^{new}{\bf x}_j
  \nonumber\\
  &=&{\bf w}^{old}-y_i\alpha_i^{old}{\bf x}_i-y_j\alpha_j^{old}{\bf x}_j
  +y_i\alpha_i^{new}{\bf x}_i+y_j\alpha_j^{new}{\bf x}_j
  \nonumber\\
  &=&{\bf w}^{old}+y_i\Delta\alpha_i{\bf x}_i+y_j\Delta\alpha_j{\bf x}_j
  ={\bf w}^{old}+\Delta{\bf w}
\end{eqnarray}
where $\Delta{\bf w}={\bf w}^{new}-{\bf w}^{old}
=y_i\Delta\alpha_i{\bf x}_i+y_j\Delta\alpha_j{\bf x}_j$.
To update $b$, consider:
\begin{eqnarray}
  \Delta E_k&=&E_k^{new}-E_k^{old}=u_k^{new}-u_k^{old}
  \nonumber\\
  &=&\sum_{n=1}^N\alpha_n^{new}y_nK_{nk}+b^{new}
  -\sum_{n=1}^N\alpha_n^{old}y_nK_{nk}-b^{old}
  \nonumber\\
  &=&y_i\Delta\alpha_iK_{ik}+y_j\Delta\alpha_jK_{jk}
  +b^{new}-b^{old} \;\;\;\;\;(k=1,2)
\end{eqnarray}
Consider the following cases:
\begin{itemize}
\item If $0<\alpha_k<C$ for either $k=i$ or $k=j$, then $y_kE_k=0$ 
  according to Eq. (\ref{KKTsoftmargin}). We let $E_k^{new}=0$ and 
  solve the equation above to get:
  \begin{equation}
    b^{new}_k=b^{old}_k-(E_k^{old}+y_i\Delta\alpha_iK_{ik}+y_j\Delta\alpha_jK_{jk})
    \;\;\;\;\;(k=1\;\mbox{or}\;2)
  \end{equation}
\item If $0<\alpha_k<C$ for both $k=i$ and $k=j$, then $b_i^{new}=b_j^{new}$.
\item If $\alpha_k=0$ or $\alpha_k=C$ for both $k=i$ and $k=2$, 
  then $E_k\ne 0$, and $b^{new}_i\ne b^{new}_j$, their average 
  $b^{new}=(b_i^{new}+b_j^{new})/2$ can be used.
\end{itemize}

The computation above for maximizing $L(\alpha_i,\alpha_j)$ is 
carried out iteratively each time when a pair of variables $\alpha_i$ 
and $\alpha_j$ is selected to be updated. Specifically, we select a
variable $\alpha_i$ that violates any of the KKT conditions given in 
Eq. (\ref{KKTsoftmargin}) in the outer loop of the SMO algorithm, 
and then pick a second variable $\alpha_j$ in the inner loop of the 
algorithm, both to be optimized. The selection of these variables can 
be either random (e.g., by following their order), or guided by some 
heuristics, such as always choosing the variable with maximum step 
size $\Delta\alpha_n=\alpha^{new}_n-\alpha^{old}_n$. This iterative 
process is repeated until convergence when the KKT conditions are 
satisfied by all $\alpha_1,\cdots,\alpha_N$ variables, i.e., no 
more variables need to be updated. All data points corresponding to 
$\alpha_n\ne 0$ are the support vectors, based on which we can find 
the offset $b$ by Eq. (\ref{FindBkernel}) and classify any unlabeled 
point ${\bf x}$ by Eq. (\ref{ClassifyKernel}).

The Matlab code for the essential part of the SMO algorithm is listed 
below, based mostly on a \htmladdnormallink{simplified SMO algorithm}{http://cs229.stanford.edu/materials/smo.pdf} and the related
\htmladdnormallink{code}{https://www.mathworks.com/matlabcentral/fileexchange/63100-smo--sequential-minimal-optimization-?s_cid=ME_prod_FX} 
with some modifications. 

In particular, the if-statement 
\begin{verbatim} 
if (alpha(i)<C & yE<-tol) | (alpha(i)>0 & yE>tol) 
\end{verbatim}
checks the three KKT conditions in Eq. (\ref{KKTsoftmargin}), where 
$yE=y_nE_n$ and $tol$ is a small constant. The first half checks if
the first two of the three conditions are violated, while the second
 half checks if the second two of the three conditions are violated. 

\begin{verbatim}

    [X y]=getData;                       % get training data
    [m n]=size(X);                       % size of data
    alpha=zeros(1,n);                    % alpha variables
    bias=0;                              % initial bias
    it=0;                                % iteration index                
    while (it<maxit)                     % number of iterations less than maximum
        it=it+1;
        changed_alphas=0;                % number of changed alphas
        N=length(y);                     % number of support vectors
        for i=1:N                        % for each alpha_i
            Ei=sum(alpha.*y.*K(X,X(:,i)))+bias-y(i);    
            yE=Ei*y(i);
            if (alpha(i)<C & yE<-tol) | (alpha(i)>0 & yE>tol)   % KKT violation
                for j=[1:i-1,i+1:N]        % for each alpha_j not equal alpha_i
                    Ej=sum(alpha.*y.*K(X,X(:,j)))+bias-y(j);
                    ai=alpha(i);         % alpha_i old
                    aj=alpha(j);         % alpha_j old
                    if y(i)==y(j)        % s=y_i y_j=1
                        L=max(0,alpha(i)+alpha(j)-C);
                        H=min(C,alpha(i)+alpha(j));
                    else                 % s=y_i y_j=-1
                        L=max(0,alpha(j)-alpha(i));
                        H=min(C,C+alpha(j)-alpha(i));
                    end
                    if L==H              % skip when L==H
                        continue
                    end
                    eta=2*K(X(:,j),X(:,i))-K(X(:,i),X(:,i))-K(X(:,j),X(:,j));
                    alpha(j)=alpha(j)+y(j)*(Ej-Ei)/eta;   % update alpha_j
                    if alpha(j) > H
                        alpha(j) = H;
                    elseif alpha(j) < L
                        alpha(j) = L;
                    end
                    if norm(alpha(j)-aj) < tol       % skip if no change
                        continue
                    end
                    alpha(i)=alpha(i)-y(i)*y(j)*(alpha(j)-aj);   % find alpha_i
                    bi = bias - Ei - y(i)*(alpha(i)-ai)*K(X(:,i),X(:,i))...
                        -y(j)*(alpha(j)-aj)*K(X(:,j),X(:,i));
                    bj = bias - Ej - y(i)*(alpha(i)-ai)*K(X(:,i),X(:,j))...
                        -y(j)*(alpha(j)-aj)*K(X(:,j),X(:,j));   
                    if 0<alpha(i) & alpha(i)<C
                        bias=bi;
                    elseif 0<alpha(j) & alpha(j)<C
                        bias=bj;
                    else
                        bias=(bi+bj)/2;
                    end
                    changed_alphas=changed_alphas+1;  % one more alpha changed
                end
            end
        end
        if changed_alphas==0             % no more changed alpha, quit
            break
        end
        I=find(alpha~=0);                % indecies of non-zero alphas
        alpha=alpha(I);                  % find non-zero alphas
        Xsv=X(:,I);                      % find support vectors
        ysv=y(I);                        % their corresponding indecies
    end                                  % end of iteration
    nsv=length(ysv);                     % number of support vectors
\end{verbatim}
where \verb|K(X,x)| is a function that takes an $m\times n$ matrix 
${\bf X}=[{\bf x}_1,\cdots,{\bf x}_n]$ and an n-D vector ${\bf x}$ 
and produces the kernel functions $K({\bf x}_i,{\bf x}),\;(i=1,\cdots,n)$
as output.
\begin{verbatim}
function ker=K(X,x)
    global kfunction
    global gm                               % parameter for Gaussian kernel
    [m n]=size(X);
    ker=zeros(1,n);
    if kfunction=='l'
        for i=1:n
            ker(i)=X(:,i).'*x;               % linear kernel
        end
    elseif kfunction=='g'
        for i=1:n
            ker(i)=exp(-gm*norm(X(:,i)-x)); % Gaussian kernel
        end
    elseif kfunction=='p'
        for i=1:n
            ker(i)=(X(:,i).'*x).^3;          % polynomial kernel
        end
    end
end
\end{verbatim}
Having found all non-zero $\alpha_n\ne 0$ and the corresponding
support vectors, we further find the offset or bias term:
\begin{verbatim}
     bias=0;
     for i=1:nsv
        bias=bias+(ysv(i)-sum(ysv.*alpha.*K(Xsv,Xsv(:,i))));
    end
    bias=bias/nsv;
\end{verbatim}
Any unlabeled data point ${\bf x}$ is classified into class $C_+$ or 
$C_-$, depending on whether the following expression is greater or 
smaller than zero:
\begin{verbatim}
    y=sum(alpha.*ysv.*K(Xsv,x))+bias;
\end{verbatim}

{\bf Example 1:} The same training set used previously to test the SVM
with a hard margin is used again to test the SVM with a soft margin.
Two results are shown below, corresponding to two different values 
$C=1.2$ and $C=0.05$ for the weight of the error term $\sum_{n=1}^N\xi_n$
in the objective function. We see that when $C=1.2$ is large, the number
of support vectors (the four solid dots) is small due to the small errors 
$\xi_n$ allowed, and the decision boundary determined by the these support
vectors independent of the rest of the data set (circles) is mostly
dictated by the local points close to the boundary, not necessarily a
good reflection of the global distribution of the data points. This
result is similar to the previous one generated by the hard-margin SVM. 

On the other hand, when $C=0.1$ is small, a greater number of data points 
become support vectors due to the larger (the 13 solid dots) error $\xi_n$
allowed, and the resulting linear decision line determined by these support
vectors reflects global distribution of the data points, and it better 
separates the two classes in terms of their mean positions.

\htmladdimg{../figures/SVMLinearExample.png}


{\bf Example 2:} The classification results for the XOR data set are
shown below with $C=1.2$ and $C=0.05$. As the two classes in the XOR 
data set are not linearly separable, a Gaussian or radial basis function 
(RBF) kernel is used to map the data from 2-D space to an infinite
dimensional space, which is partitionedinto two regions by the hyperplane
in Eq. (\ref{SVMhyperplane}). When $C=1.2$ is large, the decision boundary 
is dictated by a relatively small number of support vectors (solid dots, 
with small error $\xi_n$) and all data points are classified correctly, 
but it is more prone to the overfitting problem, if some of the small 
number of support vectors are outliers due to noise.

One the other hand, when $C=0.05$ is small, the decision boundary is
determined by a greater number of support vectors, better reflecting 
the global distribution of the data points. However, as the local
points close to the boundary are not relatively deemphasized, some
miss-classification occurs, some nine red dots are classified into
the blue region incorrectly.


\htmladdimg{../figures/SVMXORexample.png}

{\bf Example 3:}

\htmladdimg{../figures/SMOExample3.png}

{\bf Example 4:}

\htmladdimg{../figures/SMOExample4.png}


\subsection{Multi-Class Classification}



% Zhe Wang and Xiangyang Xue
%https://link.springer.com/chapter/10.1007%2F978-3-319-02300-7_2

Although the SVM method is inherently a binary classifier, it can
be adapted to classification problems of more than two classes. 

First of all, One-Vs-Rest (1VR) is a method that can be used to
convert any binary classifier, such as the SVM, into a multi-class 
classifier. This is done by converting a K-class problem ($K>2$)
into K binary problems. Specifically, we regroup the $K$ classes 
$C_1,\cdots,C_K$ into two classes $C_+=C_i$ and 
$C_-=\{C_j|j=1,\cdots,K,\;j\ne i\}$, and get the corresponding 
decision function $f({\bf x})$ of the binary problem:
\begin{equation}
  \mbox{if   } f_i({\bf x})\; \left\{\begin{array}{l}>0 \\<0 \end{array}\right.,
  \mbox{   then   } \left\{\begin{array}{l}{\bf x}\in C_+\\
  {\bf x}\in C_-\end{array}\right.
\end{equation}
which represents quantitatively the extent to which a given 
${\bf x}$ belongs to class $C_+=C_i$, instead of $C_-$ containing 
all remaining $K-1$ classes. After this process is carried out $K$ 
times for all $i=1,\cdots,K$, we find the maximum one
\begin{equation}
  f_k({\bf x})=\max\{f_1({\bf x}),\cdots,f_K({\bf x})\},
  \;\;\;\;\;\;(k=1,\cdots,K)
\end{equation}
and classify ${\bf x}$ to class $C_k$.

Alternatively, One-Vs-One (1V1) is another method to convert a
binary classifier to a multi-class classifier. In this method, an
unlabeled ${\bf x}$ is classified to a class which receives the
maximum votes out of the $K(K-1)/2$ binary classifications between 
every pair of the $K$ classes. 

In the following we consider yet another method to adapt the SVM 
is adapted to multi-class problems, by generalizing the decision 
function $f({\bf x})={\bf w}^T{\bf x}+b$ for binary classification
to $K$ functions each for one of the $K$ classes:
\begin{equation}
  f_i({\bf x})={\bf w}_i^T{\bf x}+b_i\;\;\;\;\;(i=1,\cdots,K)
\end{equation}
We can redefine ${\bf x}=[x_0=1,\,x_1,\cdots,x_n]^T$ and 
${\bf w}_i=[w_{i0}=b_i,\,w_{i1},\cdots,w_{in}]^T$, then the 
decision function can be written as $f_i({\bf x})={\bf w}_i^T{\bf x}$,
or in vector form
\begin{equation}
  {\bf f}({\bf x})={\bf W}^T{\bf x}
\end{equation}
where
\begin{equation}
  {\bf W}=[{\bf w}_1,\cdots,{\bf w}_K]
\end{equation}
An unlabeled sample ${\bf x}$ is classified to class $C_i$ if
$f_i({\bf x})=\max(f_1({\bf x}),\cdots,f_K({\bf x}))$. 

The training of the classifier is to find all weight vectors in
${\bf W}$ based on the training dataset. To do so, we first define
a cost function


////


\section{Kernelized Bayes classifier}

The \htmladdnormallink{naive Bayes (maximum likelihood) classification}
{node2.html} is based on a quadratic decision function and is therefore 
unable to classify data that are not quadratically separable. However,
as shown below, the Bayes method can be reformulated so that all data 
points appear in the form of an inner product, and the kernel method 
can be used to map the original space into a higher dimensional space 
in which all groups can be separated even though they are not 
quadratically separable in the original space.

Consider a binary Bayes classifier by which any sample ${\bf x}$ is 
classified into either of the two classes $C_+$ and $C_-$ depending on
whether ${\bf x}$ is on the positive or negative side of the quadratic
decision surface (Eq. (\ref{MLdiscriminant})):
\begin{equation}
  f({\bf x})={\bf x}^T{\bf W}{\bf x}+{\bf w}^T{\bf x}+w
  \left\{\begin{array}{ll}>0, & {\bf x}\in C_+\\<0, &
      {\bf x}\in C_-\end{array}
  \right.
\end{equation}
As show in Eq. (\ref{Www2}), here
\begin{eqnarray}
  {\bf W}&=&-\frac{1}{2}({\bf \Sigma}_+^{-1}-{\bf \Sigma}_-^{-1})	
  \nonumber\\
  {\bf w}&=&{\bf \Sigma}_+^{-1}{\bf m}_+-{\bf \Sigma}_-^{-1}{\bf m}_-
  \nonumber\\	
  w&=&-\frac{1}{2}({\bf m}_+^T{\bf \Sigma}_+^{-1}{\bf m}_+
  -{\bf m}_-^T{\bf \Sigma}_-^{-1}{\bf m}_-)
  -\frac{1}{2}\log\,\frac{|{\bf \Sigma}_+|}{|{\bf \Sigma}_-|}
  +\log\,\frac{P(C_+)}{P(C_-)}
\end{eqnarray}
where ${\bf\Sigma}_+$ and ${\bf\Sigma}_-$ are respectively the 
covariance matrices of the samples in $C_+$ and $C_-$,
\begin{equation}
  {\bf m}_+=\frac{1}{N_+}\sum_{x\in C_+}{\bf x},\;\;\;\;\;
  {\bf m}_-=\frac{1}{N_-}\sum_{x\in C_-}{\bf x}
\end{equation}
are their mean vectors, and $P_+=N_+/N$ and $P_-=N_-/N$ are their prior
probabilities. Specially if ${\bf\Sigma}_+={\bf\Sigma}_-={\bf\Sigma}$ 
and therefore ${\bf W}={\bf 0}$, the quadratic decision surface becomes 
a linear decision plane discribed by 
\begin{equation}
  f({\bf x})={\bf w}^T{\bf x}+w
  =\left[{\bf\Sigma}^{-1}({\bf m}_+-{\bf m}_-)\right]^T{\bf x}+w=0
\end{equation}
in the same form as the decision equation for the support vector machine.
An unlabeled sample ${\bf x}$ is classified into either of the two classes 
depending on on which side of a threshold $-w$ its projection onto the 
normal direction ${\bf w}$ of the decision plane lies.

As matrices ${\bf W}$, ${\bf\Sigma}_+^{-1}$, and ${\bf\Sigma}_-^{-1}$ 
are all symmetric, they can be written in the following
\htmladdnormallink{eigendecomposition}{../algebra/node8.html} form:
\begin{equation}
  {\bf W}={\bf V}{\bf\Lambda}{\bf V}^T={\bf U}{\bf U}^T, \;\;\;\;
  {\bf\Sigma}_+^{-1}={\bf V}_+{\bf\Lambda}_+{\bf V}_+^T={\bf U}_+{\bf U}_+^T, 
  \;\;\;\;
  {\bf\Sigma}_-^{-1}={\bf V}_-{\bf\Lambda}_-{\bf V}_-^T={\bf U}_-{\bf U}_-^T
\end{equation}
where ${\bf U}={\bf V\Lambda}^{1/2},\;{\bf U}_+={\bf V}_+{\bf\Lambda}_+^{1/2}$
and ${\bf U}_-={\bf V}_-{\bf\Lambda}_-^{1/2}$. We can write vector ${\bf w}$ 
as:
\begin{equation}
  {\bf w}={\bf \Sigma}_+^{-1}{\bf m}_+-{\bf \Sigma}_-^{-1}{\bf m}_-
  ={\bf U}_+{\bf U}_+^T\frac{1}{N_+}\sum_{{\bf x}_+\in C_+}{\bf x}_+
  -{\bf U}_-{\bf U}_-^T\frac{1}{N_-}\sum_{{\bf x}_-\in C_-}{\bf x}_-
\end{equation}
Any unlabeled sample ${\bf x}$ can now be classified into either 
of the two classes based on its decision function:
\begin{eqnarray}
  f({\bf x})&=&{\bf x}^T{\bf W}{\bf x}+{\bf w}^T{\bf x}+w
  \nonumber\\
  &=&{\bf x}^T{\bf UU}^T{\bf x}
  +\left(\frac{1}{N_+}\sum_{{\bf x}_+}{\bf x}_+^T{\bf U}_+{\bf U}_+^T\right){\bf x}
  -\left(\frac{1}{N_-}\sum_{{\bf x}_-}{\bf x}_-^T{\bf U}_-{\bf U}_-^T\right){\bf x}
  +w
  \nonumber\\
  &=&{\bf z}^T{\bf z}
  +\frac{1}{N_+}\sum_{{\bf z}_{++}}\left({\bf z}_{++}^T{\bf z}_+\right)
  -\frac{1}{N_-}\sum_{{\bf z}_{--}}\left({\bf z}_{--}^T{\bf z}_-\right)+w
\end{eqnarray}
where 
\begin{equation}
  \left\{\begin{array}{l}{\bf z}_{++}={\bf U}_+^T{\bf x}_+\;\;({\bf x}_+\in C_+)\\
  {\bf z}_{--}={\bf U}_-^T{\bf x}_-\;\;({\bf x}_-\in C_-)\end{array}\right.,
  \;\;\;\;\;
  \left\{\begin{array}{l}
           {\bf z}={\bf U}^T{\bf x}\\
           {\bf z}_+={\bf U}_+^T{\bf x}\\
           {\bf z}_-={\bf U}_-^T{\bf x}
  \end{array}\right.
\end{equation}
As all data points appear in the form of an inner product, the kernel 
method can be used by replacing all inner products in the decision
function by the corresponding kernel functions:
\begin{equation}
  f({\bf x})=K({\bf z},{\bf z})
  +\frac{1}{N_+}\sum_{{\bf z}_{++}}K({\bf z}_{++},{\bf z}_+)
  -\frac{1}{N_-}\sum_{{\bf z}_{--}}K({\bf z}_{--},{\bf z}_-)+b
  =p({\bf x})+b
  \label{TypeIII}
\end{equation}
This is the decision function in the new higher dimensional space,
where $p({\bf x})$ is defined as a function composed of all terms 
in $f({\bf x})$ except the last offset term $b$, which is to replace 
$w$ in the original space. To find this $b$, we first map all 
training samples into a 1-D space $x_n=p({\bf x}_n),\;(n=1,\cdots,N)$
and sort them together with their corresponding labelings 
$y_1,\cdots,y_N$, and then search through all $N-1$ possible 
ways to partition them into two groups indexed respectively by 
$1,\cdots,k$ and $k+1,\cdots,N$ to find the optimal $k$ corresponding 
to the maximum labeling consistency measured by
\begin{equation}
  \bigg|\sum_{n=1}^k y_n\bigg|+\bigg|\sum_{n=k+1}^N y_n\bigg|,
  \;\;\;\;\;\;(k=1,\cdots,N-1)
  \label{LabelingConsistency}
\end{equation}
The middle point $(x_k+x_{k+1})/2$ between $x_k$ and $x_{k+1}$ is used
as the optimal threshold to separate the training samples into two 
classes in the 1-D space, i.e., the offset is $b=-(x_k+x_{k+1})/2$. 
Now the unlabeled point ${\bf x}$ can be classified into either of 
the two classes $C_+$ and $C_-$:
\begin{equation}
  p({\bf x})+b\;\left\{\begin{array}{ll}>0, & {\bf x}\in C_+\\
    <0, & {\bf x}\in C_-\end{array}\right.
\end{equation}

In general, when all data points are kernel mapped to a higher 
dimensional space, the two classes can be more easily separated, 
even by a hyperplane based on the linear part of the decision
function without the second order term. This allows the assumption
that the two classes have the same covariance matrix so that
${\bf W}=-({\bf\Sigma}_+-{\bf\Sigma}_-)/2={\bf 0}$ and the second 
order term is dropped. This is the justification for the following
two special cases:
\begin{itemize}
\item If we approximate both ${\bf\Sigma}_+$ and ${\bf\Sigma}_-$
  by their average ${\bf\Sigma}=({\bf\Sigma}_++{\bf\Sigma}_-)/2$,
  then ${\bf W}={\bf 0}$ and the decision function of any ${\bf x}$ 
  becomes
  \begin{eqnarray}
    f({\bf x})&=&{\bf w}^T{\bf x}+w
    =\left[{\bf\Sigma}^{-1}({\bf m}_+-{\bf m}_-)\right]^T{\bf x}+w
    \nonumber\\
    &=&\left[{\bf UU}^T\left(\frac{1}{N_+}\sum_{{\bf x}_+}{\bf x}_+
      -\frac{1}{N_-}\sum_{{\bf x}_-}{\bf x}_-\right)\right]^T{\bf x}+w
    \nonumber\\
    &=&\frac{1}{N_+}\sum_{{\bf x}_+}{\bf x}_+^T{\bf UU}^T{\bf x}
      -\frac{1}{N_-}\sum_{{\bf x}_-}{\bf x}_-^T{\bf UU}^T{\bf x}+w
    \nonumber\\
    &=&\frac{1}{N_+}\sum_{{\bf z}_+}{\bf z}_+^T{\bf z}
    -\frac{1}{N_-}\sum_{{\bf z}_-}{\bf z}_-^T{\bf z}+w
    \label{TypeII}
  \end{eqnarray}
  where ${\bf UU}^T={\bf\Sigma}^{-1}$, ${\bf z}_+={\bf U}^T{\bf x}_+$,
  ${\bf z}_-={\bf U}^T{\bf x}_-$, and ${\bf z}={\bf U}^T{\bf x}$. 
  This decision function can be converted to the following if the 
  kernel method is used:
  \begin{equation}  
    f({\bf x})=\frac{1}{N_+}\sum_{{\bf z}_+}K({\bf z}_+,{\bf z})
    -\frac{1}{N_-}\sum_{{\bf z}_-}K({\bf z}_-,{\bf z})+b
  \end{equation}  

\item More specially, if ${\bf\Sigma}_+={\bf\Sigma}_-={\bf I}$, then 
  the decision function above becomes
  \begin{eqnarray}
    f({\bf x})&=&{\bf w}^T{\bf x}+w
    =\left( {\bf m}_+-{\bf m}_- \right)^T {\bf x}+w
    \nonumber\\
    &=&\frac{1}{N_+}\sum_{{\bf x}_+}{\bf x}_+^T{\bf x}
    -\frac{1}{N_-}\sum_{{\bf x}_-}{\bf x}_-^T{\bf x}+w
    \label{TypeI1}
  \end{eqnarray}
  which can be converted to the following if the kernel method is used:
  \begin{equation}  
    f({\bf x})=\frac{1}{N_+}\sum_{{\bf x}_+}K({\bf x}_+,{\bf x})
    -\frac{1}{N_-}\sum_{{\bf x}_-}K({\bf x}_-,{\bf x})+b
    \label{TypeI2}
  \end{equation}  

  \begin{comment}
  Alternatively, ${\bf w}={\bf m}_+-{\bf m}_-$ can also be obtained
  as the sum of all training samples with their mean ${\bf m}$ removed, 
  weighted by their corresponding labelings $y_n=\pm 1$ and scaled by 
  $s=N/2N_+N_-$:
  \begin{eqnarray}
    s\,\sum_{n=1}^Ny_n({\bf x}_n-{\bf m})
    &=&\frac{N}{2N_+N_-}\sum_{x\in C_+}({\bf x}-{\bf m})
    -\frac{N}{2N_+N_-}\sum_{x\in C_-}({\bf x}-{\bf m})
    \nonumber\\
    &=&\frac{N}{2N_-}({\bf m}_+-{\bf m})-\frac{N}{2N_+}({\bf m}_--{\bf m})
    \nonumber
  \end{eqnarray}
  Substituting 
  \begin{equation}
    {\bf m}=\frac{1}{N}\sum_{n=1}^N{\bf x}
    =\frac{N_+}{N}\left(\frac{1}{N_+}\sum_{x\in C_+}{\bf x}\right)
    +\frac{N_-}{N}\left(\frac{1}{N_-}\sum_{x\in C_-}{\bf x}\right)
    =\frac{N_+}{N}{\bf m}_++\frac{N_-}{N}{\bf m}_-
  \end{equation}
  into the expression above, we also get ${\bf w}$:
  \begin{equation}
    s\,\sum_{n=1}^Ny_n({\bf x}_n-{\bf m})={\bf m}_+-{\bf m}_-={\bf w}
  \end{equation}
  The decision function is
  \begin{equation}
    f({\bf x})={\bf w}^T{\bf x}'+w
    =s\,\sum_{n=1}^Ny_n({\bf x}_n-{\bf m})^T{\bf x}'+w
  \end{equation}
  which can be converted to the following by the kernel method:
  \begin{equation}
    s\,\sum_{n=1}^Ny_n K({\bf x}_n-{\bf m},{\bf x}')+b
  \end{equation}
  \end{comment}

\end{itemize}

Note that if the kernel method is used to replace an inner product by 
a kernel function $K({\bf x},{\bf x}')=\phi({\bf x})^T\phi({\bf x}')$,
we need to map all data points to $\phi({\bf x}_n),\;\;(n=1,\cdots,N)$
in the higher dimensional space, instead of only mapping their means 
$\phi({\bf m}_+)$ and $\phi({\bf m}_-)$, because the mapping of a sum 
is not equal to the sum of the mapped points if the kernel is not linear:
\begin{equation}
  \phi({\bf m}_+-{\bf m}_-)\ne \phi({\bf m}_+)-\phi({\bf m}_-), 
  \;\;\;\;\;\;\;\;
  \phi({\bf m}_\pm)=\phi\left(\frac{1}{n_\pm}\sum_{{\bf x}\in C_\pm}{\bf x}\right)
  \ne \frac{1}{n_\pm}\sum_{{\bf x}\in C_\pm} \phi({\bf x})
\end{equation}
The three cases above are summarized below:
\begin{itemize}
\item Type I, Eq. (\ref{TypeI2}) 
  \begin{equation}
  f({\bf x})=\frac{1}{N_+}\sum_{{\bf x}_+}K({\bf x}_+,{\bf x})
  -\frac{1}{N_-}\sum_{{\bf x}_-}K({\bf x}_-,{\bf x})+b
  \end{equation}
\item Type II,  Eq. (\ref{TypeII}) 
  \begin{equation}
    f({\bf x})=\frac{1}{N_+}\sum_{{\bf z}_+}K({\bf z}_+,{\bf z})
    -\frac{1}{N_-}\sum_{{\bf z}_-}K({\bf z}_-,{\bf z})+b
  \end{equation}
\item Type III, Eq. (\ref{TypeIII}) 
  \begin{equation}
  f({\bf x})={\bf z}^T{\bf z}
  +\frac{1}{N_+}\sum_{{\bf z}_{++}}K({\bf z}_{++},{\bf z}_+)
  -\frac{1}{N_-}\sum_{{\bf z}_{--}}K({\bf z}_{--},{\bf z}_-)+b
  \end{equation}
\end{itemize}

The Matlab code for the essential part of the algorithm is listed
below. Given \verb|X| and \verb|y| for the data array composed of $N$ 
training vectors ${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$ and their 
corresponding labelings ${\bf y}=[y_1,\cdots,y_N]$, the code carries 
out the training and then classifies any unlabeled data point into 
either of the two classes. Parameter \verb|type| selects any one of 
the three different versions of the algorithm, and the function 
\verb|K(X,x)| returns a 1-D array containing all $N$ kernel functions
$K({\bf x}_i,{\bf x}),\;(i=1,\cdots,N)$ of the column vectors in 
${\bf X}$ and vector ${\bf x}$.

\begin{verbatim}
    X=getData;
    [m n]=size(X);             
    X0=X(:,find(y>0));   n0=size(X0,2);     % separate C+ and C-
    X1=X(:,find(y<0));   n1=size(X1,2);   
    m0=mean(X0,2);    C0=cov(X0');          % calculate mean and covariance
    m1=mean(X1,2);    C1=cov(X1');    
    if type==1
        for i=1:n
            x(i)=sum(K(X0,X(:,i)))/n0-sum(K(X1,X(:,i)))/n1;
        end
    elseif type==2
        C=inv(C0+C1);   [V D]=eig(C);   U=(V*D^(1/2))';
        Z=U*X;  Z0=U*X0;   Z1=U*X1;
        for i=1:n
            x(i)=sum(K(Z0,Z(:,i)))/n0-sum(K(Z1,Z(:,i)))/n1;
        end
    elseif type==3
        C0=inv(C0);   C1=inv(C1);   W=-(C0-C1)/2;     
        [V D]=eig(W);     U=(V*D^(1/2)).';    Z=U*X;  
        [V0 D0]=eig(C0);  U0=(V0*D0^(1/2))';  Z0=U0*X;  Z00=U0*X0;
        [V1 D1]=eig(C1);  U1=(V1*D1^(1/2))';  Z1=U1*X;  Z11=U1*X1;
        for i=1:n
            x(i)=K(Z(:,i),Z(:,i))+sum(K(Z00,Z0(:,i)))/n0-sum(K(Z11,Z1(:,i)))/n1;
        end
    end
    [x I]=sort(x);   y=y(I);    % sort 1-D data together with their labelings
    smax=0;
    for i=1:n-1                 % find optimal threshold value b
        s=abs(sum(y(1:i)))+abs(sum(y(i+1:n)));
        if s>smax
            smax=s;  b=-(x(i)+x(i+1))/2;
        end
    end
\end{verbatim}

Note that ${\bf W}=-({\bf\Sigma}_0^{-1}-{\bf\Sigma}_1^{-1})/2$ may be 
either positive or negative definite, and its eigenvalue matrix ${\bf D}$
may contain negative values and ${\bf D}^{1/2}$ may contain complex values. 
Given any unlabeled data point ${\bf x}$, the code below is carried out
\begin{verbatim}
    if type==1
        y(i)=sum(K(X0,x))/n0-sum(K(X1,x))/n1+b;
    elseif type==2
        Z=U*x;
        y(i)=sum(K(Z0,z))/n0-sum(K(Z1,z))/n1+b;
    elseif type==3
        z=U*x;    z0=U0*x;   z1=U1*x;
        y(i)=K(z,z)+sum(K(Z00,z0))/n0-sum(K(Z11,z1))/n1+b;
    end
\end{verbatim}
to classify ${\bf x}$ into either $C_+$ if $y>0$, or $C_-$ if $y<0$.

In comparison with the SVM method, which requires solving a QP problem 
by certain iterative algorithm (either the interior point method or the
SMO method), the kernel based Bayes method is closed-form and therefore 
extremely efficient computationally. Moreover, as shown in the examples 
below, this method is also highly effective as its classification results 
are comparable and offen more accurate than those of the SVM method. 

We now show a few examples to test all three types of the kernel based 
Bayes method based on a set of simulated 2-D data. Both linear kernel
and RBF kernel $K({\bf x},{\bf y})=\exp(-\gamma||{\bf x}-{\bf y}||)$.
The value of the parameter $\gamma$ used in the examples is 5, but it 
can be fine tuned in a wide range (e.g., $1<\gamma<9$ to make proper
trade-off between accuracy and avoiding overfitting. The performances 
of these method are also compared with the SVM method implemented by 
the Matlab function: 
\begin{verbatim}
fitcsvm(X',y,'Standardize',true,'KernelFunction','linear','KernelScale','auto'))
\end{verbatim}

{\bf Example 1:} Based on some 2-D training datasets, four different 
binary classifiers are tested. The correct rates of each of the four
methods are listed below, together with the corresponding partitionings
of the 2-D space shown in the figures.

\begin{equation}
\begin{array}{l||c||c|c|c|c} \hline
& Kernel & \mbox{Matlab SVM} & \mbox{Kernel Bayes I} & \mbox{Kernel Bayes II} & \mbox{Kernel Bayes III}\\ \hline\hline
\mbox{Test 1} & \mbox{linear} & 93.0\% & 88.0\% & 94.0\% & 94.0\%\\\hline
\mbox{Test 2} & \mbox{linear} & 73.0\% & 80.0\% & 79.5\% & 96.5\%\\\hline
\mbox{Test 3} & \mbox{RBF}    & 98.5\% & 100\% & 100\% & 100\%\\\hline
\end{array}
\end{equation}

\begin{itemize}
\item Test 1: Two sets of data are generated based on the following
  mean vectors and covarience matrices:
  \begin{equation}
    {\bf m}_+=\left[\begin{array}{r}-1\\0\end{array}\right],\;\;\;\;\;\;
    {\bf m}_-=\left[\begin{array}{r}+1\\0\end{array}\right],\;\;\;\;\;\;
    {\bf\Sigma}_+={\bf\Sigma}_-
    =3\times \left[\begin{array}{cc}1&0.5\\0.5&0.5\end{array}\right]
  \end{equation}
  The linear kernel is used for all methods. The kernel Bayes methods 
  II and III achieve the best result $(94\%)$, slightly better than that
  of the standard SVM $(93\%)$. But the kernel Bayes method I performs
  poorly $(84.7\%)$, as it is based only on the means of the two classes
  without taking into consideration their covariances representing the
  distribution of the data points.
  
  \htmladdimg{../figures/myMethodEx2a.png}

\item Test 2: Two sets of data not linearly separable are classified
  by the four methods all based on the linear kernel. The kernel Bayes
  III achieves the best result $(96.5\%)$, due to its quadratic term 
  in the decision function, the kernel Bayes I and II perform much
  more poorly, but still slightly better than the SVM method.

  \htmladdimg{../figures/myMethodEx2b.png}

\item Test 3: The same dataset as above is classified by the methods 
  but now based on the RBF kernel. While the SVM performs performs
  reasonably well $(98.5\%)$, all three versions of the kernel Bayes 
  method (with $\gamma=5$) achieve the perfect result with all points 
  of the two classes completely separate. However, the partitioning
  of the space by the kernel Bayes III is highly fragmented, showing
  a sign of overfitting.
  
  \htmladdimg{../figures/myMethodEx2c.png}

\end{itemize}


%\htmladdimg{../figures/myMethodEx1.png}


{\bf Example 2:} The three datasets are generated using the Matlab
code on 
\htmladdnormallink{this site}{https://www.mathworks.com/help/stats/support-vector-machines-for-binary-classification.html}. 
A parameter value $\gamma=5$ is used for the three versions of the 
kernel Bayes method.

\begin{equation}
  \begin{array}{l||c||c|c|c|c} \hline
    & Kernel & \mbox{Matlab SVM} & \mbox{Kernel Bayes I} & \mbox{Kernel Bayes II} & \mbox{Kernel Bayes III}\\ \hline\hline
    \mbox{Test 1} & \mbox{Linear} & 58.75\% & 60.0\% & 60.25\% & 97.0\%\\\hline
    \mbox{Test 2} & \mbox{RBF} & 97.75\% & 98.50\% & 98.50\% & 99.50\%\\\hline
    \mbox{Test 3} & \mbox{RBF} & 98.0\% & 100\% & 100\% & 100\%\\\hline
    \mbox{Test 4} & \mbox{RBF} & 96.0\% & 98.5\% & 95.5\% & 97.0\%\\\hline
  \end{array}
\end{equation}

\begin{itemize}
\item Test 1: Exclusive OR dataset, linear kernel 

  As the data are not linearly separable, all methods performed poorly
  except kernelized Bayes Type III with a second order term in the 
  decison function. 

  \htmladdimg{../figures/myMethodEx3d.png}

\item Test 2: Exclusive OR dataset, RBF kernel

  All four methods performed very well, but all three variations of
  the kernelized Bayes method achieved higher correct rates than
  the SVM.

  \htmladdimg{../figures/myMethodEx3b.png}

\item Test 3: Cocentric ring dataset, RBF kernel
  
  \htmladdimg{../figures/myMethodEx3a.png}

\item Test 4: Multi-cluster dataset, RBF kernel

  \htmladdimg{../figures/myMethodEx3c.png}

\end{itemize}

{\bf Example 3:} The classification results of Fisher's iris data by
the SVM method (Matlab function \verb|fitcsvm|) and the kernel Bayes 
methods are shown below. This is an example used to illustrate the
SVM method in the 
\htmladdnormallink
{documentation of fitcsvm}{https://www.mathworks.com/help/stats/fitcsvm.html}. 

In this example only two (3rd and 4th) of the four features are used, with 
half of the samples used for training while the other half for testing. 
Note that the second class (setosa in green) and third class (versicolor 
in blue) not linearly separable can be better separated by the kernel Bayes 
method.

\htmladdimg{../figures/IrisClassification.png}


\section{Gaussian Process Classification (GPC)}



\subsection{Gaussian Process Classifier -- Binary}

% https://www.informatik.uni-ulm.de/ni/ANNPR10/InvitedTalks/AtiyaANNPR.pdf
% http://www.jmlr.org/papers/volume6/kuss05a/kuss05a.pdf

In Chapter 7 we considered logistic regression based on the linear 
regression function $y=f({\bf x})={\bf x}^T{\bf w}$, of which the 
parameter ${\bf w}$ is to be estimated based on the observed data 
${\cal D}=\{ ({\bf x}_n,\,y_n)|n=1,\cdots,N\}$, where each training 
sample ${\bf x}_n$ is labeled by $y_n$ to belong to either class $C_0$
if $y_n=0$ or class $C_1$ if $y_n=1$. We also considered the method of
\htmladdnormallink{Gaussian process regression (GPR)}{../ch7/node17.html},
based on the assumption that the regression function $f({\bf x})$ is
a nonlinear Gaussian process. 

We will now consider the method of
{\em Gaussian process classification (GPC)}, a binary classifier
based on both logistic regression and GPR. Compared to all 
previously discussed binary classifiers based on the linear 
function $f({\bf x})={\bf x}^T{\bf w}$, GPC is a more powerful 
classifier capable of non-linear classification. As a supervised
method, GPC is based on a training set 
${\cal D}=\{ ({\bf x}_n,y_n)|n=1,\cdots,N\}$, of which each 
sample ${\bf x}_n$ belongs to either class $C_-$ if $y_n=-1$
or $C_+$ if $y_n=1$. This class labeling different from that 
of the logistic regression is for the convenience of the GPC 
algorithm to be discussed below.

In logistic regression, we first find the parameter ${\bf w}$ of the
linear model $f({\bf x})={\bf x}^T{\bf w}$ based on the training set
${\cal D}$, and then map $f({\bf x}_*)$ of any unlabeled test sample
${\bf x}_*$ by the logistic function to the probability
$p(y_*=1|{\bf x}_*,{\cal D})=\sigma(f({\bf x}_*))=\sigma({\bf x}^T_*{\bf w})$,
based on which ${\bf x}_*$ is classified into either of the two 
classes by Eq. (\ref{LogisticRegClassify}). Here in GPC, same 
as logistic regression, we also classify ${\bf x}_*$ based on 
$p(y_*=1|{\bf x}_*,{\cal D})$, but now this probability is based 
on a nonlinear Gaussian process $f({\bf x})$ instead of a linear
function, to be obtained as the expectation of $\sigma(f({\bf x}_*))$
with respect to $f({\bf x})$: 
\begin{eqnarray}
  p(y_*=1|{\bf x}_*,{\cal D})
  &=&\int p(y_*=1|{\bf x}_*,\,f)\;p(f|{{\bf x}_*,\cal D})\,d f
  \nonumber\\
  &=&\int\sigma(f({\bf x}_*))\;p(f|{\bf x}_*,{\cal D})\,d f
  =E_f[\sigma(f({\bf x}_*))]=\sigma(E_f f({\bf x}_*))
\end{eqnarray}
As the function $f({\bf x})$ is marginalized (averaged out) in the 
integral above, it is not explicitly specified, and is therefore 
called a {\em latent function}. 

In the case of multiple test samples in the test dataset
${\bf X}_*=[{\bf x}_{1*},\cdots,{\bf x}_{M*}]$, the equation above
can be expressed in vector form:
\begin{eqnarray}
  p({\bf y}_*=1|{\bf X}_*,{\cal D})  &=&
  \int p({\bf y}_*={\bf 1}|{\bf X}_*,\,{\bf f})\,
  p({\bf f}|{\bf X}_*,{\cal D})\,d {\bf f}
  \nonumber\\
  &=&\int\sigma({\bf f}({\bf X}_*))\, p({\bf f}|{\bf X}_*,{\cal D})\,d {\bf f}
  =E_f[\sigma({\bf f}({\bf X}_*))]
  =\sigma(E_f f({\bf X}_*))
  \label{GPClassification}
\end{eqnarray}
where ${\bf f}({\bf X}_*)=[f({\bf x}_{1*}),\cdots,f({\bf x}_M)]^{T*}$.

%Here $p(f_*|{\bf x}_*,{\cal D})$ can be found by the following 
%integral:
%\begin{equation}
%  p(f_*|{\bf x}_*,{\cal D})
%  =\int p(f_*,{\bf f}|{\bf x}_*,{\cal D})\,d{\bf f}
%  =\int p(f_*|{\bf x}_*,{\cal D},{\bf f})\,p({\bf f}|{\cal D})\,d{\bf f}
%  \label{PosteriorIntegral}
%\end{equation}

In order to obtain the posterior $p({\bf f}|{\bf X}_*,{\cal D})$ 
in the integral for the test dataset ${\bf X}_*$, we first consider 
$p({\bf f}|{\cal D})$ where ${\bf f}={\bf f}({\bf X})$ based on the
training dataset ${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$. Similar to 
how we find the posterior of the model parameter ${\bf w}$ in 
Eq. (\ref{posteriorw}) based on 
\htmladdnormallink{Bayes' theorem}{../probability/node10.html},
here we can also find the posterior of ${\bf f}$ as:
\begin{equation}
  p({\bf f}|{\cal D})=p({\bf f}|{\bf X},{\bf y})
  =\frac{p({\bf y},{\bf f}|{\bf X})}{p({\bf y}|{\bf X})}
  =\frac{p({\bf y}|{\bf f})\,p({\bf f}|{\bf X})}{p({\bf y}|{\bf X})}
  \propto p({\bf y}|{\bf f})\,p({\bf f}|{\bf X})
  \label{PosteriorBayesGaussian}
\end{equation}
where ${\bf f}({\bf X})=[f({\bf x}_1),\cdots,f({\bf x}_N)]^T$,
$p({\bf y}|{\bf f})=L({\bf f}|{\bf y})$ is the likelihood, and
$p({\bf f}|{\bf X})$ is the prior of the function ${\bf f}$. As 
always, the denominator $p({\bf y}|{\bf X})$ is dropped as it is 
independent of ${\bf f}$ and plays no role in its estimation. 

We first find the likelihood $p({\bf y}|{\bf f})$. Same as
how $f({\bf x})={\bf x}^T{\bf w}$ is converted by the logistic 
function to $p(y=1|{\bf x})=\sigma({\bf w}^T{\bf x})$ in 
Eq. (\ref{LogisticSigmoidMapping}) in logistic regression, here 
function $f({\bf x})$ is also converted into the probability for
$y=1$ or $y=-1$, representing respectively ${\bf x}\in C_+$ or 
${\bf x}\in C_-$:
\begin{eqnarray}
  p(y=1| f({\bf x}) )&=&\sigma(f({\bf x}))
  \nonumber\\
  p(y=-1| f({\bf x}) )&=&1-p(y=1| f({\bf x}) )
  =1-\sigma(f({\bf x})) =\sigma(-f({\bf x}))
\end{eqnarray}
Combining these two cases, we get the conditional probability 
of $y$ given $f({\bf x})$:
\begin{equation}
  p(y|f({\bf x}))=\sigma(y\,f({\bf x}) )=\sigma(y\,f)
\end{equation}
and the likelihood function of ${\bf f}$ for all $N$ samples 
in the test dataset ${\bf X}$, assumed to be independent and 
identically distributed (i.i.d.):
\begin{equation}
  p({\bf y}|{\bf f})=\prod_{n=1}^N p(y_n|f_n)
  =\prod_{n=1}^N \sigma(y_n\,f_n)
\end{equation}

We next find the prior $p({\bf f}|{\bf X})$. As the latent 
function ${\bf f}({\bf X})$ is a Gaussian process, its prior 
can be assumed to be a zero-mean Gaussian 
$p({\bf f}|{\bf X})={\cal N}({\bf 0},{\bf\Sigma}_f)$, where the 
covariance matrix ${\bf\Sigma}_f$ can be constructed based on 
the training set ${\bf X}$. The same as in GPR, the component 
in the mth row and nth column of ${\bf\Sigma}_f$ is modeled by 
the squared exponential (SE) kernel:
\begin{equation}
  cov(f_m,f_n)=k({\bf x}_m,{\bf x}_n)
  =\exp\left(-\frac{1}{a^2}||{\bf x}_m-{\bf x}_n||^2\right),
  \;\;\;\;\;\;(m,n=1,\cdots,N)
\end{equation}
Such a covariance matrix is desired for GPC as well as GPR, so
that $f_m=f({\bf x}_m)$ and $f_n=f({\bf x}_n)$ are more correlated
if ${\bf x}_m$ and ${\bf x}_n$ are close together, but less so if
they are farther away from each other. However the justification 
for such a property is different. In GPR, this property is needed
for the continuity and smoothness of the regression function; while
in GPC, this property is desired so that the function values of 
${\bf x}_m$ and ${\bf x}_n$ close to each other in the feature 
space are more correlated and it is more likely for the two samples
to be classified into the same class, but less so if they are far
apart.

Also, as discussed in GPR, here the parameter $a$ in the SE controls 
the smoothness of the function. If $a\rightarrow\infty$, the value of 
the SE approaches 1, then $f({\bf x}_m)$ and $f({\bf x}_n)$ are highly
correlated and $f({\bf x})$ is very smooth; but if $a\rightarrow 0$, 
SE approaches 0, then $f({\bf x}_m)$ and $f({\bf x}_n)$ are not 
correlated and $f({\bf x})$ is no longer smooth.

Having found both the likelihood $p({\bf y}|{\bf f})$ and the prior
$p({\bf f}|{\bf X})$, we can now get their produce for the posterior 
in Eq. (\ref{PosteriorBayesGaussian}):
\begin{eqnarray}
  p({\bf f}|{\cal D})&\propto& p({\bf y}|{\bf f})\,p({\bf f}|{\bf X})
  =p({\bf y}|{\bf f})\,{\cal N}({\bf 0},{\bf K})
  =\prod_{n=1}^N\sigma(y_n\,f_n)\;
  \frac{1}{(2\pi)^{d/2}|{\bf K}|^{1/2}}
    \exp\left(-\frac{1}{2}{\bf f}^T{\bf K}^{-1}{\bf f}\right)
    \nonumber\\
    &\propto&\prod_{n=1}^N\sigma(y_n\,f_n)\;
    \exp\left(-\frac{1}{2}{\bf f}^T{\bf K}^{-1}{\bf f}\right)
  \label{PosteriorBayesGaussian1}
\end{eqnarray}
Note that the likelihood $p({\bf y}|{\bf f})$ is not Gaussian, as 
the binary labeling ${\bf y}$ of the training set ${\bf X}$ is not 
continuous. Consequently, the posterior $p({\bf f}|{\cal D})$, as a 
product of the Gaussian prior and non-Gaussian likelihood, is not 
Gaussian. However, for convenience, we can still approximate it by 
a Gaussian: 
\begin{equation}
  p({\bf f}|{\cal D})\approx {\cal N}({\bf m}_{f|D},{\bf\Sigma}_{f|D})
\end{equation}
in terms of the mean ${\bf m}_{f|D}$ and covariance ${\bf\Sigma}_{f|D}$,
which are to be obtained in the following.

We first get the log posterior denoted by $\psi({\bf f})$:
\begin{equation}
  \psi({\bf f})=\log\,p({\bf f}|{\cal D})
  =\sum_{n=1}^N\log\sigma(y_n\,f_n)-\frac{N}{2}\log(2\pi)-\frac{1}{2}\log|{\bf K}|
  -\frac{1}{2}{\bf f}^T {\bf K}^{-1}{\bf f}
  \label{PsiLogPG}
\end{equation}
The two middle terms are constant independent of ${\bf f}$ and can 
therefore be dropped. 

We can further find the gradient vector and Hessian matrix of 
$\psi({\bf f})=\log p({\bf y}|{\bf f})$ as its first and second 
order derivatives:
\begin{equation}
  {\bf g}_{\psi}({\bf f})=\frac{d}{d{\bf f}}\psi({\bf f})
  =\frac{d}{d{\bf f}}\;\log\,p({\bf y}|{\bf f})\,
  -\frac{d}{d{\bf f}}\left(\frac{1}{2} {\bf f}^T{\bf K}^{-1}{\bf f}\right)
  ={\bf w}-{\bf K}^{-1}{\bf f} 
\end{equation}
\begin{equation}
  {\bf H}_{\psi}({\bf f})=\frac{d^2}{d{\bf f}^2}\psi({\bf f}) 
  =\frac{d}{d{\bf f}} {\bf g}_\psi({\bf f})
  =\frac{d}{d{\bf f}} \left( {\bf w}-{\bf K}^{-1}{\bf f}\right)
  ={\bf W}-{\bf K}^{-1}
  \label{HessianPsi2}
\end{equation}
Here we have defined
\begin{equation}
  {\bf w}=\frac{d}{d{\bf f}}\log p({\bf y}|{\bf f}),
  \;\;\;\;\;\;\;\;\;
  {\bf W}=\frac{d^2}{d{\bf f}^2} \log p({\bf y}|{\bf f})
  =\frac{d{\bf w}}{d{\bf f}}
  \label{wW}
\end{equation}
To find these first and second order derivatives with respection to 
${\bf f}$, we first find the first and second order derivatives with
a single component:
\begin{eqnarray}
  \frac{d}{df_n}\log\,p(y_n|f_n)&=&\frac{d}{df_n}\,
  \log\left(1+\exp(-y_n f_n)\right)^{-1}
  =\frac{y_n}{1+\exp(y_n f_n)}
  \nonumber\\
  \frac{d^2}{df_n^2}\log\,p(y_n|f_n)&=&
  \frac{d}{df_n}\left[\frac{y_n}{1+\exp(y_n f_n)}\right]
  =\frac{-\exp(-y_n f_n)}{(1+\exp(-y_n f_n))^2}
\end{eqnarray}
where we have used the facts that $y_n=\pm 1$ and $y_n^2=1$. We then 
find
\begin{equation}
  {\bf w}=\frac{d}{d{\bf f}} \log p({\bf y}|{\bf f})
  =\left[\begin{array}{c}d/df_1\\\vdots\\d/df_N\end{array}\right]
  \sum_{n=1}^N\log p(y_n|f_n)
  =\left[\begin{array}{c}
      d\log p(y_1|f_1)/df_1\\\vdots\\d\log p(y_N|f_N)/df_N
    \end{array}\right]
  =\left[\begin{array}{c}
      \frac{y_1}{1+e^{y_1f_1}}\\ \vdots\\ \frac{y_N}{1+e^{y_Nf_N}}
    \end{array}\right]
  \label{Smallw}
\end{equation}
and
\begin{eqnarray}
  {\bf W}&=&\frac{d^2}{d{\bf f}^2}  \log p({\bf y}|{\bf f})
  =\left[\begin{array}{ccc}
      \frac{\partial^2}{\partial f_1\partial f_1}&\cdots
      &\frac{\partial^2}{\partial f_1\partial f_N}\\
      \vdots&\ddots&\vdots\\
      \frac{\partial^2}{\partial f_N\partial f_1}&\cdots
      &\frac{\partial^2}{\partial f_N\partial f_N}\\
    \end{array}\right]\sum_{n=1}^N\log p(y_n|f_n)
  \nonumber\\
  &=&diag \left[
    \frac{d^2}{df_1^2}\log p(y_1|f_1),\cdots,\frac{d^2}{df_N^2}\log p(y_N|f_N)
    \right]
  =diag \left[
    \frac{-e^{-y_1f_1}}{(1+e^{-y_1f_1})^2},\cdots,\frac{-e^{-y_Nf_N}}{(1+e^{-y_Nf_N})^2}
    \right]
  \label{BigW}
\end{eqnarray}

% $p(y|f)=\phi(yf)=\int_{-\infty}^{yf} {\cal N}(u,0,1)\,du$:
%  We first get
%  \begin{eqnarray}
%    \frac{d}{df} \phi(yf)&=&\frac{d}{df} 
%    \left[\int_{-\infty}^{yf}{\cal N}(u,0,1)\,du\right]
%    =y\,{\cal N}(f,0,1)
%    \nonumber\\
%    \frac{d}{df}{\cal N}(f,0,1)
%    &=&\frac{d}{df}\left[\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{f^2}{2}\right)\right]
%    =-f{\cal N}(f,0,1)
%  \end{eqnarray}
%  and then find the first and second derivatives of $\log p(y|f)$:
%  \begin{eqnarray}
%    \frac{d}{df}\log \phi(yf)&=&\frac{y\,{\cal N}(f,0,1)}{\phi(yf)}
%    \nonumber\\
%    \frac{d^2}{df^2}\log \phi(yf)
%    &=&-\frac{yf{\cal N}(f,0,1)}{\phi(yf)}-\frac{{\cal N}(f,0,1)^2}{\phi^2(yf)}
%  \end{eqnarray}

On the other hand, we can also get ${\bf g}_\psi({\bf f})$ and
${\bf H}_\psi({\bf f})$, as now the posterior $p({\bf f}|{\cal D})$ is 
approximated as a Gaussian. We can find the gradient and Hessian of 
the log-normal distribution $\psi({\bf f})=\log p({\bf f}|{\cal D})$ 
(see \htmladdnormallink{Appendix}{../probability/node4.html}):
\begin{eqnarray}
  {\bf g}_{\psi}({\bf f})&=&\frac{d}{d{\bf f}}\psi({\bf f})
  =-{\bf\Sigma}^{-1}_{f|D} ({\bf f}-{\bf m}_{f|D})
  \label{GradientPsi}\\
  {\bf H}_{\psi}({\bf f})&=&\frac{d^2}{d{\bf f}^2}\psi({\bf f})
  =\frac{d}{d{\bf f}}{\bf g}_{\psi}({\bf f})=-{\bf\Sigma}_{f|D}^{-1}
  \label{HessianPsi1}
\end{eqnarray}

Equatiing the two expressions for ${\bf H}_{\psi}({\bf f})$ in
Eqs. (\ref{HessianPsi1}) and (\ref{HessianPsi2}) we get:
\begin{equation}
  {\bf H}_\psi({\bf f})={\bf W}-{\bf K}^{-1}=-{\bf\Sigma}_{f|D}^{-1},
  \;\;\;\;\;\;\;\;\mbox{i.e.,}\;\;\;\;\;\;\;\;
  {\bf\Sigma}_{f|D}=({\bf K}^{-1}-{\bf W})^{-1}
  \label{SigmaFY}
\end{equation}
where ${\bf W}$ is given in Eq. (\ref{BigW}).

The mean ${\bf m}_{f|D}$ can be found as the point at which the 
$p({\bf f}|{\cal D})$, now assumed to be Gaussian, reaches maximum, 
and so does the log-normal $\psi({\bf f})=\log p({\bf f}|{\cal D})$.
We can therefore find ${\bf m}_{f|D}$ by solving the equation
${\bf g}_{\psi}({\bf f})={\bf 0}$, by 
\htmladdnormallink{Newton's method}{../ch3/node6.html}:
\begin{eqnarray}
  {\bf f}_{n+1}&=&{\bf f}_n-{\bf H}_{\psi}^{-1}({\bf f}_n)\;{\bf g}_{\psi}({\bf f}_n)
  ={\bf f}_n+({\bf K}^{-1}-{\bf W})^{-1}
  \left({\bf w}-{\bf K}^{-1}{\bf f}_n\right)
  \nonumber\\
  &=&{\bf f}_n+({\bf K}^{-1}-{\bf W})^{-1}\left[-({\bf K}^{-1}-{\bf W}){\bf f}_n
    +{\bf w}-{\bf W}{\bf f}_n\right]
  \nonumber\\
  &=&({\bf K}^{-1}-{\bf W})^{-1}\left({\bf w}-{\bf W}{\bf f}_n\right)
  \label{GPCiteration}
\end{eqnarray}
Note that ${\bf f}$ is updated based on ${\bf w}$ and ${\bf W}$
given in Eqs (\ref{Smallw}) and (\ref{BigW}), which in turn are 
functions of ${\bf f}$. We therefore see that the function ${\bf f}$ 
and the parameters ${\bf w}$ and ${\bf W}$ rely on each other and 
they need to be updated alternatively uring the iteration above.

This iterative process converges to ${\bf f}={\bf m}_{f|D}$, 
at which both $\psi({\bf f})=\log p({\bf f}|{\cal D})$ and 
$p({\bf f}|{\cal D})$ are maximized, and
\begin{equation}
  {\bf g}_{\psi}({\bf f})=\frac{d}{d{\bf f}} \psi({\bf  f})
  ={\bf w}-{\bf K}^{-1}{\bf f} 
  ={\bf 0},\;\;\;\;\;\;\mbox{i.e.,}\;\;\;\;\;\;
  {\bf f}={\bf K}\;{\bf w}
  \label{aaa}
\end{equation}
Taking the expectation on both sides we get
\begin{equation}
  {\bf m}_{f|D}=E({\bf f})=E\left( {\bf K}{\bf w}\right)
  = {\bf K} E( {\bf w})={\bf K}{\bf w}
  \label{MeanFY}
\end{equation}
Having found ${\bf w}$ and ${\bf W}$ as well as
${\bf f}={\bf m}_{f|D}$, we can further obtain
${\bf\Sigma}_{f|D}$ in Eq. (\ref{SigmaFY}), and approximate 
the posterior as a Gaussian
$p({\bf f}|{\cal D})\approx{\cal N}({\bf m}_{f|D},\,{\bf\Sigma}_{f|D})$.

We are now ready to proceed to consider the classification of the 
test set ${\bf X}_*$ by Eq. (\ref{GPClassification}). To do so, we 
first need to approximate the posterior of ${\bf f}_*={\bf f}({\bf X}_*)$ 
in the equation by a Gaussian
\begin{equation}
  p({\bf f}|{\bf X}_*,{\cal D})\approx
  {\cal N}({\bf m}_{f_*},{\bf\Sigma}_{f_*})
\end{equation}
where the mean ${\bf m}_{f_*}$ and covariance ${\bf\Sigma}_{f_*}$ can 
be obtained based on the fact that both ${\bf f}_*={\bf f}({\bf X}_*)$ 
and ${\bf f}={\bf f}({\bf X})$ are the same Gaussian process, i.e., 
their joint probability $p({\bf f},{\bf f}_8)$ is a Gaussian. The 
method is therefore the same as what is discussed in the method of 
\htmladdnormallink{GPR}{../ch7/node17.html}. Specifically, we take 
the following steps:

\begin{itemize}
\item Find the mean ${\bf m}_{f_*|f}=E({\bf f}_*|{\bf f})$ and 
  covariance ${\bf\Sigma}_{f_*|f}=Cov({\bf f}_*|{\bf f})$ of 
  $p({\bf f}_*|{\bf X}_*,{\bf X},{\bf f})$ conditioned
  on the latent function ${\bf f}$, same as in Eq. (\ref{MeanCovff}) 
  for GPR:
  \begin{equation}
    \left\{  \begin{array}{lcl}
      {\bf m}_{f_*|f}={\bf K}_*^T {\bf K}^{-1} {\bf f} \\
      {\bf\Sigma}_{f_*|f}={\bf K}_{**}-{\bf K}_*^T{\bf K}^{-1} {\bf K}_*
    \end{array}\right.
    \label{MeanCovffGPC}
  \end{equation}

\item Find ${\bf m}_{f_*}=E_f({\bf f}_*)$ as the expectation of
  ${\bf m}_{f_*|f}={\bf K}_*^T {\bf K}^{-1}{\bf f}$ conditioned on 
  ${\bf f}$, i.e. find the average of ${\bf m}_{f_*|f}$ over ${\bf f}$ 
  based on $p({\bf f}|{\cal D})$ (marginalize over ${\bf f}$):
  \begin{eqnarray}
    {\bf m}_{f_*}&=&E_f ({\bf m}_{f_*|f})
    =E_f \left( {\bf K}_*^T {\bf K}^{-1} {\bf f} \right)
    ={\bf K}_*^T {\bf K}^{-1} E_f({\bf f})={\bf K}_*^T {\bf K}^{-1} {\bf m}_f
    \nonumber\\
    &=&{\bf K}_*^T {\bf K}^{-1}{\bf K}\;{\bf w}_{{\bf m}_f}
    ={\bf K}_*^T {\bf w}_{{\bf m}_f}
    \label{MeanFsY}
  \end{eqnarray}
  where ${\bf m}_f=E_f({\bf f})={\bf K}{\bf w}_{m_f}$ given in 
  Eq. (\ref{MeanFY}).

\item Find the covariance ${\bf\Sigma}_{m_{f_*|f}}=E_f[({\bf m}_{f_*|f}-{\bf m}_{f_*})^2]$.
  Given the covariance ${\bf\Sigma}_{f|D}=({\bf K}^{-1}-{\bf W})^{-1}$
  of ${\bf f}$ in Eq. (\ref{SigmaFY}), we can further find the covariance of
  ${\bf m}_{f_*|f}={\bf K}_*^T {\bf K}^{-1}{\bf f}$ as a linear combination 
  of ${\bf f}$ (Recall if ${\bf y}={\bf Ax}$, then 
  ${\bf\Sigma}_y={\bf A\Sigma}_x{\bf A}^T$):
  \begin{equation}
    {\bf\Sigma}_{m_{f_*|f}}
    ={\bf K}_*^T {\bf K}^{-1}\,{\bf\Sigma}_{f|D} \,{\bf K}^{-1}{\bf K}_*
    ={\bf K}_*^T {\bf K}^{-1} ({\bf K}^{-1}-{\bf W})^{-1} {\bf K}^{-1}{\bf K}_*
  \end{equation}

\item Find the covariance ${\bf\Sigma}_{f_*}$ of ${\bf f}_*$ as the sum of 
  ${\bf\Sigma}_{f_*|f}=E[({\bf f}_*-{\bf m}_{f_*|f})({\bf f}_*-{\bf m}_{f_*|f})^T]$
  for the variation of ${\bf f}_*$ with respect to ${\bf m}_{f_*|f}$, and 
  ${\bf\Sigma}_{m_{f_*|f}}=E_f[({\bf m}_{f_*|f}-{\bf m}_{f_*})^2]$ for the
  variation of ${\bf m}_{f_*|f}$ with respect to ${\bf m}_{f_*}$:  
  \begin{eqnarray}
    {\bf\Sigma}_{f_*}&=&{\bf\Sigma}_{f_*|f}+{\bf\Sigma}_{m_{f_*|f}}
    \nonumber\\
    &=&({\bf K}_{**}-{\bf K}_*^T{\bf K}^{-1}{\bf K}_*)
    +({\bf K}_*^T{\bf K}^{-1}({\bf K}^{-1}-{\bf W})^{-1}{\bf K}^{-1}{\bf K}_*)
    \nonumber\\
    &=&{\bf K}_{**}-{\bf K}_*^T{\bf K}^{-1}{\bf K}_*
    +{\bf K}_*^T{\bf K}^{-1} [{\bf K}-{\bf K}({\bf K}-{\bf W}^{-1})^{-1}{\bf K}]{\bf K}^{-1}{\bf K}_*
    \nonumber\\
    &=&{\bf K}_{**}-{\bf K}_*^T({\bf K}-{\bf W}^{-1})^{-1}{\bf K}_*
    \label{CovarianceFsY}
  \end{eqnarray}
  Here we have used the identity for $({\bf A}+{\bf B})^{-1}$ given 
  in the \htmladdnormallink{Appendices}{../algebra/node6.html}.
\end{itemize}

Now that the posterior 
$p({\bf f}|{\bf X}_*,{\cal D})={\cal N}({\bf m}_{f_*},{\bf\Sigma}_{f_*})$
is approximated as a Gaussian and its mean and covariance are
obtained in Eqs. (\ref{MeanFsY}) and (\ref{CovarianceFsY}), we 
can finally carry out Eq. (\ref{GPClassification}) to find the 
probability for the test points in ${\bf X}_*$ to belong to class
$C_1$: 
\begin{eqnarray}
  p({\bf y}_*=1|{\bf X}_*,{\cal D})
  &=&\int\sigma({\bf f}_*)\,p({\bf f}_*|{\bf X}_*,{\cal D})\,d {\bf f}_*
  \nonumber\\
  &=&E_f[ \sigma({\bf f}({\bf X}_*)) ]=\sigma(E_f({\bf f}({\bf X}_*)))
  =\sigma( {\bf m}_{f_*} )
\end{eqnarray}
Moreover, the certainty or confidence of this classification result 
is represented by the covariance ${\bf\Sigma}_{f_*}$.

The Matlab code for the essential parts of this algorithm is 
listed below. Here \verb|X| and \verb|y| are for the training data 
${\cal D}=\{{\bf X},{\bf y}\}$, and \verb|Xs| is an array composed 
of test vectors. First, the essential segment of the main program 
listed below takes in the training and test data, generates the 
covariance matrices ${\bf K}$, ${\bf K}_*$, and ${\bf K}_{**}$ 
represented by \verb|K|, \verb|Ks|, and \verb|Kss|, respectively. 
The function \verb|Kernel| is exactly the same as the one used for 
Gaussian process regression. This code segment then further calls 
a function \verb|findPosteriorMean| which finds the mean and 
covariance of $p({\bf f}|{\bf X},{\bf y})$ based on covariance of 
training data (${\bf K}=Cov({\bf X})$ and ${\bf y}$, and computes 
the mean ${\bf m}_{f_*|y}$ and covariance ${\bf\Sigma}_{f_*|y}$ based 
on Eqs. (\ref{MeanFsY}) and (\ref{CovarianceFsY}), respectively. The
sign function of ${\bf m}_{f_*|y}$ indicates the classification of 
the test data points in ${\bf X}_*$.

\begin{verbatim}
    K=Kernel(X,X);                     % cov(f,f), covariance of prior p(f|X)
    Ks=Kernel(X,Xs);                   % cov(f_*,f)
    Kss=Kernel(Xs,Xs);                 % cov(f_*,f_*)
    [Sigmaf W w]=findPosteriorMean(K,y);   
                                       % get mean/covariance of p(f|D), W, w
    meanfD=Ks'*w;                      % mean of p(f_*|X_*,D)
    SigmafD=Kss-Ks'*inv(K-inv(W))*Ks;  % covariance of p(f_*|X_*,D)
    ys=sign(meanfD);                   % binary classification of test data
    p=1./(1+exp(-meanfD));             % p(y_*=1|X_*,D) as logistic function
\end{verbatim}

The function \verb|findPosteriorMean| listed below uses Newton's method
to find the mean and covariance of the posterior $p({\bf f}|{\cal D})$
of the latent function ${\bf f}$ based on the training data, and returns 
them in \verb|meanf| and \verb|covf|, respectively, together with \verb|w| 
and \verb|W| for the gradient and Hessian of the likelihood 
$p({\bf y}|{\bf f})$, to be used for computing ${\bf m}_{f_*|D}$ 
and ${\bf\Sigma}_{f_*|D}$.

\begin{verbatim}
function [covf W w]=findPosteriorMean(K,y)  
    % K: covariance of prior of p(f|X)
    % y: labeling of training data X
    % w: gradient vector of log p(y|f)
    % W: Hessian matrix of log p(y|f)
    % meanf: mean of p(f|X,y)
    % covf: covariance of p(f|X,y)
    n=length(y);                    % number of training samples
    f0=zeros(n,1);                  % initial value of latent function
    f=zeros(n,1);        
    er=1;
    while er > 10^(-9)              % Newton's method to get f that maximizes p(f|X,y)
        e=exp(-y.*f);
        w=y.*e./(1+e);              % update w
        W=diag(-e./(1+e).^2);       % update W
        f=inv(inv(K)-W)*(w-W*f0);   % iteration to get f from previous f0
        er=norm(f-f0);              % difference between two consecutive f's
        f0=f;                       % update f
    end
    covf=inv(inv(K)-W);             % coviance of f
end
\end{verbatim}

{\bf Example 1} The GPC method is trained by the two classes shown in
the figure below, represented by 100 red points and 80 blue points 
drawn from two Gaussian distributions ${\cal N}({\bf m}_0,{\bf\Sigma}_0)$ 
and ${\cal N}({\bf m}_1,{\bf\Sigma}_1)$, where
\begin{equation}
{\bf m}_0=\left[\begin{array}{c}1\\0\end{array}\right],\;\;\;
{\bf m}_1=\left[\begin{array}{c}-1\\0\end{array}\right],\;\;\;
{\bf\Sigma}_0=\left[\begin{array}{cc}1&0\\0&1\end{array}\right],\;\;\;
{\bf\Sigma}_1=\left[\begin{array}{cc}1&0.9\\0.9&1\end{array}\right]
\end{equation}
The 3-D plot of these two normalized Gaussian distributions is also 
shown in the figure.

\htmladdimg{../figures/GPCexample1.png}

Three classification results are shown in the figure below corresponding 
to different values used in the kernel function. On the left, the 2-D
space is partitioned into red and blue regions corresponding to the 
two classes based on the sign function of ${\bf m}_{f_*|y}$; on the right,
the 3-D distribution plots of $\sigma({\bf m}_{f_*|y})$ representing the
estimated probability for ${\bf x}_*$ to belong to either class (not 
normalized) is shown, to be compared with the original Gaussian 
distributions from which the traning samples were drawn. 

We see that wherever there is evidence represented by the training 
samples of either class in red or blue, there are high probabilities 
for the neighboring points to belong to either $C_1$ or $C_0$ represented 
by the positive or negative peaks in the 3-D plots. Data points far 
away from any evidence will have low probability to belong to either 
class.

We make the following observations for three different values of the
parameter $\alpha$ in SE:
\begin{itemize}
\item $\alpha=0.4$ (top row), the space is partitioned into three regions,
  with 20 out of 180 training points misclassified. The estimated
  distribution is smooth.
\item $\alpha=0.2$ (middle row), the space is fragmented into several 
  more pieces for the two classes (blue islands inside the red region 
  and vice versa), with 5 out of the 180 training points misclassified.
  The estimated distribution is jagged.
\item $\alpha=0.03$ (bottom row), the space is partitioned into still 
  more pieces, with all 180 training points correctly classified. The 
  estimated distribution is spiky. 
\end{itemize}
Although the error rate is lowest when $\alpha$ is small, the classificatioin
result is not necessarily the best as it may well be an overfitting of 
the noisy data. We conclude that by adjusting parameter $\alpha$, we can 
make proper tradeoff between error rate and over-fitting.

\htmladdimg{../figures/GPCexample1a.png}


\subsection{Gaussian Process Classifier -- Multi-Class}

%http://publications.aston.ac.uk/4491/1/IEEE_transactions_on_pattern_analysis_20(12).pdf

% cross-entropy H(p,q)

The binary GPC considered previously can be generalized to multi-class 
GPC, similar to the how binary classification based on logistic function
is generalized to multi-class classification based on softmax function.
First of all, we define the following variables for each class $C_k,\; 
(k=1,\cdots,K)$ of the $K$ classes $\{C_1,\cdots,C_K\}$:
\begin{itemize}
\item {\bf Class labeling:}

  Binary labeling vector ${\bf y}_k=[y_1^k,\cdots,y_N^k]^T$ 
  indicating whether each of the $N$ samples in the training set 
  ${\bf X}=\{{\bf x}_1,\cdots,{\bf x}_N\}$ belongs to $C_k$:
  \begin{equation}
    y_n^k=\left\{\begin{array}{ll}1 & \mbox{if ${\bf x}_n\in C_k$}\\
    0 & \mbox{if ${\bf x}_n\notin C_k$}\end{array}\right.
  \end{equation}

\item {\bf Latent functions:}

  Latent function ${\bf f}_k=[f_1^k,\cdots,f_N^k]^T$ evaluated at the
  $N$ training samples in ${\bf X}$, where $f_n^k=f^k({\bf x}_n)$.

\item {\bf Probability modeling:}

  Vector ${\bf p}_k=[p_1^k,\cdots,p_N^k]^T$, of which the nth component
  $p_n^k\;(n=1,\cdots,N)$ is the probability for ${\bf x}_n\in C_k$,
  modeled by the solftmax function based on $f_n^k,\;(k=1,\cdots,K,\;
  n=1,\cdots,N)$:
  \begin{equation}
    p_n^k=p(y_n^k=1|f_n^1,\cdots,f_n^K)
    =\frac{\exp(f_n^k)}{\sum_{l=1}^K\exp(f_n^l)}
    =\left\{\begin{array}{ll}1 & \mbox{if $f_n^k=\infty$}\\ 
    0 & \mbox{if $f_n^k=-\infty$}\end{array}\right.
    \label{SoftmaxGPC}
  \end{equation}
  Note that if $y_n^k=1$ then necessarily $y_n^l=0$ for all 
  $l=1,\cdots,K$ but $l\ne k$, i.e., the $K$ variables 
  $y_n^1,\cdots,y_n^K$ are not independent.
%  given $p(y_n^k=1)$, we do not need to consider $p(y_n^l=0)$ for any $l\ne k$.
\end{itemize}

Based on ${\bf y}_k$, ${\bf f}_k$, and ${\bf p}_k$ for all $k=1,\cdots,K$,
we further define the following $KN$ dimensional vectors:
\begin{equation}
  {\bf y}=\left[\begin{array}{c}{\bf y}_1\\\vdots\\{\bf y}_K\end{array}\right]
  =\left[\begin{array}{c}y_1^1\\\vdots\\y_N^1\\\vdots\\y_1^K\\\vdots\\y_N^K\end{array}\right],\,\;\;\;\;\;
  {\bf f}=\left[\begin{array}{c}{\bf f}_1\\\vdots\\{\bf f}_K\end{array}\right]
  =\left[\begin{array}{c}f_1^1\\\vdots\\f_N^1\\\vdots\\f_1^K\\\vdots\\f_N^K\end{array}\right],\,\;\;\;\;\;
  {\bf p}=\left[\begin{array}{c}{\bf p}_1\\\vdots\\{\bf p}_K\end{array}\right]
  =\left[\begin{array}{c}p_1^1\\\vdots\\p_N^1\\\vdots\\p_1^K\\\vdots\\p_N^K\end{array}\right]
\end{equation}

We also consider the posterior of ${\bf f}$ given the training 
set ${\cal D}$, same as in Eq. (\ref{PosteriorBayesGaussian}) in
the binary case:
\begin{equation}
  p({\bf f}|{\cal D})=p({\bf f}|{\bf X},{\bf y})
  \propto p({\bf y}|{\bf f})\; p({\bf f}|{\bf X})
\end{equation}

We first find the likelihood based on all $p_n^k$:
\begin{equation}
  p({\bf y}|{\bf f})=\prod_{n=1}^N\prod_{k=1}^K (p_n^k)^{y_n^k}
  =\prod_{n=1}^N\prod_{k=1}^K 
  \left( \frac{\exp(f_n^k)}{\sum_{h=1}^K\exp(f_n^h)}\right)^{y_n^k}
\end{equation}
As each sample ${\bf x}_n$ belongs to only one of the $K$ classes, 
only one of $\{y_n^1,\cdots,y_n^K\}$ can be 1 while all others are
0, consequently, the product above contains only $N$ probabilities 
raised to the power of $y_n^k=1$, each for one of the $N$ samples
belonging to a certain class, while all other probabilities raised
to the power of $y_n^k=0$ become 1 and not considered.

We next assume the prior probability of the latent function 
${\bf f}_k$ for each class $C_k$ to be a zero-mean Gaussian 
process $p({\bf f}_k|{\bf X})={\cal N}({\bf 0},{\bf\Sigma}_k)$, 
where the covariance matrix ${\bf\Sigma}_k$ is constructed based
on the squared exponential (SE) for its mn-th component:
\begin{equation}
  cov(f_k({\bf x}_m),f_k({\bf x}_n))=cov(f^k_m,f^k_n)
  =k({\bf x}_m,{\bf x}_n)
  =\exp\left(-\frac{1}{a^2}||{\bf x}_m-{\bf x}_n||^2\right),
  \;\;\;\;\;\;(m,n=1,\cdots,N)
\end{equation}
Then the prior of ${\bf f}$ for ${\bf f}$ containing all 
$K$ such latent functions is also a Gaussian 
$p({\bf f}|{\bf X})={\cal N}({\bf 0},{\bf K})$, where ${\bf 0}$ 
is a KN-dimensional zero vector and ${\bf K}$ is a $KN \times KN$
block diagonal matrix
\begin{equation}
{\bf K}=\left[\begin{array}{cccc}{\bf K}_1 & {\bf 0} &\cdots &{\bf 0}\\
    {\bf 0} & {\bf K}_2 & \cdots & {\bf 0}\\
    \vdots & \vdots & \ddots & \vdots \\
    {\bf 0} & \cdots & {\bf 0} &{\bf K}_K\end{array}\right]
\end{equation}
All off-diagonal blocks are zero as the latent functions of 
different classes are uncorrelated. 

Having found both $p({\bf y}|{\bf f})$ and $p({\bf f}|{\bf X})$,
we can write the posterior as
\begin{equation}
  p({\bf f}|{\cal D})
  =p({\bf f}|{\bf X},{\bf y})\propto p({\bf y}|{\bf f})\; p({\bf f}|{\bf X})
  \propto\prod_{n=1}^N\prod_{k=1}^K 
  \left( \frac{\exp(f_n^k)}{\sum_{h=1}^K\exp(f_n^h)} \right)^{y_n^k} 
       {\cal N}({\bf 0},\,{\bf K})
\end{equation}
We note that the likelihood function $p({\bf y}|{\bf f})$ is not
Gaussian, as $y_n^k \in\{0,\,1\}$ is a binary labeling, instead 
of a continuous function. As a product of the Gaussian prior and
non-Gaussian likelihood, the posterior $p({\bf f}|{\cal D})$ is
not a Gaussian process either. However, for convnience, we will 
still approximate it by a Gaussian
$p({\bf f}|{\cal D})\approx {\cal N}({\bf f},\,{\bf m}_{f|D},\,{\bf\Sigma}_{f|D})$.

To find the mean ${\bf m}_{f|D}$ and covariance ${\bf\Sigma}_{f|D}$,
we first find the log posterior
\begin{eqnarray}
  \psi({\bf f})&=&\log p({\bf f}|{\cal D})
  =\log p({\bf y}|{\bf f}) + \log p({\bf f}|{\bf X})
  =\sum_{n=1}^N\sum_{k=1}^K y_n^k\left(f_n^k-\log\sum_{h=1}^K\exp(f_n^h)\right)
  +\log {\cal N}({\bf 0},{\bf K})
  \nonumber\\
  &=&{\bf y}^T{\bf f}-\sum_{n=1}^N\sum_{k=1}^K \log \sum_{h=1}^K\exp(f_n^h)
  -\frac{N}{2}\log(2\pi)-\frac{1}{2}\log|{\bf K}|
  -\frac{1}{2}{\bf f}^T {\bf K}^{-1}{\bf f}
\end{eqnarray}
and its gradient
\begin{eqnarray}
  {\bf g}_\psi&=&
  \frac{d}{d{\bf f}} \psi({\bf f})=\frac{d}{d{\bf f}} \left(
    {\bf y}^T{\bf f}-\sum_{n=1}^N\sum_{k=1}^K \log \sum_{h=1}^K\exp(f_n^h)
    -\frac{N}{2}\log(2\pi)-\frac{1}{2}\log|{\bf K}|
    -\frac{1}{2}{\bf f}^T {\bf K}^{-1}{\bf f}\right)
  \nonumber\\
  &=&{\bf y}-{\bf p}-{\bf K}^{-1}{\bf f}
\end{eqnarray}
where the second term ${\bf p}$ comes from
\begin{equation}
  \sum_{n=1}^N\sum_{k=1}^K \frac{d}{d{\bf f}} \log \sum_{h=1}^K\exp(f_n^h) 
  =\sum_{n=1}^N\sum_{k=1}^K \left[\begin{array}{c}0\\\vdots\\0\\
      \frac{\exp(f_n^k)}{\sum_{h=1}^K\exp(f_n^h)}\\0\\\vdots\\0\end{array}\right]
  =\sum_{n=1}^N\sum_{k=1}^K \left[\begin{array}{c}0\\\vdots\\0\\
      p_n^k\\0\\\vdots\\0\end{array}\right]={\bf p}
  \label{ppp}
\end{equation}
Note that ${\bf p}$ is a function of ${\bf f}$.

As the posterior
$p({\bf f}|{\cal D})\approx{\cal N}({\bf m}_{f|D},{\bf\Sigma}_{f|D})$
is approximated as a Gaussian, which reaches maximum at its mean 
${\bf m}_{f|D}$, and so does the log prior 
$\psi({\bf f})=\log p({\bf f}|{\cal D})$, and 
the gradient of $psi({\bf f})$ is zero at ${\bf f}={\bf m}_{f|D}$:
\begin{equation}
  {\bf g}_\psi({\bf f})\bigg|_{{\bf f}={\bf m}_{f|D}}
  ={\bf y}-{\bf p}-{\bf K}^{-1}{\bf f}\bigg|_{{\bf f}={\bf m}_{f|D}}
  ={\bf y}-{\bf p}_{m_f}-{\bf K}^{-1}{\bf m}_{f|D}={\bf 0},
\end{equation}
i.e.,
\begin{equation}
  {\bf m}_{f|D}={\bf K}({\bf y}-{\bf p}_{m_f})
  \label{MeanFMC}
\end{equation}
where ${\bf p}_{m_f}=E_f({\bf p})$ is the vector defined above 
evaluated at ${\bf f}={\bf m}_{f|D}$.

We further get the Hessian matrix of $\psi({\bf f})$:
\begin{equation}
  {\bf H}_\psi({\bf f})=\frac{d^2}{d{\bf f}^2} \psi({\bf f})
  =\frac{d}{d{\bf f}} {\bf g}_\psi
  =\frac{d}{d{\bf f}}\left( {\bf y}-{\bf K}^{-1}{\bf f}-{\bf p} \right)
  =-{\bf K}^{-1} -{\bf W}=-{\bf\Sigma}_{f|D}^{-1}
\end{equation}
The last equality is due to the property of the Gaussian distribution
(see \htmladdnormallink{Appendix}{../probability/node4.html}), from
which we get the KN by KN covariance matrix of $p({\bf f}|{\cal D})$:
\begin{equation}
  {\bf\Sigma}_{f|D}=-{\bf H}^{-1}_\psi({\bf f})=({\bf K}^{-1}+{\bf W})^{-1}
  \label{SigmaF}
\end{equation}
Here ${\bf W}=d{\bf p}/d{\bf f}$ is a $KN\times KN$ Jacobian matrix
of ${\bf p}$, of which the $ijkl$-th component 
$(i,j=1,\cdots,N,\;\;k,l=1,\cdots,K)$ is:
\begin{eqnarray}
  \frac{\partial}{\partial f_j^l} p_i^k
  &=&\frac{\partial}{\partial f_j^l}\left[\frac{\exp(f_i^k)}{\sum_{h=1}^K\exp(f_i^h)}\right]
  =\frac{\exp(f_i^k)\left(\sum_{h=1}^K\exp(f_i^h)\right)\delta_{kl}
    -\exp(f_i^k)\exp(f_j^l)}{\left(\sum_{h=1}^K\exp(f_i^h)\right)^2}\delta_{ij}
  \nonumber\\
  &=&\left[\frac{\exp(f_i^k)}{\sum_{h=1}^K\exp(f_i^h)}\delta_{kl}
    -\frac{\exp(f_i^k)\exp(f_j^l)}{\left(\sum_{h=1}^K\exp(f_i^h)\right)^2}\right]
  \delta_{ij}
  =(p_i^k\delta_{kl}-p_i^k p_j^l)\delta_{ij},
\end{eqnarray}
and ${\bf W}$ can be written also in two terms:
\begin{equation}
  {\bf W}=diag({\bf p})-{\bf PP}^T
  \label{WWW}
\end{equation}
where the first term $diag({\bf p})$ is a diagnal matrix containing 
all $KN$ components of ${\bf p}$ along the diagnal, and ${\bf P}$ 
in the second term is a KN by N matrix composed of $K$ $N\times N$ 
diagonal matrices $diag({\bf p}_k)$:
\begin{equation}
  {\bf P}=\left[\begin{array}{c}diag({\bf p}_1)\\
      \vdots\\ diag({\bf p}_K)\end{array}\right],\;\;\;\;\;
  diag({\bf p}_k)=\left[\begin{array}{cccc}p_1^k & 0 & \cdots & 0\\
      0 & p_2^k & \cdots & 0\\\vdots & \vdots & \ddots & \vdots\\
      0 & \cdots & 0 & p_N^k
    \end{array}\right],\;\;\;\;\;(k=1,\cdots,K)
\end{equation}
so that
\begin{equation}
  {\bf PP}^T=\left[\begin{array}{c}diag({\bf p}_1)\\
      \vdots\\ diag({\bf p}_K)\end{array}\right]
  \left[diag({\bf p}_1),\cdots,diag({\bf p}_K)\right]
  =\left[\begin{array}{ccc}diag({\bf p}_1^2) & \cdots & diag({\bf p}_1) diag({\bf p}_K)\\
      \vdots & \ddots & \vdots \\
      diag({\bf p}_K) diag({\bf p}_1) & \cdots & diag({\bf p}_K^2) 
    \end{array}\right]
\end{equation}
with
\begin{equation}
diag({\bf p}_k) diag({\bf p}_l)=\left[\begin{array}{cccc}
    p_1^kp_1^l & 0 & \cdots & 0\\
    0 & p_1^kp_1^l & \cdots & 0\\
    \vdots & \vdots & \ddots & \vdots \\
    0 & \cdots & 0 & p_N^kp_N^l \end{array}\right]
\end{equation}

Having found both the gradient
${\bf g}_\psi={\bf y}-{\bf p}-{\bf K}^{-1}{\bf f}$ and Hessian 
${\bf H}_\psi=-({\bf K}^{-1}+{\bf W})$, we can further find the
mean ${\bf m}_{f|D}$ at which $\psi({\bf f})$ achieves maximum by 
the following iteration of 
\htmladdnormallink{Newton's method}{../ch3/node6.html}:
\begin{eqnarray}
  {\bf f}_{n+1}&=&{\bf f}_n-{\bf H}_\psi^{-1}{\bf g}_\psi
  ={\bf f}_n+({\bf K}^{-1}+{\bf W})^{-1}({\bf y}-{\bf K}^{-1}{\bf f}_n-{\bf p})
  \nonumber\\
  &=&{\bf f}_n+({\bf K}^{-1}+{\bf W})^{-1}(-({\bf K}^{-1}+{\bf W}){\bf f}_n+{\bf W}{\bf f}_n+{\bf y}-{\bf p})
  \nonumber\\
  &=&({\bf K}^{-1}+{\bf W})^{-1}({\bf W}{\bf f}_n+{\bf y}-{\bf p})
\end{eqnarray}
We note that during the iteration, we need to update not only 
${\bf f}$, but also ${\bf p}$ as a functions of ${\bf f}$ given 
in Eq. (\ref{ppp}).

Now that we have got both ${\bf m}_{f|D}$ and ${\bf\Sigma}_{f|D}$ of
${\bf f}$, we can further get ${\bf m}_{f_*}$ and ${\bf\Sigma}_{f_*}$
of ${\bf f}_*$:

\begin{itemize}
\item Get mean ${\bf m}_{f_*}$:

  Similar to Eq. (\ref{MeanFsY}) in the binary case, we have 
  the following based on ${\bf f}={\bf K}({\bf y}-{\bf p})$ in 
  Eq. (\ref{MeanFMC}):
  \begin{eqnarray}
    {\bf m}_{f_*} &=& E_f({\bf m}_{f_*|f})
    =E_f ({\bf K}_*^T{\bf K}^{-1}{\bf f})={\bf K}_*^T{\bf K}^{-1}E_f({\bf f})
    \nonumber\\
    &=&{\bf K}_*^T{\bf K}^{-1}\,[{\bf K}({\bf y}-{\bf p}_{m_f})]
    ={\bf K}_*^T({\bf y}-{\bf p}_{m_f})
  \end{eqnarray}
  As the $K$ classes are uncorrelated, i.e., ${\bf K}$ is block-diagonal,
  the above can be separated into $K$ equations each for one of the classes:
  \begin{equation}
    {\bf m}_{f_*^k}=({\bf K}_*^k)^T({\bf y}^k-{\bf p}^k),
    \;\;\;\;\;\;(k=1,\cdots,K)
  \end{equation}

\item Get covariance ${\bf\Sigma}_{f_*}$:

  Similar to Eq. (\ref{CovarianceFsY}) in the binary case, we have
  the following based on ${\bf\Sigma}_{f|D}=({\bf K}^{-1}+{\bf W})^{-1}$ 
  in Eq. (\ref{SigmaF}):
  \begin{equation}
    {\bf\Sigma}_{m_{f_*|f}}
    ={\bf K}_*^T {\bf K}^{-1}\,{\bf\Sigma}_{f|D} \,{\bf K}^{-1}{\bf K}_*
    ={\bf K}_*^T {\bf K}^{-1} ({\bf K}^{-1}+{\bf W})^{-1} {\bf K}^{-1}{\bf K}_*
  \end{equation}
  where ${\bf W}=diag({\bf p})-{\bf PP}^T$ is given in Eq. (\ref{WWW}) 
  with ${\bf p}$ evaluated at ${\bf m}_{f|D}$. Then, similar to
  Eq. (\ref{CovarianceFsY}), we get
  \begin{equation}
    {\bf\Sigma}_{f_*}={\bf K}_{**}-{\bf K}_*^T({\bf K}+{\bf W}^{-1})^{-1}{\bf K}_*
  \end{equation}
\end{itemize}

Now we can further get the probability for ${\bf x}_*$ to belong to
$C_k$ based on the softmax function in Eq. (\ref{SoftmaxGPC}) 
\begin{equation}
  p_*^k=\frac{\exp(m_{f_*}^k)}{\sum_{l=1}^K\exp(m_{f_*}^l)},
  \;\;\;\;\;\;(k=1,\cdots,K)
\end{equation}
and classifiy ${\bf x}_*$ to class $C_k$ if 
$p_*^k=\max\{ p_*^1,\cdots,p_*^K \}$.

Moreover, the certainty or confidence of this classification result 
can be found from ${\bf\Sigma}_{f_*}$.


The Matlab code for the essential parts of the algorithm is 
listed below. First, the following code segment carries out the 
classification of $n$ given test samples based on the $N$ training 
set ${\cal D}=\{{\bf X},{\bf y}\}$. 

\begin{verbatim}
    K=Kernel(X,X);                        % covariance of prior of p(f|X)
    Ks=Kernel(X,Xs);
    Kss=Kernel(Xs,Xs);
    [meanf Sigmaf p W]=findPosteriorMeanMC(K,y,C);  
                                          % find mean and covariance of p(f|D), and W, p
    Sigmafy=Kss-Ks'*inv(K+inv(W))*Ks;     % covariance of p(f|D) 
    p=reshape(p,N,C);
    y=reshape(y,N,C);
    for k=1:C
        meanfD(:,k)=Ks'*(y(:,k)-p(:,k);   % mean of p(f_*|X_*,D) for kth class
    end
    for i=1:n                             % for each of n test samples
        d=sum(exp(meanfD(i,:)));          % denominator of softmax function
        [pmax k]=max(exp(meanfD(i,:))/d); % find class with max probability
        ys(i)=k;                          % label ith sample as member of kth class
        pr(i,k)=pmax;                     % probility of ith sample belonging to kth class 
    end
\end{verbatim}


The code segment above calls the following function which computes the
mean and covariance of $p({\bf f}|{\bf X},{\bf y})$ by Newton's method:

\begin{verbatim}
function [meanf Sigmaf p W]=findPosteriorMeanMC(K,y,C)     
    % find mean and covariance of p(f|X,y) by Newton's method, based on D={X,y}
    N=length(y);                    % number of training samples
    f0=zeros(n,1);                  % initial value of latent function
    f=zeros(n,1);        
    er=1;
    k=0;  
    while er > 10^(-9)              % find f that maximizes p(f|X,y)
        k=k+1;
        [p W]=findp(f,N,C);         % call function findp to find vector p and matrix W
        f=inv(inv(K)+W)*(W*f0+y-p); % iteratively update value of f
        er=norm(f-f0);              % difference between consecutive iterations
        f0=f;                       % update f
    end
    meanf=f;
    Sigmaf=inv(inv(K)+W);            
end

\end{verbatim}

The following function called by the previous function finds vector 
${\bf p}$ and matrix ${\bf W}=diag({\bf p})-{\bf PP}^T$:

\begin{verbatim}

function [p W]=findp(f,N,C)         % find vector p and matrix W=diag(p)-P*P'
    F=reshape(f,N,C)';              % kth row contains N samples of class k
    for i=1:N                       % for each of N training samples
        d=sum(exp(F(:,i)));         % sum of all C terms in denominator
        for k=1:C                   % for all C classes 
            p(k,i)=exp(F(k,i))/d;
        end
    end
    P=[];
    for k=1:C                       % generate P
        P=[P; diag(p(k,:))];        % stack C diagonal matrices
    end
    p=reshape(p',N*C,1);            % turn p into a vertical vector
    W=diag(p)-P*P';                 % generate W matrix
end

\end{verbatim}

{\bf Example 1:}

This example shows the classification result of the same dataset of
three classes used before. The top two panels show the distributions
of the three classes in the training data set. The bottom two panels
show the classification results in terms of the partitioning of the
feature space (bottom left) and the posterior distribution
$p({\bf f}_*|{\bf X},{\bf y},{\bf X}_*)$ (bottom right), which can be
compared with the distribution of the training set (top right). The 
confusion matrix of the classification result is shown below, with the
error rate $30/600=0.045$:

\begin{equation}
\left[\begin{array}{rrr} 
 197 &    3 &    0   \\
   0 &  195 &    5   \\
   1 &   21 &  178   \\
\end{array}\right]
\end{equation}
\htmladdimg{../figures/GPMCexample1.png}

{\bf Example 2:}

This example shows the classification of the XOR data set. The confusion
matrix of the classification result is shown below, with the error rate 
$13/400=0.0325$:

\begin{equation}
\left[\begin{array}{rr} 
 193 &    7   \\
   6 &  194   
\end{array}\right]
\end{equation}

\htmladdimg{../figures/GPMCexample1a.png}

We see that in both examples, the error rates of the GPC method are lower
than those of the naive Bayesian method. However, the naive Bayesian method
does not have the overfitting problem, while in the method of GPC, we may 
need to carefully adjust the parameter of squared exponential for the kernel
functions to make proper tradeoff between overfitting and error rate.



\section{Hierarchical (Tree) Classifiers}


Both supervised classification and unsupervised clustering can be carried
out in a hierarchical fashion to classify the input patterns or group them
into clusters, very much like the hierarchy of biological classifications
with different 
\htmladdnormallink{taxonomic ranks}{https://en.wikipedia.org/wiki/Taxonomic_rank}
(domain, kingdom, phylum, class, order, family, genus, and species). 

\begin{itemize}
\item {\bf Unsupervised clustering}

  The hierarchical clustering can be obtained in either a top-down or bottom 
  up manner. 

  \begin{itemize}

  \item Top-down method:

    All patterns in the data set are initially 
    treated as a single cluster as the root of the tree, which is then 
    subdivided (split) into a set of two or more smaller clusters, each 
    represented as a node in the tree structure. This process is carried 
    out recursively until eventually each cluster contains only one pattern, 
    represented as a leaf node of the tree. 

  \item Bottom-up method:

    every pattern in the data set is initially 
    treated as a cluster as a leaf node of the tree, which will then be merged 
    to form larger clusters. Again, this process is carried out recursively 
    until eventually all patterns are merged into a single cluster at the root 
    of the tree. 

  \end{itemize}

  In either the top-down or the bottom-up method, the specific method for 
  the splitting or merging at each tree node is based on certain similarity
  measurement such as the distance between two clusters. The resulting tree
  structure obtained by either method can then be truncated at any level
  between the root and the leaf nodes to obtain a set of clusters, depending
  on the desired number and sizes of these clusters.  

\item {\bf Supervised classification}

  If labeled training data are available, both the top-down and the bottom-up 
  clustering methods can also be used in the training stage of the supervised 
  classification methods, with the only difference that now the splitting or
  merging is applied to labeled classes instead of individual patterns, and 
  each leaf node represents one of the classes, rather than a single pattern. 
  After the tree structure is obtained, the training is complete and any 
  unlabeled pattern can be classified at the tree root and then subsequently 
  the tree nodes at lower levels until it is classified into one of the leaf 
  nodes of the tree, corresponding to a specific class.

  This hierarchical classification method is especially useful when the number 
  of classes and the number $D$ of feature are both large. In this case it may
  be very difficult to select a subset of $d<D$ features good for separating 
  all classes for a single-level classifier, by which {\em all} classes need 
  to be classified at the same time, requiring, most likely, all $N$ features. 
  However, for a tree classifier, since each node is a two-class classifier, 
  it is possible to select a small number of $d\ll D$ features that are most 
  relevant and suitable to represent the two subsets of classes. 

\end{itemize}

In the following we consider both the bottom-up and top-down methods for 
hierarchical clustering/classification.

{\bf Bottom-Up method}

  The bottom-up hierarchical classifier is trained based on $K$ classes 
  $C_1,\cdots,C_K$, each containing $n_k$ ($k=1,\cdots,K$) labeled
  patterns ${\bf x}\in C_k$.

  \begin{enumerate}

  \item Compute the $K(K-1)/2$ pairwise Bhattacharyya distances between
    every two classes $C_i$ and $C_j$:
    \begin{equation} 
    d_B(C_i,C_j)
    =\frac{1}{4}({\bf m}_i-{\bf m}_j)^T\left[\frac{{\bf \Sigma}_i+{\bf \Sigma}_j}{2}\right]^{-1}({\bf m}_i-{\bf m}_j)+\log\left[\frac{\left|\frac{{\bf \Sigma}_i+{\bf \Sigma}_j}{2} 
        \right|}{(\left|{\bf \Sigma}_i\right|\;\left|{\bf \Sigma}_j\right|)^{1/2}}\right]
    \end{equation}
  
  \item Merge the two classes corresponding to the smallest $d_B$ to form 
    a new class $C_i \cup C_j = C_k$, compute its mean and covariance:
    \begin{equation}	
      {\bf m}_k=\frac{1}{n_i+n_j}[n_i {\bf m}_i+n_j {\bf m}_j]	
    \end{equation}
    and
    \begin{equation}
      {\bf \Sigma}_k=\frac{1}{n_i+n_j}
      [n_i (\Sigma_i+({\bf m}_i-{\bf m}_k)({\bf m}_i-{\bf m}_k)^T)+
        n_j (\Sigma_j+({\bf m}_j-{\bf m}_k)({\bf m}_j-{\bf m}_k)^T ) ]
    \end{equation}
    Delete the old classes $C_i$ and $C_j$. Now there are $K-1$ 
    classes left.

  \item Compute the distance between the new class $C_k$ and all $K-2$
    remaining classes.

  \item Repeat the previous steps until eventually all classes are merged 
    into a single group containing all $K$ classes, the binary tree 
    structure is thus obtained.

  \end{enumerate}
  
{\bf Top-Down method}

  Generate a binary tree by recursively partitioning all classes
  into two sub-groups with the maximum Bhattacharyya distance

  \begin{enumerate}
  \item Compute the between-class scatter matrix ${\bf S}_B$ of the 
    $K$ classes, find its maximum eigenvalue $\lambda_i$ and the 
    corresponding eigenvectors ${\bf v}_i$;

  \item Project all data points onto ${\bf v}_1$:
    \begin{equation} 
    y_n={\bf x}^T_n {\bf v}\;\;\;\;\;\;(n=1,\cdots,N)
    \end{equation}

  \item Sort all data points $\{y_1,\cdots,y_N\}$ along this 1-D space
    and partition them into two subgroups with maximum Bhattacharyya
    (between-group) distance.

  \item Carry out the steps above recursively to each of the two
    subgroups, until eventually every subgroup contains only one 
    classes

  \end{enumerate}
    
Once the hierarchical structure is constructed by either the bottom-up
or top-down method, we need to build a binary classifier at each node
of structure, by which any given pattern is classified into either the
left group $G_l$ or right group $G_r$:

\begin{itemize}
\item According to the specific classification method used, find
  the discriminant functions $D_l({\bf x})$ and $D_r({\bf x})$ for the
  two subgroups based on the training data.

\item Select the best $d<D$ features most suitable for separating the 
  two groups $G_l$ and $G_r$, based on any of the feature selection 
  methods such as those listed below:
  \begin{itemize}
  \item Choosing $d$ features directly from the $D$ original ones using 
    between-class distance (Bhattacharrya distance) as the criterion,
      
  \item Carry out KLT based on the between-class scatter matrix ${\bf S}_B$
    and use the first $d$ principal components for the binary classification.
  \end{itemize}
  As here only two groups of classes need to be distinguished, the number
  of features $M$ can be expected to be small.

\item Any unlabeled pattern ${\bf x}$ enters the classifier at the root of
    the tree and is classified to either the left or the right sub-group
    of the node according to the discriminant function
    \begin{equation} 
      {\bf x} \in \left\{ \begin{array}{ll} G_l & if \;\;D_l({\bf x}) > D_r({\bf x}) \\
        G_r & if \;\;D_r({\bf x}) < D_l({\bf x}) \end{array} \right. 
    \end{equation}
    This process is carried out recursively at each of the subsequent nodes
    until eventually ${\bf x}$ reaches one of the leaf nodes corresponding 
    to a single class, to which the sample ${\bf x}$ is therefore classified. 
\end{itemize}

\begin{comment}

When both the number of classes $c$ and the number of features $N$ are large, 
the feature selection and classification discussed before may encounter some
difficulties because of the following reasons.
\begin{itemize}
\item Feature selection is no longer effective as it is difficult to find $M$ 
  out of $N$ features which are suitable for separating {\em all} of the $c$ 
  classes (some features may be good from some classes but not good for others).
\item Classification is costly as a large number of features are necessary.
\end{itemize}

The solution is to carry out the classification in several steps implemented 
as a {\em hierarchical (tree) classifier}, which can be constructed by either 
a bottom-up (merge) or top-down (split) algorithm based on the training data
of known classes.


{\bf Bottom-Up Classifier}

\begin{enumerate}
\item From the training samples of each class $C_i\;\;\;(i=1,\cdots,C)$,
  estimate the mean and covariance:
  \begin{equation} {\bf m}_i=\frac{1}{K_i}\sum_{{\bf x} \in C_i} {\bf x},\;\;\;\;\;\;\;\;
  {\bf \Sigma}_i=\frac{1}{K_i}\sum_{{\bf x} \in C_i} ({\bf x}-{\bf m}_i)({\bf x}-{\bf m}_i)^T	\end{equation}

\item Compute Bhattacharyya distances for every pair of different classes 
  ($c(c-1)/2$ of them in total):
  \begin{equation} 
  D_{ij}=\frac{1}{4}({\bf m}_i-{\bf m}_j)^T\left[\frac{{\bf \Sigma}_i+{\bf \Sigma}_j}{2}\right]^{-1}({\bf m}_i-{\bf m}_j)+\log\left[\frac{\left|\frac{{\bf \Sigma}_i+{\bf \Sigma}_j}{2} 
      \right|}{(\left|{\bf \Sigma}_i\right|\;\left|{\bf \Sigma}_j\right|)^{1/2}}\right]
  \;\;\;\;\;\mbox{for all $i>j$}		\end{equation}
  
\item Merge the two classes corresponding to the smallest $D_{ij}$ to form a 
  new class:

  $C_i \cup C_j = C_k$ and compute its mean and covariance:
  \begin{equation}	{\bf m}_k=\frac{1}{K_i+K_j}[K_i {\bf m}_i+K_j {\bf m}_j]	\end{equation}
  and
  \begin{equation}	{\bf \Sigma}_k=\frac{1}{K_i+K_j}[K_i (\Sigma_i+({\bf m}_i-{\bf m}_k)({\bf m}_i-{\bf m}_k)^T)+
    K_j (\Sigma_j+({\bf m}_j-{\bf m}_k)({\bf m}_j-{\bf m}_k)^T ) ]
  \end{equation}
  Delete the old classes $C_i$ and $C_j$. Now there are $c-1$ 
  classes left.

\item Compute the distance between the new class $C_k$ and all remaining
  classes.

\item Repeat the above steps until eventually all classes are merged into one
  and a binary tree structure is thus obtained.

\item At each node of the tree build a 2-class classifier to be used to
  classify a sample into one of the two children $G_l$ and $G_r$ representing 
  the two groups of classes. According to the classification method used, we
  find the discriminant functions $D_l({\bf x})$ and $D_r({\bf x})$.

\item At each node of the tree adaptively select features that are best
  for separating the two groups of classes $G_l$ and $G_r$. Any feature 
  selection method can be used here, such as directly choosing $M$ from
  $N$ features using between-class distance (Bhattacharrya distance) as 
  the criterion, or feature selection using some orthogonal transform 
  (KLT, DFT, WHT, etc.). Only a small number of selected features may 
  be needed as here only two groups of classes need to be distinguished.

\end{enumerate}

After the classifier is constructed in the training process, the classification
can be carried out. Specifically, a testing sample ${\bf x}=[x_1,\cdots,x_N]^T$
of unknown class enters the classifier at the root of the tree and is classified
to either the left or the right child of the node according to
\begin{equation} 
  {\bf x} \in \left\{ \begin{array}{ll} G_l & if \;\;D_l({\bf x}) > D_r({\bf x}) \\
  G_r & if \;\;D_l({\bf x}) < D_r({\bf x}) \end{array} \right. 
\end{equation}

This process is repeated recursively at the child node (either $G_l$ or $G_r$)
and its child and so on, until eventually ${\bf x}$ reaches a leaf node 
corresponding to a single class, to which the sample ${\bf x}$ is therefore 
classified. 


{\bf Top-Down Hierarchical Clustering}


Unsupervised clustering can also be implemented hierarchically in a top-down 
manner. The algorithm generates a binary tree by recursively partitioning 
all patterns, treated as vectors in an N-dimensional vector space, into two 
sub-groups in the following two steps:
\begin{enumerate}
\item Compute the covariance matrix of the samples and then apply PCA method.
  Specifically, find the eigenvector ${\bf \phi}$ of the covariance matrix 
  ${\bf \Sigma}$ corresponding to the largest eigenvalue $\lambda$. Project
  all patterns ${\bf x}$ onto a 1-D space:
  \begin{equation} x={\bf x}^T {\bf \phi} \end{equation}
\item Sort all data point along this 1-D space and partition them into two
  subgroups with maximum between-group (Bhattacharyya) distance.
\item Recursively carry out the two steps above to each subgroup, until 
  each subgroup contains only one pattern. Alternatively, the recursion can
  be terminated when certain criterion is satisfied.
\end{enumerate}

\end{comment}

{\bf Example}

The hierarchical clustering method is applied to a dataset composed of
seven normally distributed clusters each containing 25 sample vectors in an $N=4$
dimensional space. The PCA method is used to project the data in 4-D space into 
a 2-D space spanned by the first two principal components, as shown below:

\htmladdimg{../figures/TreeClustering1.png}

The clustering result is shown below. Each column in the display represents the
four components of a 4-D vector, color coded by a spectrum from red (low values)
through green (middle) to blue (high values).

\htmladdimg{../figures/TreeClustering.png}

See 
\htmladdnormallink{more examples}{http://fourier.eng.hmc.edu/bioinformatics/intro/index.html} in clustering analysis applied to gene data analysis in bioinformatics.

An example of this method is available \htmladdnormallink{here}{http://fourier.eng.hmc.edu/bioinformatics/intro/node12.html}.




\section{Clustering Analysis}

All classification algorithms discussed above are supervised in nature 
based on the assumped availability of some training data in which each 
of the data patterns ${\bf x}_n\in {\bf X}=[{\bf x}_1,\cdots,{\bf x}_N\}]$
is labeled by $y_n\in{\bf y}=[y_1,\cdots,y_n]^T$, i.e., the class they 
each belong is known. However, when such training set is not available,
there is still the need to explore some potential structure of the data 
in the form of a set of unknown number $K$ of clusters $\{C_1,\cdots,C_K\}$,
each composed of a set of similar data points close to each other in the 
feature space. This can be accomplished by the unsupervised method of 
{\em clustering analysis}, based only on the given dataset 
${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$, without any additional prior
knowledge, such as the labeling of the data samples. We will now consider 
a few algorithms for clustering analysis.


\subsection{K-means clustering}

As suggested by the name, the K-means clustering uses a set of $K$ 
mean vectors ${\bf m}_1,\cdots,{\bf m}_K$ to represent the clusters 
in the feature space, based on the assumption that there exist a set
of $K$ clusters in the dataset. 

The K-means clustering algorithm can be formulated as an optimization
problem to minimize an objective function
\begin{equation}
  J=\sum_{n=1}^N\sum_{k=1}^K P_{nk}||{\bf x}_n-{\bf m}_k||^2
\end{equation}
where 
\begin{equation}
  P_{nk}=\left\{\begin{array}{ll}
  1 & \mbox{if $k=\argmin_l ||{\bf x}_n-{\bf m}_l||$}\\
  0 & \mbox{otherwise}
  \end{array}\right.
\end{equation}
indicates ${\bf x}_n$ is assigned to the kth cluster $C_k$ if its
distance to the mean ${\bf m}_k$ is minimum. To minimize the objective 
function $J$ with respective to ${\bf m}_k$, we set its derivative with
respect to ${\bf m}_k$ to zero:
\begin{equation}
  \frac{d}{d{\bf m}_k}J=2\sum_{n=1}^N P_{nk}||{\bf x}_n-{\bf m}_k||={\bf 0}
\end{equation}
and solve for ${\bf m}_k$:
\begin{equation}
  {\bf m}_k=\frac{\sum_{n=1}^N P_{nk}{\bf x}_n}{\sum_{n=1}^N P_{nk}}
  =\frac{1}{N_k}\sum_{n=1}^N P_{nk}{\bf x}_n
\end{equation}
where $N_k=\sum_{n=1}^N P_{nk}$ is the number of all samples assigned to $C_k$,
and ${\bf m}_k$ is their mean.

The K-means algorithm is an iterative process that starts with $K$ randomly
initialized mean vectors which are iteratively revised until the eventual
convergence. Here are steps of the process:

\begin{enumerate}

\item Step 0: Initialize randomly the mean vectors for the $K$ clusters,
  such as any $K$ samples of the dataset:
  ${\bf m}_1^{(0)},\;{\bf m}_2^{(0)},....,{\bf m}_K^{(0)}$,
  set iteration index to zero $l=1$;

\item Assign every sample ${\bf x}\in{\bf X}$ in the dataset to one of the
  $K$ clusters according to its distance to the corresponding mean vector:
  \begin{equation}
    \mbox{if}\;\;\;
    ||{\bf x}-{\bf m}_k^{(l)}||^2=\min_{1\le l\le K} \;||{\bf x}-{\bf m}_l^{(l)}||^2,
    \;\;\;\mbox{then}\;\;\;\;{\bf x} \in C_k^{(l)}
  \end{equation}
  where $C_k^{(l)}$ denotes the kth cluster with mean vector ${\bf m}_k^{(l)}$
  in the lth iteration;

\item Update the mean vectors to get the new mean ${\bf m}_k^{(l+1)}$ 
  so that the objective function $J$ given above, i.e., the sum of 
  the distances squared from all ${\bf x}\in C_k$ to ${\bf m}_k^{l+1}$ 
  is minimized:
  \begin{equation} 
    {\bf m}_k^{(l+1)}=\frac{1}{N_k} \sum_{{\bf x} \in C_k} {\bf x},\;\;\;\;\;
    (k=1,\cdots,K)	
  \end{equation}   

\item Terminate if the algorithm has converged:
  \begin{equation} 
    {\bf m}_k^{(l+1)}={\bf m}_k^{(l)}\;\;\;\;(k=1,\cdots,K)	
  \end{equation}
  Otherwise, $l \leftarrow l+1 $, go back to Step 2.
\end{enumerate}

This method is simple and effective, but it has the main drawback that the 
number of clusters $K$ needs to be estimated based on some prior knowledge, 
and it stays fixed through out the clustering process, even it may turn out 
later that more or fewer clusters may fit the data better. One way to resolve
this is to carry out the algorithm multiple times with different $K$, and
then evaluate each result based on the objective function $J$, or some 
other separability criteria, such as $tr({\bf S}_T^{-1}{\bf S}_B)$.

The Matlab code for the iteration loop of the algorithm is listed below,
where \verb|Mold| and \verb|Mnew| are respectively the $K$ mean vectors 
before and after each modification. The iteration terminates when the
mean vectors no longer change.

\begin{verbatim}

    Mnew=X(:,randi(L,1,K));          % use any K data points as initial means

    it=0;
    er=inf;
    while er>0                       % main iteration
        it=it+1;
        Mold=Mnew;
        Mnew=zeros(N,K);             % initialize new means
        Number=zeros(1,K);     
        for i=1:L                    % for all data points 
            x=X(:,i);
            dmin=inf;
            for k=1:K                % for all K clusters
                d=norm(x-Mold(:,k));
                if d<dmin
                    dmin=d;  j=k;
                end
            end
            Number(j)=Number(j)+1;
            Mnew(:,j)=Mnew(:,j)+x;
        end 
        for k=1:K
            if Number(k)>0
                Mnew(:,k)=Mnew(:,k)/Number(k);
            end
        end
        er=norm(Mnew-Mold);          % terminate if means no longer change
    end

\end{verbatim}

{\bf Example 1} 

The K-means algorithm is applied to a simulated dataset in 3-D space with 
$C=4$ clusters. The results are shown in the figure below, where the three 
panels show the results corresponding to $K=C-1$ (left), $K=C$ (middle), 
and $K=C+1$ (right). The initial positions of the $K$ means are marked 
by the black squares, while their subsequent positions through out the 
iteration are marked by smaller dots. The iteration terminates once the
means have moved to the centers of the clusters and no longer change 
positions. 

\htmladdimg{../figures/KmeansEx3.png}

The clustering results corresponding to $K=3,\,4,\,5$ can be evaluated 
by the separability $tr ({\bf S}_T^{-1}{\bf S}_B)$, the $K$ intra-cluster 
istance $tr({\bf\Sigma}_k)\,\;(k=1,\cdots,K)$ of the resulting clusters:
\begin{equation}
\begin{tabular}{c||ccc|cccc|ccccc}\hline
  &\multicolumn{3}{c}{K=C-1=3} & \multicolumn{4}{|c|}{K=C=4} & 
  \multicolumn{5}{c}{K=C+1=5} \\\hline\hline
  \mbox{Separability} & \multicolumn{3}{c}{1.76}&\multicolumn{4}{|c|}{2.56}
  & \multicolumn{5}{c}{2.58}\\\hline
  \mbox{Intra-cluster distance}
& 9.1 & 44.3 & 11.8&10.8&12.7&11.1&9.1&10.8&9.1&11.1&8.9&9.4\\\hline
\end{tabular}
\end{equation}
and the $K(K-1)/2$ inter-cluster (Bhattacharyya) distances (between any 
two of the $K$ clusters):
\begin{equation}
\begin{tabular}{r|rr}\hline
  \multicolumn{3}{c}{K=C-1=3}    \\\hline\hline
  & 1    & 2     \\\hline
2 & 10.9 &       \\
3 & 21.9 & 184.7 \\\hline
\end{tabular}
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
\begin{tabular}{r|rrr}\hline
\multicolumn{4}{c}{K=C=4}\\\hline\hline
  & 1   & 2   & 3   \\\hline
2 & 4.0 &     &     \\
3 & 5.1 & 1.6 &     \\
4 & 2.4 & 2.3 & 4.4 \\\hline
\end{tabular}
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
\begin{tabular}{r|rrrr}\hline
  \multicolumn{5}{c}{K=C+1=5}          \\\hline\hline
  & 1   & 2    & 3   & 4    \\\hline
2 & 4.0 &      &     &      \\
3 & 5.1 &  1.6 &     &      \\
4 & 7.0 &  4.3 & 0.3 &      \\
5 &17.3 & 15.5 & 8.9 & 11.1 \\\hline
\end{tabular}
\end{equation}
We see that when $K=3<C=4$, the intra-cluster distance of the 2nd cluster is 
significantly greater than the other two, indicating the cluster may contain
two smaller clusters. Also, when $K=5>C=4$, the inter-cluster distance between 
clusters 3 and 4 is significantly smaller than others, indicating the two 
clusters are too close and can therefore be merged.

%\htmladdimg{../figures/KmeansEx2.png}


While the K-means method is simple and effective, it has the main shortcoming 
that the number of clusters $K$ needs to be specified, although in practice it 
is typically unknown a head of time. In this case, one could try different $K$ 
values and compare the corresponding results in terms of the intra and inter 
cluster distances, as well as the separabilities of the resulting clusters, as 
shown in the example above. Moreover, if the intra-cluster distance of a cluster 
is too large indicating it may contain more than one cluster (e.g., $K=3$ in 
the example), it can be split; on the other hand, if the inter-cluster distance 
between two clusters is too small (e.g., $K=5$ in the example), the two clusters
may belong to the same cluster and need to be merged. Following such merging 
and/or splitting, a few more iterations can be carried out to make sure the
final clustering results are optimal. The figure below shows the clustering
results of the same dataset above but modified by merging and splliting. The
left pannel is for $K=3$, but the second cluster (green) is split, while the
right pannel is for $K=5$, when the 4th (yellow) and 5th (cyan) clusters are 
merged.

\htmladdimg{../figures/KmeansEx3a.png}


The idea of modifying the clustering results by merging and spliting leads to 
the algorithm of Iterative Self-Organizing Data Analysis Technique (ISODATA), 
which allows the number of clusters $K$ to be adjusted automatically during the 
iteration by merging clusters close to each other and splitting clusters with 
large intra-cluster distances. However, this algorithm is highly heuristic as 
the various parameters such the threshold values for merging and splitting need
to be specified.

{\bf Example 2}

The K-means clustering method is applied to the dataset of ten digits from 
0 to 9 used previously. The resulting clustering is visualized based on the
KLT to map the data points in the original 256-D space into the 3-D space 
spanned by the three eigenvectors corresponding to the three greatest 
eigenvalues of the covariance matrix of the dataset. The ground truth (with
known class labelings) is shown on the left, and the clustering result is
shown on the right below. We see that the clustering results match the
original data reasonably well.

\htmladdimg{../figures/KmeansEx10.png}

The confusion matrix shown below is still used to show the clustering 
results when compared with the ground truth, where the element in the
ith row and jth column is the number of samples in the ith cluster 
(ground truth) but assigned to the jth cluster. 

\begin{equation}
\left[ \begin{array}{rrrrrrrrrr}
     1 &  27 &   3 & 166 &   0 &   0 &   3 &  24 &   0 &   0 \\
     0 &   1 &  13 &   0 & 210 &   0 &   0 &   0 &   0 &   0 \\
     0 &   3 &  16 &   0 &   3 & 180 &   9 &   8 &   5 &   0 \\
     1 &   0 &  88 &   1 &   0 &   0 &   5 &   1 & 128 &   0 \\
     4 &   1 &  52 &   0 &  26 &   0 &   0 &   6 &   0 & 135 \\
     1 &   1 &  10 &   0 &   0 &   0 &   0 & 167 &  43 &   2 \\
     0 & 161 &   5 &  14 &  13 &   2 &   0 &  29 &   0 &   0 \\
     4 &   0 &  70 &   0 &   5 &   4 & 137 &   3 &   1 &   0 \\
     4 &   1 &  69 &   2 &   1 &   3 &   2 &  31 & 110 &   1 \\
    92 &   0 & 101 &   1 &   1 &   1 &   5 &   0 &   2 &  21 \\
%     1 &   2 &  17 &   3 &   0  & 31 &   0 &   1 & 169 &   0 \\
%     0 &   0 &   0 &   0 &   5  & 12 & 207 &   0 &   0 &   0 \\
%     7 &  12 & 154 &   0 &   4  & 47 &   0 &   0 &   0 &   0 \\
%   119 &   5 &   1 &   0 &  73  & 23 &   0 &   2 &   1 &   0 \\
%     0 &   0 &   0 &   0 &  30  & 34 &  25 &  11 &   0 & 124 \\
%    30 &   1 &   1 & 118 &   7  & 63 &   0 &   1 &   1 &   2 \\
%     0 &   0 & 126 &   0 &   0  & 88 &   1 &   0 &   8 &   1 \\
%     1 & 143 &   0 &   0 &  49  & 22 &   5 &   4 &   0 &   0 \\
%    99 &   3 &   5 &   9 &  56  & 42 &   1 &   6 &   1 &   2 \\
%     1 &   7 &   1 &   1 & 103  &  8 &   1 &  81 &   1 &  20 \\
  \end{array}\right]
\end{equation}

\htmladdimg{../figures/KmeansDigits.png}



\subsection{Gaussian mixture model}

%https://www.ics.uci.edu/~smyth/courses/cs274/readings/domke_notes_on_EM.pdf

%In general, a mixture model is a probabilistic model for 
%representing the subpopulations within an overall population. 

The {\em Gaussian mixture model (GMM)} models the given dataset 
${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$ by a linear combination of 
$K$ Gaussian distributions:
\begin{equation}
  p({\bf x})=\sum_{k=1}^K P_k {\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k)
  =\sum_{k=1}^K P_k \left[
  \frac{1}{(2\pi)^{d/2}|{\bf\Sigma}_k|^{1/2}}
  \exp\left(-\frac{1}{2}({\bf x}-{\bf m}_k)^T
  {\bf\Sigma}_k^{-1}({\bf x}-{\bf m}_k)\right)\right]
  \label{GMM}
\end{equation}
where $P_k$ is the weight for the kth Gaussian 
${\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k)$, satisfying
\begin{equation}
  \int_{-\infty}^\infty p({\bf x})\,d{\bf x}
  =\sum_{k=1}^K P_k \int_{-\infty}^\infty
  {\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k)\,d{\bf x}
  =\sum_{k=1}^K P_k =1
\end{equation}

This GMM model, in combination with the method of 
\htmladdnormallink{\em expectation maximization (EM)} 
{https://en.wikipedia.org/wiki/Expectation-maximization_algorithm},
can be applied to clustering analysis. Specifically, we first 
model the clusters $\{C_1,\cdots,C_K\}$ by $K$ Gaussians
${\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k),\;(k=1,\cdots,K)$, 
then estimate all model parameters denoted by 
$\theta =\{P_k,{\bf m}_k,{\bf\Sigma}_k,\;(k=1,\cdots,K)\}$ 
based on the given dataset, and finally obtain the probability 
$P_{nk}=P({\bf x}_n\in C_k)$ for ${\bf x}_n$ to belong to $C_k$ 
for all $n=1,\cdots,N$ and $k=1,\cdots,K$, and assign ${\bf x}_n$ 
to $C_k$ is $P_{nk}=\max_l P_{nl}$.

Note that the GMM model in Eq. (\ref{GMM}) is actually the same 
as Eq. (\ref{pofxNB}) in the naive Bayes classification. These two
methods are similar in the sense that each cluster or classe $C_k$
is modeled by a Gaussian ${\cal N}({\bf x},{\bf m}_k,{\bf\Sigma}_k)$, 
weighted by $P_k$, and the model parameters ${\bf m}_k$ and ${\bf\Sigma}_k$,
as well as $P_k$, need to be estimated based on the given dataset. 
However, the two methods are different in that the dataset ${\bf X}$ 
in the supervised naive Bayes method is labeled by ${\bf y}$, while
here in GMM the dataset is not labeled. However, we can introduce a
latent or hidden variable ${\bf z}$ for the labeling of the samples 
in the given dataset ${\bf X}$.

Specifically, the latent variable ${\bf z}=[z_1,\cdots,z_K]^T$ is a
binary vector, of which all components are binary random variables 
$z_k\in\{0,\,1\}$. Only one of these $K$ components is 1, e.g., 
$z_k=1$, indicating a sample ${\bf x}$ in the dataset belongs to 
the kth cluster $C_k$, while all others are 0, i.e., these $K$ 
binary variables add up to 1, $\sum_{k=1}^K z_k=1$. 

We further introduce the following probabilities for each of the
$K$ clusters $C_k\;\;(k=1,\cdots,K)$:
\begin{itemize}
\item The {\em prior probability} for any unobserved data sample 
  in the dataset to belong to $C_k$, represented by $z_k=1$:
  \begin{equation}
    P(z_k=1)=P_k
  \end{equation}
  As any sample ${\bf x}$ belongs to one and only one of the $K$ 
  clusters, the events ${\bf x}\in C_k,\;(k=1,\cdots,K)$ are mutually
  exclusive and complementary, i.e., the $K$ prior probabilities add
  up to 1:
  \begin{equation}
    \sum_{k=1}^K P_k =1
  \end{equation}
\item The probability distribution of all samples in $C_k$, assumed 
  to be a Gaussian:
  \begin{equation}
    p({\bf x}|z_k=1,\theta)={\cal N}({\bf x};{\bf m}_k,{\bf\Sigma}_k)
    \label{likelihoodMG}
  \end{equation}

\item The joint probability of any ${\bf x}$ and $z_k=1$:
  \begin{equation}
    p({\bf x},z_k=1|\theta)=p({\bf x}|z_k=1,\theta)\;P(z_k=1)
    =P_k {\cal N}({\bf x};{\bf m}_k,{\bf\Sigma}_k)
    \label{jointprobMG}
  \end{equation}
\end{itemize}
Marginalizing this joint probability over the latent variable 
${\bf z}$, we get the Gaussian mixture model, the distribution 
$p({\bf x})$ of any sample ${\bf x}$ regardless to which cluster 
it belongs:
\begin{equation}
  p({\bf x}|\theta)=\sum_{k=1}^K p({\bf x},z_k=1|\theta)
  =\sum_{k=1}^K p({\bf x}|z_k=1,\theta)\;P(z_k=1)
  =\sum_{k=1}^K P_k {\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k)
  \label{pofxMG}
\end{equation}
Note that Eqs. (\ref{likelihoodMG}), (\ref{jointprobMG}), and
(\ref{pofxMG}) are the same as Eqs. (\ref{likelihoodNB}),
(\ref{jointprobNB}), and (\ref{pofxNB}) in the naive Bayes 
classifier, respectively.

All such probabilities defined for $z_k=1$ can be generalized to 
${\bf z}=[z_1,\cdots,z_K]^T$ for all $K$ clusters:
\begin{eqnarray}
  p({\bf z}|{\bf\theta})&=&\prod_{k=1}^K P_k^{z_k} \\
  p({\bf x}|{\bf z},{\bf\theta})&=&
  \prod_{k=1}^K {\cal N}({\bf x},{\bf m}_k,{\bf\Sigma}_k)^{z_k} \\
  p({\bf x},{\bf z}|{\bf\theta})&=&p({\bf z})\;p({\bf x}|{\bf z},\theta)
  =\prod_{k=1}^K \left(P_k {\cal N}({\bf x},{\bf m}_k,{\bf\Sigma}_k)\right)^{z_k}
\end{eqnarray}

Given the dataset ${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$ containing
$N$ i.i.d. samples, we introduce $N$ corresponding latent variables 
in ${\bf Z}=[{\bf z}_1,\cdots,{\bf z}_N]$, of which 
${\bf z}_n=[z_{n1},\cdots,z_{nK}]^T$ is the labeling of ${\bf x}_n$,
i.e., ${\bf x}_n$ belongs to $C_k$ if $z_{nk}=1$ (while $z_{nl}=0$
for all $l\ne k$). Note that here ${\bf Z}=[{\bf z}_1,\cdots,{\bf z}_N]$
is defined in the same way as ${\bf Y}=[{\bf y}_1,\cdots,{\bf y}_N]$ 
in maxsolft regression, both as the labeling of ${\bf X}$, with the 
only difference that ${\bf Y}$ is provided in the training data available
for a supervised method, but here ${\bf Z}$ is a latent variable not part
of the data provided for unsupervised clustering. Now we have 
\begin{equation}
  p({\bf x}_n,{\bf z}_n|\theta)=\prod_{k=1}^K 
  \left(P_k {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right)^{z_{nk}},
       \;\;\;\;\;\;\;\;\;\;\;\;\;\;(n=1,\cdots,N)
\end{equation}
The likelihood function of the GMM model parameters ${\bf\theta}$ to 
be estimated can be expressed as:
\begin{eqnarray}
  L(\theta|{\bf X},{\bf Z})&=&p({\bf X},{\bf Z}|\theta)
  =p([{\bf x}_1,\cdots,{\bf x}_N],[{\bf z}_1,\cdots,{\bf z}_N]\bigg|{\bf m}_k,{\bf\Sigma}_k,P_k(k=1,\cdots,K))
  \nonumber\\
  &=&\prod_{n=1}^N p({\bf x}_n,{\bf z}_n|\theta)
  =\prod_{n=1}^N \prod_{k=1}^K \left(P_k{\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right)^{z_{nk}}
\end{eqnarray}
and the log likelihood function is:
\begin{eqnarray}
  \log L(\theta|{\bf X},{\bf Z})&=&\log p({\bf X},{\bf Z}|\theta)
  =\log \left[ \prod_{n=1}^N \prod_{k=1}^K 
    \left(P_k{\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right)^{z_{nk}}\right]
  \nonumber\\
  &=& \sum_{n=1}^N \sum_{k=1}^K z_{nk}
  \left[\log P_k+\log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right]
  \label{logL}
\end{eqnarray}
Similar to the method of maximum likelihodd estimation (MLE) which 
finds the model parameters in ${\bf\theta}$ as those that maximize 
the likelihood function $L({\bf\theta})$ or its log function
$\log\;L({\bf\theta})$, here we find the model parameters in
$\theta =\{P_k,{\bf m}_k,{\bf\Sigma}_k,\;(k=1,\cdots,K)\}$ as those 
that maximize the expectation of the log likelihood function above
with respect to the latent variables in ${\bf Z}$. This method is 
therefore called {\em expectation maximization (EM)}, containing the 
following two iterative steps:
\begin{itemize}
\item {\bf E-step:} Find the expectation of the log likelihood function.
  
  We first find the posterior probability for any ${\bf x}_n$ to belong 
  to any $C_k$ (indicated by $z_{nk}=1$ and $z_{nl}=0$ for all $l\ne k$) 
  is modeled by
  \begin{equation}
    P_{nk}=P(z_{nk}=1|{\bf x}_n,\theta)
    =\frac{p({\bf x}_n,z_{nk}=1|\theta)}{p({\bf x}_n|\theta)}
    =\frac{P_k\,{\cal N}({\bf x}_n;{\bf m}_k{\bf\Sigma}_k)}
    {\sum_{l=1}^K P_l\,{\cal N}({\bf x}_n;{\bf m}_l{\bf\Sigma}_l)}
    \;\;\;\;\;\;\;\;(n=1,\cdots,N;\;k=1,\cdots,K)
    \label{GMMprob}
  \end{equation}
  These are the posterior probabilities for $z_{nk}=1$ given an observed
  ${\bf x}_n$, and they add up to 1, ss the prior probabilities 
  $P(z_k=1)=P_k$:
  \begin{equation}
    \sum_{k=1}^K P(z_k=1)=\sum_{k=1}^K P_k=1,
    \;\;\;\;\;\;\;\;\;\;\;\;\;\;
    \sum_{k=1}^K P(z_{nk}=1|{\bf x}_n,\theta)=\sum_{k=1}^K P_{nk}=1
  \end{equation}
  We note that the definition of $P_{nk}$ is similar to the 
  \htmladdnormallink{softmax function $s_{nk}$}{../ch7/node15.html} 
  with the only difference that here weighted Gaussian functions are 
  used instead of the logistic functions used in softmax.

  We also note that the posterior probability $P_{nk}$ defined above 
  represents a {\em soft} decision, in the sense that it is possible
  for a ${\bf x}_n$ to belong to each $C_k$ with probability $0<P_{nk}<1$
  for all $k=1,\cdots,K$, instead of a {\em hard} decision, in the sense 
  that ${\bf x}_n$ belongs to only one specific $C_k$ with $P_{nk}=1$, 
  while $P_{nl}=0$ for all $l\ne k$, as in the case of K-means clustering.

  We also find the expectation of the log likelihood with respect to 
  the latent variables in ${\bf Z}$:
  \begin{eqnarray}
    E_{\bf Z}\left( \log L(\theta|{\bf X},{\bf Z}) \right)
    &=&E_{\bf Z}\left[\sum_{n=1}^N \sum_{k=1}^K z_{nk}
    \left[\log P_k+\log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right]\right]
    \nonumber\\
    &=&\sum_{n=1}^N \sum_{k=1}^K E(z_{nk})
    \left[\log P_k+\log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right]
    \nonumber\\
    &=&\sum_{n=1}^N \sum_{k=1}^K P_{nk}
    \left[\log P_k+\log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right]
    \label{expectationoflog}
  \end{eqnarray}
  The last equality is due to the following fact:
  \begin{equation}
    E(z_{nk})=1\;P(z_{nk}=1|{\bf x}_n)+0\;P(z_{nk}=0|{\bf x}_n)
    =P(z_{nk}=1|{\bf x}_n)=P_{nk}
  \end{equation}

\item {\bf M-step:} Find the optimal model parameters that maximize
  the expectation of the log likelihood function.

  We first set to zero the derivatives of the expectation of 
  the log likelihood with respect to each of the parameters in 
  ${\bf\theta}=\{P_k,\;{\bf m}_k\;(k=1,\cdots,K)\}$, and then solve
  the resulting equations to get the optimal parameters.

  \begin{itemize}
  \item Find $P_k$:
    
    Due to the constraint $\sum_{k=1}^K P_k=1$, we first construct the
    Lagrangian function compsed of the log likelihood as the objective 
    function and an extra term for the constraint:
    \begin{equation}
      L(\theta,\,\lambda)
      =\sum_{n=1}^N \sum_{k=1}^K P_{nk}
      \left[\log P_k+\log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right]
      +\lambda\left(\sum_{k=1}^K P_k-1\right)
    \end{equation}
    and set it derivative with respect to $P_k$ to zero:
    \begin{eqnarray}
      \frac{\partial}{\partial P_k} L(\theta,\,\lambda)
      &=&\frac{\partial}{\partial P_k} \left[
        \sum_{n=1}^N \sum_{k=1}^K P_{nk}
        \left[\log P_k+\log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right]
        +\lambda\left(\sum_{k=1}^K P_k-1\right) \right]
      \nonumber\\
      &=&\sum_{n=1}^N P_{nk}\frac{1}{P_k}+\lambda=0
    \end{eqnarray}
    Multiplying both sides by $P_k$, we get
    \begin{equation}
      \sum_{n=1}^N P_{nk}+P_k \lambda=N_k+P_k\lambda=0
      \label{equationPik}
    \end{equation}
    where we have defined
    \begin{equation}
      N_k=\sum_{n=1}^N P_{nk}
      \label{priorP_mg}
    \end{equation}    
    that satisfies
    \begin{equation}
      \sum_{k=1}^K N_k=\sum_{k=1}^K\left(\sum_{n=1}^N P_{nk}\right)
      =\sum_{n=1}^N\left(\sum_{k=1}^K P_{nk}\right)=\sum_{n=1}^N 1=N
    \end{equation}
    Summing Eq. (\ref{equationPik}) over $k=1,\cdots,K$, we get
    \begin{equation}
      \sum_{k=1}^K\left(N_k+P_k \lambda\right)
      =\sum_{k=1}^K N_k+\lambda\left(\sum_{k=1}^K P_k\right)
      =N+\lambda=0
    \end{equation}
    Substituting $\lambda=-N$ back into Eq. (\ref{equationPik}), 
    we get the expression for the prior
    \begin{equation}
      p(z_k=1)=P_k=\frac{N_k}{N}=\frac{1}{N}\sum_{n=1}^N P_{nk}
    \end{equation}
    This is actually the same as the prior $P_k$ in Eq. (\ref{priorNB})
    used in the naive Bayes classification, but here $N_k$ defined in 
    Eq. (\ref{priorP_mg}) is the sum of the probabilities for all $N$ 
    data samples to belong to $C_k$, instead of the number of data
    samples in $C_k$ (unknown in this unsupervised case).

  \item Find ${\bf m}_k$:

    \begin{eqnarray}
      \frac{\partial}{\partial{\bf m}_k} E_{\bf z}(\log L(\theta|{\bf X},{\bf Z}))
      &=&\frac{\partial}{\partial{\bf m}_k} 
      \left[\sum_{n=1}^N \sum_{k=1}^K P_{nk}
        \left[\log P_k+\log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right]\right]
      \nonumber\\
      &=&\sum_{n=1}^N P_{nk}\frac{\partial}{\partial{\bf m}_k} 
      \log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)
      \nonumber\\
      &=&\sum_{n=1}^N P_{nk}\frac{\partial}{\partial{\bf m}_k} 
      \left[-\frac{1}{2}({\bf x}_n-{\bf m}_k)^T{\bf\Sigma}_k^{-1}({\bf x}_n-{\bf m}_k)\right]
      \nonumber\\
      &=&\frac{1}{2}\sum_{n=1}^N P_{nk}{\bf\Sigma}_k^{-1}({\bf x}_n-{\bf m}_k)={\bf 0}
      \label{equationmk}
    \end{eqnarray}
    Here we have neglected the constant coefficient 
    $(2\pi)^{-d/2}\,|{\bf\Sigma}_k|^{-1/2}$ of the Gaussian,
    independent of ${\bf m}_k$. Multiplying $2{\bf\Sigma}_k$ on
    both sides, we get
    \begin{equation}
      \sum_{n=1}^N P_{nk}({\bf x}_n-{\bf m}_k)
      =\sum_{n=1}^N P_{nk}{\bf x}_n-\sum_{n=1}^N P_{nk}{\bf m}_k
      =\sum_{n=1}^N P_{nk}{\bf x}_n-N_k{\bf m}_k={\bf 0}
    \end{equation}
    Solving for ${\bf m}_k$ we get
    \begin{equation}
      {\bf m}_k=\frac{1}{N_k}\sum_{n=1}^N P_{nk}{\bf x}_n
    \end{equation}

  \item Find ${\bf\Sigma}_k$:

    \begin{eqnarray}
      \frac{\partial}{\partial{\bf m}_k} E_{\bf z}(\log L(\theta|{\bf X},{\bf Z}))
      &=&\frac{\partial}{\partial{\bf\Sigma}_k} 
      \left[\sum_{n=1}^N \sum_{k=1}^K P_{nk}
        \left[\log P_k+\log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)\right]\right]
      \nonumber\\
      &=&\sum_{n=1}^N P_{nk}\frac{\partial}{\partial{\bf\Sigma}_k} 
      \log {\cal N}({\bf x}_n,{\bf m}_k,{\bf\Sigma}_k)
      \nonumber\\
      &=&-\frac{1}{2}\sum_{n=1}^N P_{nk}
      \left[\frac{\partial}{\partial{\bf\Sigma}_k}\log|{\bf\Sigma}_k|
        +\frac{\partial}{\partial{\bf\Sigma}_k}({\bf x}_n-{\bf m}_k)^T{\bf\Sigma}_k^{-1}({\bf x}_n-{\bf m}_k)\right]
      \nonumber\\
      &=&-\frac{1}{2}\sum_{n=1}^N P_{nk}
      \left[{\bf\Sigma}_k^{-1}-{\bf\Sigma}_k^{-1}
        ({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T{\bf\Sigma}_k^{-1}\right]={\bf 0}
      \label{equationsigmak}
    \end{eqnarray}
    Here we have used the following facts
    (for more details see \htmladdnormallink{Matrix Cookbook}
    {https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}):
    \begin{equation}
    \frac{d}{d{\bf A}}\log|{\bf A}|=({\bf A}^{-1})^T,
    \;\;\;\;\;\;
    \frac{d}{d{\bf A}} \left({\bf a}^T{\bf A}^{-1}{\bf b}\right)
    =-({\bf A}^{-1})^T{\bf a}{\bf b}^T({\bf A}^{-1})^T
    \end{equation}
    Pre and post multiplying ${\bf\Sigma}_k$ on both sides of the equation above
    we get
    \begin{equation}
      \sum_{n=1}^N P_{nk}\left({\bf\Sigma}_k-({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T\right)={\bf 0}
    \end{equation}
    Solving for ${\bf\Sigma}_k$, we get
    \begin{equation}
      {\bf\Sigma}_k=\frac{1}{N_k}\sum_{n=1}^N P_{nk}({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T
    \end{equation}

  \end{itemize}
\end{itemize}
Note that the expectation of log likelihood in Eq. (\ref{expectationoflog})
in the E-step is a function of the model parameters 
${\bf\theta}=\{P_k,{\bf m}_k,{\bf\Sigma}_k,(k=1,\cdots,K)\}$, which 
are to be updated in Eqs. (\ref{equationPik}), (\ref{equationmk}), and
(\ref{equationsigmak}) in the M-step, i.e., these two steps need to be
carried out in an alternative and iterative fashion from some initial
values of the parameters until convergence.

In summary, here is the EM clustering algorithm based on Gaussian mixture model:
\begin{enumerate}
\item Initialize means ${\bf m}_k$, covariance ${\bf\Sigma}_k$ and coefficient
  $P_k$. 

\item The E-step: 

  Find the responsibility $P_{nk}$ for all $N$ data points and all 
  $K$ clusters and then $N_k$:
  \begin{equation}
    P_{nk}=P(r_k=1|{\bf x}_n)=\frac{P_k\,{\cal N}({\bf x}_n;{\bf m}_k,{\bf\Sigma}_k)}
    {\sum_{l=1}^K P_l\,{\cal N}({\bf x}_n;{\bf m}_l,{\bf\Sigma}_l)},
    \;\;\;\;\;\;\;N_k=\sum_{n=1}^N P_{nk}
  \end{equation}

\item The M-step: 

  Recalculate the parameters that maximize the likelihood function:
  \begin{eqnarray}
    P_k&=&\frac{N_k}{N}
    \nonumber\\
    {\bf m}_k&=&\frac{1}{N_k} \sum_{n=1}^N P_{nk}{\bf x}_n 
    \nonumber\\
    {\bf\Sigma}_k&=&\frac{1}{N_k}\sum_{n=1}^N P_{nk}({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T
  \end{eqnarray}

\item If the parameters or the log likelihood function have not 
  converged, go back to step 2.  Otherwise terminate the iteration. 
  The probability for each sample ${\bf x}_n$ to belong to cluster 
  $C_l$ is $p_{nl}=P(z_{nl}=1|{\bf x}_n,\theta)$, and it is therefore
  assigned to $C_k$ if $P_{nk}=\max_l p_{nl}$.

\end{enumerate}

We can show that the K-means algorithm is actually a special case 
of the EM algorithm, when all covariance matrices are the same
${\bf\Sigma}_k=\varepsilon{\bf I}$, where $\varepsilon$ is a 
scaling factor which appraoches to zero. In this case we have:
\begin{equation}
  p({\bf x}|z_k=1,\theta)={\cal N}({\bf x}|{\bf m}_k,\varepsilon{\bf I})
  =\frac{1}{(2\pi)^{d/2}\varepsilon^{1/2}}
  \exp\left(-\frac{1}{2\varepsilon}||{\bf x}-{\bf m}_k||^2\right)
\end{equation}
and the probability for any ${\bf x}_n\in{\bf X}$ to belong to cluster 
$C_k$ is:
\begin{equation}
  P_{nk}=P(z_k=1|{\bf x}_n,\theta)
  =\frac{P_k{\cal N}({\bf x}_n;{\bf m}_l{\bf\Sigma}_l)}
  {\sum_{l=1}^K P_l {\cal N}({\bf x}_n;{\bf m}_l{\bf\Sigma}_l)}
  =\frac{P_k\exp(-||{\bf x}_n-{\bf m}_k||^2/2\varepsilon)}
  {\sum_{l=1}^K P_l\exp(-||{\bf x}_n-{\bf m}_l||^2/2\varepsilon)}
\end{equation}
When $\varepsilon\rightarrow 0$, all terms in the denominator approach
to zero, but the one with minimum $||{\bf x}-{\bf m}_k||$ approaches 
to zero most slowly, and becomes the dominant term of the denominator.
If the numerator happens to be this term as well, then $P_{nk}=1$,
otherwise the numerator approaches zero and $P_{nk}=0$. Now $P_{nk}$ 
defined above becomes:
\begin{equation}
  \lim\limits_{\varepsilon\rightarrow 0} P_{nk}=\lim\limits_{\varepsilon\rightarrow 0} 
  \frac{P_k\exp(-||{\bf x}_n-{\bf m}_k||^2/2\varepsilon)}
  {\sum_{l=1}^K P_l\exp(-||{\bf x}_n-{\bf m}_l||^2/2\varepsilon)}
  =\left\{\begin{array}{ll}
  1 & \mbox{if $||{\bf x}_n-{\bf m_k}||=\min_l ||{\bf x}_n-{\bf m_l}|$}|\\
  0 & \mbox{otherwise}\end{array}\right.
\end{equation}
Now the posterior probability $0<P_{nk}<1$ defined in Eq. 
(\ref{GMMprob}) for a soft decision becomes a binary value 
$P_{nk}\in\{0,\;1\}$ for a hard binary decision to assign 
${\bf x}_n$ to $C_k$ with the smallest distance. Also 
$N_k=\sum_{n=1}^N P_{nk}$ defined in Eq. (\ref{priorP_mg})
as the sum of the posterior probabilities for all $N$ data 
points to belong to $C_k$ becomes $N_k$ as the number of 
data samples assigned only to $C_k$. In other words, now 
the probabilistic EM method based on both ${\bf m}$ and 
${\bf\Sigma}$ becomes the deterministic K-means method based 
on ${\bf m}$ only.

We can also make a comparison between the GMM method for 
unsupervised clustering and the softmax regression for 
supervised classification. First, the latent variables 
${\bf Z}=[{\bf z}_1,\cdots,{\bf z}_N]$ in GMM play a similar 
role as the labeling ${\bf Y}=[{\bf y}_1,\cdots,{\bf y}_N]$ in 
\htmladdnormallink{softmax regression}{../ch7/node14.html} for
multi-class classificatioin. However, the difference is that 
${\bf Y}$ is explicitely given in the training set for a supervised 
classification, while ${\bf Z}$ is hidden for an unsupervised 
cllustering analysis. Second, we note that the probability 
$P_{nk}=p(z_{nk}=1|{\bf x}_n,\theta)$ given in Eq. (\ref{GMMprob}) 
is similar to the softmax function $\phi_{nk}=P(y'=k|{\bf x}_n)$ 
in the softmax method in terms of their form, with the only 
difference that the Gaussian function is used for GMM while the
exponential function is used for solfmax.

{\bf Examples} 

The same dataset is used to test both the K-means and EM clustering 
methods. The first panel shows 10 iterations of the K-means method,
while the second panel shows 16 iterations of the EM method. In both
cases, the iteration converges to the last plot. Comparing the two 
clustering results, we see that the K-means method cannot separate 
the red and green data points from two different clusters, both normally 
distributed with similar means but very different covariance matrices, 
while the blue data points all in the same cluster are separated into 
two clusters. But the EM method based on the Gaussian mixture model 
can correctly identified all three clusters.

\htmladdimg{../figures/ClusteringKmeans.png}
\htmladdimg{../figures/clusteringEM.png}

The two clustering methods are also applied to the Iris dataset,
which has three classes each of 50 4-dimensional sample vectors.
The PCA method is used to visualize the first two principal 
compnents, as shown below. Also, as can be seen from their c
onfussion matrices, the error rate of the K-means method is 18/150,
while that of the EM method is 5/150.

\begin{equation}
\begin{tabular}{r|r|r}
\multicolumn{3}{c}{K-means}\\\hline\hline 
0 & 0 & 50\\\hline 50 & 0 & 0\\\hline18 & 32 & 0\\\hline\hline
  \end{tabular}
\;\;\;\;\;\;\;\;\;\;
\begin{tabular}{r|r|r}
\multicolumn{3}{c}{EM}\\\hline\hline
0 & 0 & 50\\\hline45 & 5 & 0\\\hline0 & 50 & 0\\\hline\hline
\end{tabular}
\end{equation}


\htmladdimg{../figures/IrisKmeans.png}
\htmladdimg{../figures/IrisEM.png}


\subsection{Mixture of Bernoulli}

If the data are binary, i.e., each data point $x$ is treated 
as a discrete random variable that takes either of two binary 
values $1$ and $0$ with probabilities $\mu$ and $1-\mu$, then 
the {\em probability mass function (pmf)} is the Bernoulli 
distribution:
\begin{equation}
  {\cal B}(x|\mu)=\mu^x(1-\mu)^{1-x}=\left\{\begin{array}{cl}
  \mu & \mbox{if $x=1$} \\  1-\mu & \mbox{if $x=0$} \end{array}\right.
\end{equation}
The mean and variance of $x$ are
\begin{eqnarray}
  E(x)&=&1\;P(x=1)+0\;P(x=0)=1\;\mu+0\;(1-\mu)=\mu
  \\
  Var(x)&=&E[(x-E(x))^2]=E(x^2)-E(x)^2
  \nonumber\\
  &=&1^2\;P(x=1)+0^2\;P(x=0)-\mu^2=\mu-\mu^2=\mu(1-\mu)
\end{eqnarray}
A set of $d$ independent binary variables can be represented 
as a random vector ${\bf x}=[x_1,\cdots,x_d]^T$ with mean vector
and covariance matrix as shown below:
\begin{eqnarray}
  E({\bf x})&=&{\bf m}=[\mu_1,\cdots,\mu_N]^T
  \\
  Cov({\bf x})&=&{\bf\Sigma}=diag( \mu_i(1-\mu_i) )
  =\left[ \begin{array}{ccc}
      \mu_1(1-\mu_1) & & 0 \\ & \ddots & \\
      0 & & \mu_d(1-\mu_d)\end{array}\right]
\end{eqnarray}
Note that the covariance matrix ${\bf\Sigma}$ is solely determined 
by the means $\{\mu_1,\cdots,\mu_N\}$.

Now we can get the pmf of the a binary random vector ${\bf x}$:
\begin{equation}
  {\cal B}({\bf x}|{\bf m})=\prod_{i=1}^d {\cal B}(x_i|\mu_i)
  =\prod_{i=1}^d \mu_i^{x_i}(1-\mu_i)^{1-x_i}
\end{equation}
and the log pmf:
\begin{equation}
  \log {\cal B}({\bf x}|{\bf m})
  =\log\left(\prod_{i=1}^d {\cal B}(x_i|\mu_i)\right)
  =\sum_{i=1}^d \left[ x_i\log\mu_i+(1-x_i)\log(1-\mu_i) \right]
\end{equation}
Similar to the Gaussian mixture model, the Bernoulli mixture model 
of $K$ multivariate Bernoulli distributions is defined as:
\begin{equation}
  p({\bf x}|{\bf m}_k,P_k,(k=1,\cdots,K))=p({\bf x}|{\bf\theta})
  =\sum_{k=1}^K P_k {\cal B}({\bf x},{\bf m}_k)
  =\sum_{k=1}^K P_k \prod_{i=1}^d \mu_{ki}^{x_i}(1-\mu_{ki})^{1-x_i}
\end{equation}
where ${\bf\theta}=\{{\bf m}_k,P_k,(k=1,\cdots,K)\}$ denotes 
all parameters of the mixture model to be estimated based on 
the given dataset, and ${\bf m}_k=E_k({\bf x})$ respect to 
${\cal B}({\bf x}|{\bf m}_k)$. The mean of this mixture model is
\begin{equation}
  {\bf m}=E({\bf x})=\sum_{k=1}^KP_k E_k({\bf x})=\sum_{k=1}^K P_k{\bf m}_k
\end{equation}
\begin{comment}
The covariance of the mixture model is
\begin{eqnarray}
  {\bf\Sigma}&=&Cov({\bf x})
  =E({\bf x}{\bf x}^T)-E({\bf x})\,E({\bf x})^T
  =\sum_{k=1}^KP_k E_k({\bf x}{\bf x}^T)-E({\bf x})\,E({\bf x})^T
  \nonumber\\
  &=&\sum_{k=1}^K \left[{\bf\Sigma}_k+{\bf m}_k{\bf m}_k^T\right]-{\bf m}{\bf m}^T
\end{eqnarray}
\end{comment}

Also similar to the Gaussian mixture model, we introduce a set of $K$
latent binary random variables ${\bf z}=[z_1,\cdots,z_K]^T$ with binary 
conponents$z_k\in\{0,\;1\}$ and $\sum_{k=1}^K z_k=1$, and get the prior 
probability of ${\bf z}$, the conditional probability of ${\bf x}$ given 
${\bf z}$, and the joint probability of ${\bf x}$ and ${\bf z}$ as the
following
\begin{eqnarray}
  p({\bf z}|{\bf\theta})&=&\prod_{k=1}^KP_k^{z_k}  \\
  p({\bf x}|{\bf z},{\bf\theta})&=&\prod_{k=1}^K {\cal B}({\bf x},{\bf m}_k)^{z_k}  \\
  p({\bf x},{\bf z}|{\bf\theta})
  &=&p({\bf z}|{\bf\theta})\;p({\bf x}|{\bf z},{\bf\theta})
  =\prod_{k=1}^K \left(P_k\;{\cal B}({\bf x},{\bf m}_k)\right)^{z_k}
\end{eqnarray}
Given the dataset ${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$ containing 
$N$ i.i.d. samples, we introduce the corresponding latent variables 
in ${\bf Z}=[{\bf z}_1,\cdots,{\bf z}_N]$, of which each 
${\bf z}_n=[z_{n1},\cdots,z_{nK}]^T$ is for the labeling of ${\bf x}_n$.
Then we can find the likelihood function of the Bernoulli mixture model
parameters ${\bf\theta}=\{P_k,{\bf m}_k,\;(k=1,\cdots,K)\}$:
\begin{eqnarray}
  L({\bf\theta}|{\bf X},{\bf Z})=p({\bf X},{\bf Z}|\theta)
  &=&p([{\bf x}_1,\cdots,{\bf x}_N],[{\bf z}_1,\cdots,{\bf z}_N]
  |{\bf m}_k,P_k(k=1,\cdots,K))
  \nonumber\\
  &=&\prod_{n=1}^N p({\bf x}_n,{\bf z}_n|{\bf\theta})
  =\prod_{n=1}^N \prod_{k=1}^K \left(P_k{\cal B}({\bf x}_n,{\bf m}_k)\right)^{z_{nk}}
\end{eqnarray}
and the log likelihood function:
\begin{eqnarray}
  \log\;L({\bf\theta}|{\bf X},{\bf Z})&=&\log p({\bf X},{\bf Z}|\theta)
  =\log\prod_{n=1}^N \prod_{k=1}^K \left(P_k{\cal B}({\bf x}_n,{\bf m}_k)\right)^{z_{nk}}
  \nonumber\\
  &=&\sum_{n=1}^N \sum_{k=1}^K {z_{nk}} \left[ \log P_k
    +\log {\cal B}({\bf x}_n,{\bf m}_k)\right]
\end{eqnarray}
Based on the same EM method used in Gaussian mixture model, we can 
find the opptimal parameters that maximize the expectation of the
log likelihood function in the following two steps:
\begin{itemize}
\item {\bf E-step:} Find the expectation of the likelihood function.
  
  We first find the posterior probability for any sample 
  ${\bf x}_n$ to belong to cluster $C_k$, denoted by $P_{nk}$:
  \begin{equation}
    P_{nk}=P(z_{nk}=1|{\bf x}_n,{\bf\theta})
    =\frac{p({\bf x}_n,z_{nk}=1|{\bf\theta})}{p({\bf x}_n)|{\bf\theta}}
    =\frac{P_k\,{\cal B}({\bf x}_n;{\bf m}_k)}
    {\sum_{l=1}^K P_l\,{\cal B}({\bf x}_n;{\bf m}_l)}
    \;\;\;\;\;\;\;\;\;\;\;(n=1,\cdots,N;\;\;k=1,\cdots,K)
  \end{equation}
  which is the expectation of $z_{nk}$:
  \begin{equation}
    E(z_{nk})=1\;P(z_{nk}=1|{\bf x}_n)+0\;P(z_{nk}=0|{\bf x}_n)
    =P(z_{nk}=1|{\bf x}_n)=P_{nk}
  \end{equation}
  Now we can find the expectation of the log likelihood with respect to 
  the latent variables in ${\bf Z}$:
  \begin{eqnarray}
    E_{\bf Z} \left(\log L({\bf\theta}|{\bf X},{\bf Z})\right)
    &=&E_{\bf Z}\;\sum_{n=1}^N \sum_{k=1}^K z_{nk}\left[\log P_k
      +\log {\cal B}({\bf x}_n,{\bf m}_k)\right]
    \nonumber\\
    &=&\sum_{n=1}^N \sum_{k=1}^K E(z_{nk}) \left[\log P_k
      +\log \prod_{i=1}^d \mu_{ki}^{x_{ni}} (1-\mu_{ki})^{1-x_{ni}} \right]
    \nonumber\\
    &=&\sum_{n=1}^N \sum_{k=1}^K P_{nk}   \left[\log P_k
      +\sum_{i=1}^d \left[ x_{ni}\log\mu_{ki}+(1-x_{ni})\log(1-\mu_{ki}) \right]\right]
  \end{eqnarray}

\item {\bf M-step:} Find the optimal model parameters that maximize 
  the expectation of the log likelihood function.

  We first set to zero the derivatives of the expectation of 
  the log likelihood with respect to each of the parameters in 
  ${\bf\theta}=\{P_k,\;{\bf m}_k\;(k=1,\cdots,K)\}$, and then solve
  the resulting equations to get the optimal parameters.

  \begin{itemize}
  \item Find $P_k$: same as in the case of the GMM model:
    \begin{equation}
      P_k=\frac{N_k}{N}=\frac{1}{N}\sum_{n=1}^N P_{nk}
    \end{equation}

  \item Find ${\bf m}_k$:
    \begin{eqnarray}
      && \frac{\partial}{\partial{\bf m}_k}     
      E_{\bf Z}\left( \log p({\bf X},{\bf Z}|\theta) \right)
      \nonumber\\
      &=&\frac{\partial}{\partial{\bf m}_k}     
      \sum_{n=1}^N \sum_{k=1}^K P_{nk}   \left[\log P_k
        +\sum_{i=1}^d \left[ x_{ni}\log\mu_{ki}+(1-x_{ni})\log(1-\mu_{ki}) \right]\right]
      \nonumber\\
      &=&\sum_{n=1}^N P_{nk} \frac{\partial}{\partial{\bf m}_k}
      \sum_{i=1}^d\left[ x_{ni}\log\mu_{ki}+(1-x_{ni})\log(1-\mu_{ki}) \right]={\bf 0}
    \end{eqnarray}
    The ith component of the equation is
    \begin{eqnarray}
      &&\sum_{n=1}^N P_{nk}\frac{d}{d\mu_{ki}}
      \left[ x_{ni}\log \mu_{ki}+(1-x_{ni})\log(1-\mu_{ki})\right]
      \nonumber\\
      &=&\sum_{n=1}^N P_{nk}\left(\frac{x_{ni}}{\mu_{ki}}-\frac{1-x_{ni}}{1-\mu_{ki}}\right)=0
    \end{eqnarray}
    i.e.,
    \begin{equation}
      (1-\mu_{ki})\sum_{n=1}^N P_{nk} x_{ni}=\mu_{ki}\sum_{n=1}^N P_{nk}(1-x_{ni})
      =\mu_{ki}N_k-\mu_{ki} \sum_{n=1}^N P_{nk}x_{ni}
    \end{equation}
    Solving for $\mu_{ki}$ we get
    \begin{equation}
      \mu_{ki}=\frac{1}{N_k}\sum_{n=1}^N P_{nk}x_{ni}\;\;\;\;\;\;(i=1,\cdots,d)
    \end{equation}
    or, in vector form,
    \begin{equation}
      {\bf m}_k=\frac{1}{N_k}\sum_{n=1}^N P_{nk}{\bf x}_n
    \end{equation}


  \end{itemize}

{\bf Example:}

Clustering results of hand-written digits with $K=10$ and $K=12$. The mean
vectors ${\bf m}_k$ of each of the $K$ clusters are visualized as shown:

\htmladdimg{../figures/BernoulliDigits10.png}

\htmladdimg{../figures/BernoulliDigits12.png}


\end{itemize}





\begin{comment}
\htmladdimg{../figures/TSM2a.png}

The first 16 iterations

\htmladdimg{../figures/TSM2b.png}

The last 16 iterations

{\bf Example 2:} Traveling salesman problem, 1-D SOM trained by a set of random dots:

\htmladdimg{../figures/TSM1c.png}

The first 16 iterations

\htmladdimg{../figures/TSM1d.png}

The last 16 iterations
\end{comment}




\section{Linear Models for Binary Classification}

We first consider binary classification based on the same
linear model $y=f({\bf x})+r={\bf x}^T{\bf w}+r$ used in 
linear regression considered before. Any test sample ${\bf x}$
is classified into one of the two classes $\{C_0,\,C_1\}$ 
depending on whether $f({\bf x})$ is greater or smaller than 
zero:
\begin{equation}
  \mbox{if}\;\;f({\bf x})={\bf w}^T{\bf x}\left\{\begin{array}{l}<0\\>0
  \end{array}\right.,\;\;\;\;\;\;\;\mbox{then}\;\;\;\;
       {\bf x}\in \left\{\begin{array}{c}  C_0 \\ C_1\end{array}\right.
\end{equation}
Here ${\bf w}$ is a model parameter to be determined based on
the training set 
${\cal D}=\{({\bf x}_n,\,y_n)|n=1,\cdots,N\}=\{{\bf X},{\bf y}\}$,
where ${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$, ${\bf y}=[y_1,\cdots,y_N]^T$,
and $y_i=1$ if ${\bf x}_i\in C_1$ and $y_i=-1$ if ${\bf x}_i\in C_0$,
so that ${\bf w}$ fits all data points optimally in certain sense.

While in the previously considered least square classification 
method, we find the optimal ${\bf w}$ that minimizes the squared 
error $\varepsilon({\bf w})=||{\bf r}||^2$, here we find the optimal
${\bf w}$ based on a probabilistic model. Specifically, we now 
convert the linear function $f({\bf x})={\bf x}^T{\bf w}$ into 
the probability for ${\bf x}$ to belong to either class:
\begin{eqnarray}
  p({\bf x}\in C_1|{\bf w})&=&p(y=1|{\bf x},{\bf w})
  \nonumber\\
  p({\bf x}\in C_0|{\bf w})&=&p(y=-1|{\bf x},{\bf w})
  =1-p(y=1|{\bf x},{\bf w})
\end{eqnarray}
This is done by a sigmoid function defined as either a logistic or 
cumulative Gaussian (error function, erf):
\begin{eqnarray}
  \mbox{logistic:} \;\;\;\; & \sigma(z)=&\frac{1}{1+e^{-z}}=\frac{e^z}{1+e^z}
  \nonumber\\
  \mbox{erf:} \;\;\;\; & \phi(z)=&\int_{-\infty}^z {\cal N}(u|0,1)\,du
  =\frac{1}{\sqrt{2\pi}} \int_{-\infty}^z \exp\left(-\frac{1}{2} u^2\right)\,du
  \label{sigmoid}
\end{eqnarray}
In either case, we have
\begin{equation}
  \sigma(z)=\left\{\begin{array}{ll}0 & z=-\infty\\
  1 & z=\infty\end{array}\right.
  \;\;\;\;\;\;\;\mbox{and}\;\;\;\;\;\;\sigma(-z)=1-\sigma(z)
\end{equation}
Using this sigmoid function, $f={\bf x}^T{\bf w}$ in the range of 
$(-\infty,\;\infty)$ is mapped to $\sigma(f({\bf x}))$ in the range 
of $(0,\;1)$, which can be used as the probability for ${\bf x}$ to
belong to either class:
\begin{eqnarray}
  p(y=1|{\bf x},{\bf w})&=&\sigma({\bf x}^T{\bf w})=\sigma(f)
  \nonumber\\
  p(y=-1|{\bf x},{\bf w})&=&1-p(y=1|{\bf x},{\bf w})
  =1-\sigma(f)=\sigma(-f)
\end{eqnarray}
The two cases can be combined:
\begin{equation}
  p(y|{\bf x},{\bf w})=\sigma(y\;{\bf x}^T{\bf w})=\sigma(yf),
  \;\;\;\;\mbox{or}\;\;\;\;   
  p(y|{\bf x},{\bf w})=\phi(y{\bf x}^T{\bf w})=\phi(yf)
  \label{CombinedSigmoid}
\end{equation}

\htmladdimg{../figures/SigmoidSigma.png}

The binary classification problem can now be treated as a regression 
problem to find the model parameter ${\bf w}$ that best fits the data 
in the training set ${\cal D}=\{{\bf X},{\bf y}\}$. Such a regression 
problem is called {\em logistic regression} if $\sigma(z)$ is used, or 
{\em probit regression} if $\phi(z)$ is used.

Same as in the case of Bayesian regression, we assume the prior 
distribution of ${\bf w}$ to be a zero-mean Gaussian 
$p({\bf w})={\cal N}({\bf 0},{\bf\Sigma}_w)$, and for simplicity
we further assume ${\bf\Sigma}_w={\bf I}$, and find the likelihood of
${\bf w}$ based on the linear model applied to the observed data set
${\cal D}=\{({\bf x}_n,\,y_n)|n=1,\cdots,N\}$:
\begin{equation}
  {\cal L}({\bf w}|{\cal D})=p({\cal D}|{\bf w})
  =p({\bf y}|{\bf X},{\bf w})=\prod_{n=1}^N p(y_n|{\bf x}_n,{\bf w})
  =\prod_{n=1}^N \sigma(y_if_i)=\prod_{n=1}^N \sigma(y_n{\bf x}_n^T{\bf w})
\end{equation}
Note that this likelihood is not Gaussian (as in the case of 
Eq. (\ref{BayesLH})), because ${\bf y}=\pm 1$ is the binary class 
labeling of the training samples in ${\bf X}$, instead of a continuous 
function as in the case of regression.

The posterior of ${\bf w}$ can now be expressed in terms of the 
prior $p({\bf w})$ and the likelihood $p({\bf y}|{\bf X},{\bf w})$:
\begin{eqnarray}
  p({\bf w}|{\cal D})&=&p({\bf w}|{\bf X},{\bf y})
  =\frac{p({\bf y},{\bf w}|{\bf X})}{p({\bf y})}
  =\frac{p({\bf y}|{\bf X},{\bf w})\; p({\bf w})}{p({\bf y}|{\bf X})}
  \propto p({\bf y}|{\bf X},{\bf w})\;p({\bf w}|{\bf X})
  \nonumber\\
  &=&\prod_{n=1}^Np(y_n|{\bf x}_n,{\bf w})\;{\cal N}({\bf 0},{\bf\Sigma}_w) 
  =\prod_{n=1}^N\sigma(y_nf_n)\;
  \frac{1}{(2\pi)^{d/2}|{\bf\Sigma}_w|^{1/2}}
  \exp\left(-\frac{1}{2}{\bf w}^T{\bf\Sigma}_w^{-1}{\bf w}\right)
  \label{PosteriorGaussianLinear}
\end{eqnarray}
where the denominator $p({\bf y}|{\bf X})=p({\cal D})$ is dropped as 
it is a constant independent of the variable ${\bf w}$ of interest. 
We can further find the log posterior denoted by $\psi({\bf w})$:
\begin{equation}
  \psi({\bf w})=\log \;p({\bf w}|{\cal D})
  =\sum_{n=1}^N \log \sigma(y_n\,f_n)
  -\frac{N}{2}\log(2\pi)-\frac{1}{2}\log|{\bf\Sigma}_w|
  -\frac{1}{2}{\bf w}^T{\bf\Sigma}_w^{-1}{\bf w}
  \label{PsiLogPL}
\end{equation}

%The log prior in the first term can be considered as a quadratic 
%penalty term for the maximization of the log likelihood in the 
%second term. 

The optimal ${\bf w}$ that best fits the training set
${\cal D}=\{{\bf X},{\bf y}\}$ can now be found as the one that 
maximizes this posterior $p({\bf w}|{\cal D})$, or, equivalently, 
the log posterior $\psi({\bf w})$, by setting the derivative of 
$\psi({\bf w})$ to zero and solving the resulting equation below
by Newton's method or conjugate gradient ascent method:
\begin{eqnarray}
  {\bf g}_{\psi}({\bf w})
  &=&\frac{d}{d{\bf w}} \left(\sum_{n=1}^N\log \sigma(y_nf_n) \right)
  -\frac{d}{d{\bf w}}\left(\frac{1}{2}{\bf w}^T{\bf\Sigma}_w^{-1}{\bf w}\right)
  \nonumber\\
  &=&\sum_{n=1}^N\frac{d}{d{\bf w}} 
  \left(\log \frac{1}{1+e^{-y_n{\bf x}_n^T{\bf w}}}\right)
  -{\bf\Sigma}_w^{-1}{\bf w}
  \nonumber\\
  &=&\sum_{n=1}^N\frac{y_n{\bf x}_n }{1+e^{y_n{\bf x}_n^T{\bf w}}}
  -{\bf\Sigma}_w^{-1}{\bf w} ={\bf 0}
\end{eqnarray}

Having found the optimal ${\bf w}$, we can classify any test pattern 
${\bf x}_*$ in terms of the posterior of its corresponding labeling $y_*$:
\begin{equation}
  p(y_*=1|{\bf x}_*,{\bf w})=\sigma({\bf x}_*^T{\bf w})
  =\sigma(f({\bf x}_*))=\frac{1}{1+\exp({\bf x}_*^T{\bf w})}
\end{equation}

In a multi-class case with $(C_1,\cdots,C_K)$, we can still use a vector
${\bf w}_i$ $(i=1,\cdots,K)$ to represent each class $C_i$, the direction 
of the class with respect to the origin in the feature space, and the inner 
product ${\bf x}^T{\bf w}_i$ proportional to the projection of ${\bf x}$
onto vector ${\bf w}_i$ measures the extent to which ${\bf x}$ belongs to
$C_i$. Similar to the logistic function used in the two-class case, here 
the {\em soflmax function} defined below is used to convert 
$-\infty<{\bf x}^T{\bf w}_i<\infty\,(i=1,\cdots,K)$ into the probability 
that ${\bf x}$ belongs to $C_i$:
\begin{equation}
  p(y\in C_i|{\bf x},{\bf W})
  =\frac{\exp({\bf x}^T{\bf w}_i)}{\sum_{k=1}^K\exp({\bf x}^T{\bf w}_k)}
  =\left\{\begin{array}{cc}
  0 & {\bf x}^T{\bf w}_i=-\infty\\1 & {\bf x}^T{\bf w}_i=\infty\end{array}\right.
\end{equation}
where ${\bf W}=[{\bf w}_1,\cdots,{\bf w}_K]$.

\end{document}


\begin{comment}
\subsection{Self-organizing property of the SOM}

In the SOMs shown below, the number of training samples increases in 
horizontal direction ($4^3=64$, $6^3=216$, $8^3=512$, $10^3=1,000$, 
$20^3=8,000$, $40^3=64,000$), while the size of the SOM increases in 
vertical direction ($20\time 20$, $40\time 40$, $60\time 60$, $80\time 80$, 
$100\time 100$, $200\time 200$). 

\htmladdimg{../figures/SOM_Fig1.png}

The SOMs in the figure below all have the same size of $300\times 300$, while the 
number of color vectors increase in horizontal direction ($6^3=216$, $8^3=512$, 
$10^3=1,000$, $20^3=8,000$, $40^3=64,000$). The Gaussian width $\sigma$ is 1/3 of
the size of the SOM.

\htmladdimg{../figures/SOM_Fig2.png}

The SOMs in the figure below are similar to the previous ones except the SOMs 
have the same  size of $400\times 400$. 

\htmladdimg{../figures/SOM_Fig3.png}

%\htmladdimg{../figures/SOM_training.png}

%SOMs trained by progressively more iterations (top row: 50, 60, 70, 80, 90, 100,
%bottom: 200, 400, 800, 1600, 3200, 6400).

The four images below are obtained after 18, 19, 20, and 21 training iterations, 
respectively, show how the patterns start to emerge from random distributions of 
the nodes responding to the training samples. More examples can be found 
\htmladdnormallink{here}{../SOM1.html}.

\htmladdimg{../figures/movie18.gif}
\htmladdimg{../figures/movie19.gif}
\htmladdimg{../figures/movie20.gif}
\htmladdimg{../figures/movie21.gif}


%\htmladdnormallink{SOM genesis 1}{../SOM.html}, 

The figures below show the SOMs of size $600\times 600$ trained by $50^3=125,000$ 
samples. Here a smaller Gaussian width $\sigma$ (1/12 of the SOM size for the first 
two, 1/10 for the third) is used, consequently the resulting SOMs are less continuous 
and homogeneous in comparison to the SOMs trained with greater $\sigma$. The first 
and last SOMs are trained by 3000 iterations, while the middle one by 4000.

\htmladdimg{../figures/SOMa_125000_600_3000.png}\htmladdimg{../figures/SOMb_125000_600_3000.png}

\htmladdimg{../figures/SOMa_125000_600_4000.png}\htmladdimg{../figures/SOMb_125000_600_4000.png}

\htmladdimg{../figures/SOMa_125000_600_3000.gif}\htmladdimg{../figures/SOMb_125000_600_3000.gif}


The image below is the SOM of size $1,000\times 1,000$, trained by $100^3=1,000,000$
color vectors, with $\sigma$ equal to 1/3 of the size.

\htmladdimg{../figures/GODSOM1.png}

The image below is the same as before, with the only difference that here $\sigma$
is reduced from 1/3 to 1/12 of the size of the SOM.

\htmladdimg{../figures/SOMa_1000000_1000_5000.png}\htmladdimg{../figures/SOMb_1000000_1000_5000.png}

Below is a SOM of $2000\times 2000$ ($\sigma=1/3$ and $\sigma=1/10$ of image size).

\htmladdimg{../figures/SM_4096000_2000_2000.png}
\htmladdimg{../figures/SM_4096000_2000_3000.png}

The figure below is the 2-D SOM trained by a set of 4-D vector samples.
Although there seem to be some patterns in the image, they are not as obvious as 
those in the SOMs trained by 3-D vector samples.

\htmladdimg{../figures/SOM2Dfrom4D.gif}

This figure below shows the 3-D SOM trained by a set of 4-D vector samples. There 
are some obvious patterns in the SOM, very similar to those in a 2-D SOM.

\htmladdimg{../figures/SOM3Da.png}

\end{comment}


\subsection{Learning Vector Quantization (LVQ)}

LVQ is a supervised learning algorithm based on a set of training vectors
with known classes (labeled). The learning rule for unsupervised competitive
algorithm is modified to take advantage of the class information. Specifically,
if the winning node belongs to the same class of the input vector, its similarity
to the input vector is positively reinforced by moving its weight vector closer
to the input vector:
\begin{equation} 
  {\bf w}^{new}={\bf w}^{old}+\triangle {\bf w}
  ={\bf w}^{old}+\eta\;({\bf x}-{\bf w}^{old}) 
\end{equation}
However, if the winning node does not belong to the class of the input vector,
its weight vector is moved away from the input vector:
\begin{equation}
  {\bf w}^{new}={\bf w}^{old}-\triangle {\bf w}
  ={\bf w}^{old}-\eta\;({\bf x}-{\bf w}^{old}) 
\end{equation}
The weight vectors of all other nodes are unchanged.

This LVQ algorithm can be improved if two nodes closest to the input vector
are considered. If the winning node does not belong to the class of the input
but the second closest node does, and if the input vector is close enough to
the boundary between the two nodes, then both of the weight vectors of both
of these nodes are updated, so that the closest node is moved away from the
input vector while the second closest node is moved closer to the it.




\subsection{Gaussian Mixtures and Expectation Maximization}

In K-means clustering, each sample point ${\bf x}$ is assigned to one
of the $K$ clusters if it has the minimum Euclidean distance to the 
center of the cluster, without taking into consideration of the 
covariance of the cluster representing how widely or narrowly its 
member data points are distributed. 

We now consider the model of mixtures of Gaussians by which the 
distribution of the dataset can be better approximated, and 
clustering can be more efectively carried out by the associated 
{\em expectation maximization (EM)} algorithm, which finds the 
optimal parameters of the Gaussian mixture model that best fit the 
given dataset ${\bf X}=\{ {\bf x}_1,\cdots,{\bf x}_N \}$, including 
the covariance as well as the mean of each cluster, based on which 
the probabilities for each one of the $N$ samples ${\bf x}_n$ to
belong to each of the $K$ clusters are determined. This algorithm
can therefore considered as a soft-clustering method, compared to
the K-means clustering which is a har-clustering method that assigns 
each data point to only one particular cluster.

In the model of mixtures of Gaussians, each of the $K$ clusters (also
called {\em components} in this context) is modeled by a Gaussian 
${\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k)$ in terms of the mean vector
${\bf m}_k$ and covariance matrix ${\bf\Sigma}_k$, and the overall pdf 
is a weighted sum of all $K$ Gaussians each for one of the clusters
\begin{equation}
  p({\bf x})=\sum_{k=1}^K P_k {\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k)
\end{equation}


Specifically, we first introduce a random vector ${\bf z}=[z_1,\cdots,z_K]^T$
that indicates to which of the $K$ clusters a given sample ${\bf x}$
belongs. As ${\bf z}$ is hidden (not directly observed), it is called a
{\em latent variable}, and this method is a ``latent variable model''. 
Each of the $K$ components of ${\bf z}$ is a binary random variable 
$z_k\in\{0,\,1\}$, and they add up to 1, $\sum_{k=1}^K z_k =1$, i.e., 
only one of the $K$ components of ${\bf z}$ is 1, while all others are 
0. If $z_k=1$, then ${\bf z}$ indicates the given sample belongs to 
cluster $C_k$. The {\em prior probability} of such an event (without 
actually observating the specific sample ${\bf x}$) is denoted by 
\begin{equation}
  p(z_k=1)=P_k,\;\;\;\;\;\;\;\;(k=1,\cdots,K)
\end{equation}
As any sample ${\bf x}$ belongs to one and only one of the $K$
clusters, i.e., they are mutually exclusive and complementary,
the $K$ prior probabilities add up to 1:
\begin{equation}
  \sum_{k=1}^K p(z_k=1) =\sum_{k=1}^K P_k =1
\end{equation}
Also, all samples in each cluster are assumed to be normally distributed:
\begin{equation}
  p({\bf x}|z_k=1)={\cal N}({\bf x};{\bf m}_k,{\bf\Sigma}_k)
  =\frac{1}{(2\pi)^{d/2}|{\bf\Sigma}_k|^{1/2}}
  \exp\left(-\frac{1}{2}({\bf x}-{\bf m}_k)^T{\bf\Sigma}_k^{-1}({\bf x}-{\bf m}_k)\right)
\end{equation}
The product of these is
\begin{equation}
  p({\bf x},z_k=1)=p({\bf x}|z_k=1) p(z_k=1)
  =P_k {\cal N}({\bf x};{\bf m}_k,{\bf\Sigma}_k)
\end{equation}
These probabilities can be generalized to all $K$ cases: 
\begin{equation}
  p({\bf z})=\prod_{k=1}^K P_k^{z_k},\;\;\;\;\;\;\;
  p({\bf x}|{\bf z})=\prod_{k=1}^K {\cal N}({\bf x},{\bf m}_k,{\bf\Sigma}_l)^{z_k}
\end{equation}
and
\begin{equation}
  p({\bf x},{\bf z})=p({\bf z})\;p({\bf x}|{\bf z})
  =\prod_{k=1}^K P_k^{z_k}{\cal N}({\bf x},{\bf m}_k,{\bf\Sigma}_k)^{z_k}
\end{equation}
The overall distribution $p({\bf x})$ of a sample ${\bf x}$, regardless 
to which of the clusters it belongs, is a weighted sum of all $K$ such 
Gaussians, the {\em Gaussian mixture model (GMM)} of the dataset:
\begin{eqnarray}
  p({\bf x})&=&\sum_{\bf z} p({\bf x}|{\bf z})\;p({\bf z})
  =\sum_{k=1}^K p({\bf x}|z_k=1)\;p(z_k=1)
  \nonumber\\
  &=&\sum_{k=1}^K p({\bf x},z_k=1)
  =\sum_{k=1}^K P_k {\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k)
\end{eqnarray}
We have
\begin{equation}
  \int_{-\infty}^\infty p({\bf x})\,d{\bf x}=\sum_{k=1}^K P_k \int_{-\infty}^\infty
  {\cal N}({\bf x}; {\bf m}_k,{\bf\Sigma}_k)\,d{\bf x}=\sum_{k=1}^K P_k =1
\end{equation}

Moreover, based on Bayes' theorem, we can further find the
{\em posterior probability} $p(z_k=1|{\bf x})$, denoted by $r_k$, for 
an observed ${\bf x}$ to belong to cluster $C_k$:
\begin{equation}
  r_k=p(z_k=1|{\bf x})=\frac{p({\bf x}|z_k=1)\;p(z_k=1)}{p({\bf x})}
  =\frac{P_k\,{\cal N}({\bf x};{\bf m}_k{\bf\Sigma}_k)}
  {\sum_{l=1}^K P_l\,{\cal N}({\bf x};{\bf m}_l{\bf\Sigma}_l)}
\end{equation}
Obviously, these posterior probabilities also add up to 1, same as the
prior probabilities $P_k$:
\begin{equation}
  \sum_{k=1}^K p(z_k=1|{\bf x})=\sum_{k=1}^K r_k=1
\end{equation}
Here the posterior probability $p(z_k=1|{\bf x})$ can also be viewed
and referred to as the {\em responsibility} cluster $C_k$ takes for 
${\bf x}$.

We next consider the {\em expectation maximization (EM)} algorithm,
by which the model parameters of the GMM, including $P_k$, ${\bf m}_k$, 
and ${\bf\Sigma}_k$ for all $k=1,\cdots,K$, can be obtained by the 
method of maximum likelihood (ML) based on the observed dataset
${\bf X}=[{\bf x}_1,\cdots,{\bf x}_N]$. Specifically, the likelihood 
function of the model parameters is the conditional probability of all
data points in ${\bf X}$ given the model parameters ${\bf m}_k,\;{\bf\Sigma}_k\;
P_k,\;(k=1,\cdots,K)$:
\begin{eqnarray}
  &&L(P_k,\;{\bf m}_k,\;{\bf\Sigma}_k\;(k=1,\cdots,K)|{\bf X})
  = p({\bf X}|P_k,\;{\bf m}_k,\;{\bf\Sigma}_k\;(k=1,\cdots,K))
  \nonumber\\
  &=&\prod_{n=1}^N p({\bf x}_n|P_k,\;{\bf m}_k,\;{\bf\Sigma}_k\;(k=1,\cdots,K))
  =\prod_{n=1}^N \sum_{k=1}^K P_k {\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)
\end{eqnarray}
and the log likelihood is
\begin{equation}
  \log L(P_k,\;{\bf m}_k,\;{\bf\Sigma}_k\;(k=1,\cdots,K)|{\bf X})
  =\sum_{n=1}^N \log \sum_{k=1}^K P_k {\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)
\end{equation}
We can now set the derivatives of this log likelihood with respect to 
each of the parameters to zero, and solve the resulting equations, to
find the optimal model parameters ${\bf m}_k$, ${\bf\Sigma}_k$ and $P_k$ 
that maximize likelihood function, as shown below.

\begin{itemize}
\item Find ${\bf m}_k$ 

  \begin{eqnarray}
    \frac{\partial}{\partial{\bf m}_k}\log L
    &=&\sum_{n=1}^N \frac{\partial}{\partial{\bf m}_k}
    \left[\log \sum_{k=1}^K P_k {\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k) \right]
    \nonumber\\
    &=&\sum_{n=1}^N \frac{1}{\sum_{l=1}^K P_l {\cal N}({\bf x}_n|{\bf m}_l,{\bf\Sigma}_l)}
    \frac{\partial}{\partial{\bf m}_k}P_k{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)
    \nonumber\\
    &=&-\frac{1}{2}\sum_{n=1}^N \frac{P_k{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)}
        {\sum_{l=1}^K P_l {\cal N}({\bf x}_n|{\bf m}_l,{\bf\Sigma}_l)}
        {\bf\Sigma}_k^{-1}({\bf x}_n-{\bf m}_k)
    \nonumber\\
    &=&-\frac{1}{2}\sum_{n=1}^N P_{nk}{\bf\Sigma}_k^{-1}({\bf x}_n-{\bf m}_k)={\bf 0}
  \end{eqnarray}
  Here we have defined
  \begin{equation}
    P_{nk}=\frac{P_k\,{\cal N}({\bf x}_n;{\bf m}_k{\bf\Sigma}_k)}
    {\sum_{l=1}^K P_l\,{\cal N}({\bf x}_n;{\bf m}_l{\bf\Sigma}_l)}
    =p(z_k=1|{\bf x}_n)
  \end{equation}
  which happens to be the posterior probability for ${\bf x}_n$ to belong to
  cluster $C_k$, as defined previously. We have also used the logarithmic 
  derivative rule
  \begin{equation}
    \frac{d}{dx}\left(\log f(x)\right)=\frac{1}{f(x)}\;\frac{df(x)}{dx}
    \;\;\;\;\;\;\mbox{i.e.}\;\;\;\;\;\;
    \frac{df(x)}{dx}=f(x)\;\frac{d}{dx}\left(\log f(x)\right)
  \end{equation}
  to get
  \begin{eqnarray}
    \frac{\partial}{\partial{\bf m}_k}{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)
    &=&{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)\frac{\partial}{\partial{\bf m}_k}
    \left[\log{\cal N}({\bf x}|{\bf m},{\bf\Sigma})\right]
    \nonumber\\
    &=&{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)\frac{\partial}{\partial{\bf m}_k}
    \left[-\frac{1}{2}({\bf x}_n-{\bf m}_k)^T{\bf\Sigma}_k^{-1}({\bf x}_n-{\bf m}_k)\right]
    \nonumber\\
    &=&-\frac{1}{2}{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}){\bf\Sigma}_k^{-1}({\bf x}_n-{\bf m}_k)
  \end{eqnarray}
  Multiplying $-2{\bf\Sigma}_k$ on both sides of the equation above we get
  \begin{equation}
    \sum_{n=1}^N P_{nk}({\bf x}_n-{\bf m}_k)={\bf 0}
  \end{equation}
  which can be solved to get
  \begin{equation}
    {\bf m}_k=\frac{1}{\sum_{n=1}^N P_{nk}} \sum_{n=1}^N P_{nk}{\bf x}_n 
    =\frac{1}{N_k} \sum_{n=1}^N P_{nk}{\bf x}_n 
  \end{equation}
  where we have defined
  \begin{equation}
    N_k=\sum_{n=1}^N P_{nk}=\sum_{n=1}^N p(z_k=1|{\bf x}_n)
  \end{equation}
  satisfying
  \begin{equation}
    \sum_{k=1}^K N_k=\sum_{k=1}^K\left(\sum_{n=1}^N P_{nk}\right)
    =\sum_{n=1}^N\left(\sum_{k=1}^K P_{nk}\right)=\sum_{n=1}^N 1=N
  \end{equation}

\item Find ${\bf\Sigma}_k$

%https://www.ics.uci.edu/~welling/teaching/KernelsICS273B/MatrixCookBook.pdf

  \begin{eqnarray}
    \frac{\partial}{\partial{\bf\Sigma}_k}\log L
    &=&\sum_{n=1}^N \frac{\partial}{\partial{\bf\Sigma}_k}\left[\log \sum_{k=1}^K P_k {\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k) \right]
    \nonumber\\
    &=&\sum_{n=1}^N \frac{1}{\sum_{l=1}^K P_l {\cal N}({\bf x}_n|{\bf m}_l,{\bf\Sigma}_l)}
    \frac{\partial}{\partial{\bf\Sigma}_k}P_k{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)
    \nonumber\\
    &=&-\frac{1}{2}\sum_{n=1}^N \frac{P_k{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)} 
        {\sum_{l=1}^K P_l {\cal N}({\bf x}_n|{\bf m}_l,{\bf\Sigma}_l)}
    \left[{\bf\Sigma}^{-1}_k-{\bf\Sigma}^{-1}_k({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T{\bf\Sigma}^{-1}_k\right]
    \nonumber\\
    &=&-\frac{1}{2}\sum_{n=1}^N P_{nk} \left[{\bf\Sigma}^{-1}_k-{\bf\Sigma}^{-1}_k({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T{\bf\Sigma}^{-1}_k\right]={\bf 0}
  \end{eqnarray}
  where we have used the following fact
  \begin{eqnarray}
    \frac{\partial}{\partial{\bf\Sigma}_k}{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)
    &=&{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)\frac{\partial}{\partial{\bf\Sigma}_k}
    \left[\log{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)\right]
    \nonumber\\
    &=&-\frac{1}{2}{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)
    \left[\frac{\partial}{\partial{\bf\Sigma}_k}\log|{\bf\Sigma}_k|
      +\frac{\partial}{\partial{\bf\Sigma}_k}({\bf x}_n-{\bf m}_k)^T{\bf\Sigma}_k^{-1}({\bf x}_n-{\bf m}_k)\right]
    \nonumber\\
    &=&-\frac{1}{2}{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)
    \left[{\bf\Sigma}_k^{-1}-{\bf\Sigma}_k^{-1}
      ({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T{\bf\Sigma}_k^{-1}\right]={\bf 0}
  \end{eqnarray}
  Here we have used the following facts:
  \begin{equation}
    \frac{d}{d{\bf A}}\log|{\bf A}|=({\bf A}^{-1})^T,
    \;\;\;\;\;\;
    \frac{d}{d{\bf A}} \left({\bf a}^T{\bf A}^{-1}{\bf b}\right)
    =-({\bf A}^{-1})^T{\bf a}{\bf b}^T({\bf A}^{-1})^T
  \end{equation}
  For more details see \htmladdnormallink{Matrix Cookbook}{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}.
  Pre and post multiplying ${\bf\Sigma}_k$ on both sides of the equation above
  we get
  \begin{equation}
    \sum_{n=1}^N P_{nk}\left({\bf\Sigma}_k-({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T\right)={\bf 0}
  \end{equation}
  Solving for ${\bf\Sigma}_k$, we get
  \begin{equation}
    {\bf\Sigma}_k=\frac{1}{N_k}\sum_{n=1}^N P_{nk}({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T
  \end{equation}

\item Find $P_k$

  As $P_k$ also has to satisfy the constraint $\sum_{k=1}^K P_k=1$, an additional
  term is included in the Lagrangian function to be maximized:
  \begin{eqnarray}
    &&\frac{\partial}{\partial P_k} \left[\sum_{n=1}^N \log \sum_{k=1}^K 
      P_k {\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k) \right]
    +\lambda\left(\sum_{k=1}^K P_k-1\right)
    \nonumber\\
    &=& \sum_{n=1}^N \frac{{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)}
        {\sum_{l=1}^K P_l {\cal N}({\bf x}_n|{\bf m}_l,{\bf\Sigma}_l)} +\lambda=0
  \end{eqnarray}
  where $\lambda$ is the Lagrange multiplier for the equation constraint.
  Multiplying both sides by $P_k$ we get
  \begin{equation}
    \sum_{n=1}^N \frac{P_k{\cal N}({\bf x}_n|{\bf m}_k,{\bf\Sigma}_k)}
        {\sum_{l=1}^K P_l {\cal N}({\bf x}_n|{\bf m}_l,{\bf\Sigma}_l)}+P_k\lambda
        =\sum_{n=1}^N P_{nk}+P_k \lambda=N_k+P_k\lambda=0
  \end{equation}
  Summing the above for $k=1,\cdots,K$, we get
  \begin{equation}
    \sum_{k=1}^K \left(\sum_{n=1}^N P_{nk}+P_k \lambda\right)
    =\sum_{n=1}^N \sum_{k=1}^K P_{nk}+\lambda\sum_{k=1}^K P_k 
    =N+\lambda=0
  \end{equation}
  i.e., $\lambda=-N$. Substituting this back into the previous equation we get
  \begin{equation}
    P_k=\frac{N_k}{N}
  \end{equation}
 
\end{itemize}

In summary, here is the EM clustering algorithm based on Gaussian mixture model:
\begin{enumerate}
\item Initialize means ${\bf m}_k$, covariance ${\bf\Sigma}_k$ and coefficient
  $P_k$ 
\item Expectation ({\bf E}-step): Find $P_{nk}$ for all $N$ data points and 
  all $K$ clusters:
  \begin{equation}
    P_{nk}=p(r_k=1|{\bf x}_n)=\frac{P_k\,{\cal N}({\bf x}_n;{\bf m}_k{\bf\Sigma}_k)}
    {\sum_{l=1}^K P_l\,{\cal N}({\bf x}_n;{\bf m}_l{\bf\Sigma}_l)}
  \end{equation}
  Find $N_k=\sum_{n=1}^N P_{nk}$.
  Assign ${\bf x}_n$ to cluster $C_k$ if $P_{nk}=\max\{ p_{nl},\;(l=1,\cdots,K)\}$.

\item Maximization ({\bf M}-step): Recalculate the parameters that
  maximize the likelihood function:
  \begin{eqnarray}
    {\bf m}_k&=&\frac{1}{N_k} \sum_{n=1}^N P_{nk}{\bf x}_n 
    \nonumber\\
    {\bf\Sigma}_k&=&\frac{1}{N_k}\sum_{n=1}^N P_{nk}({\bf x}_n-{\bf m}_k)({\bf x}_n-{\bf m}_k)^T
    \nonumber\\
    P_k&=&\frac{N_k}{N}
  \end{eqnarray}
\item Termination if the parameters or the log likelihood function have 
  converged. Otherwise go back to step 2.

\end{enumerate}

We can show that the K-means algorithm is actually a special case 
of the EM algorithm when all covariance matrices are the same
${\bf\Sigma}_k=\varepsilon{\bf I}$, where $\varepsilon$ is a 
scaling factor which appraoches to zero. In this case we have:
\begin{equation}
  p({\bf x}|z_k=1)={\cal N}({\bf x}|{\bf m}_k,\varepsilon{\bf I})
  =\frac{1}{(2\pi)^{d/2}\varepsilon^{1/2}}
  \exp\left(-\frac{1}{2\varepsilon}||{\bf x}-{\bf m}_k||^2\right)
\end{equation}
and 
\begin{equation}
  r_k=\frac{P_k{\cal N}({\bf x};{\bf m}_l{\bf\Sigma}_l)}
  {\sum_{l=1}^K P_l {\cal N}({\bf x};{\bf m}_l{\bf\Sigma}_l)}
  =\frac{P_k\exp(-||{\bf x}-{\bf m}_k||^2/2\varepsilon)}
  {\sum_{l=1}^K P_l\exp(-||{\bf x}-{\bf m}_l||^2/2\varepsilon)}
\end{equation}
When $\varepsilon\rightarrow 0$, all terms approach to zero, 
but the one with minimum $||{\bf x}-{\bf m}_k||$ is the slowest,
and $P_{nk}$ defined above for the EM algorithm becomes the same as 
that defined for the K-means algorithm:
\begin{equation}
  \lim\limits_{\varepsilon\rightarrow 0} P_{nk}=\lim\limits_{\varepsilon\rightarrow 0} 
  \frac{P_k\exp(-||{\bf x}_n-{\bf m}_k||^2/2\varepsilon)}
  {\sum_{l=1}^K P_l\exp(-||{\bf x}_n-{\bf m}_l||^2/2\varepsilon)}
  =\left\{\begin{array}{ll}
  1 & \mbox{if $k=\argmin_l ||{\bf x}_n-{\bf m}_l||$}\\
  0 & \mbox{otherwise}\end{array}\right.
\end{equation}
i.e., $P_{nk}=p(z_k=1|{\bf x}_n)$ defined above as a {\em soft} 
posterior probability becomes a {\em hard} binary decision, and 
$N_k=\sum_{n=1}^N P_{nk}$ defined above as the sum of posterior 
probabilities for all $N$ data points to belong to cluster $C_k$ 
becomes $N_k$ in the K-means method, the number of data points 
assigned to cluster $C_k$, i.e, the probabilistic EM method based
on both ${\bf m}$ and ${\bf\Sigma}$ becomes the deterministic 
K-means method based on ${\bf m}$ only.

{\bf Examples} The same dataset is used to test both the K-means
and EM clustering methods. The first panel shows the 10 iterations
of the K-means method by the time it converged, while the second
panel shows the first 16 iterations of the EM method. We see that
the red and green clusters both normally distributed with similar
means but very different covariance matrices. They cannot be 
separated by the K-means method but can be correctly identified by
the EM method.

\htmladdimg{../figures/ClusteringKmeans.png}
\htmladdimg{../figures/clusteringEM.png}

The two clustering methods are also applied to the Iris dataset,
which has three classes each of 50 4-dimensional sample vectors.
The PCA method is used to visualize the first two principal 
compnents, as shown below. Also, as can be seen from their c
onfussion matrices, the error rate of the K-means method is 18/150,
while that of the EM method is 5/150.

\begin{table}\caption{K-means}
\begin{array}{r|r|r}\hline 0 & 0 & 50\\\hline 50 & 0 & 0\\\hline18 & 32 & 0\\\hline
\end{array}
\end{table}
\begin{table}\caption{EM}
\begin{array}{r|r|r}\hline0 & 0 & 50\\\hline45 & 5 & 0\\\hline0 & 50 & 0\\\hline
\end{array}
\end{table}

\htmladdimg{../figures/IrisKmeans.png}
\htmladdimg{../figures/IrisEM.png}
