<!DOCTYPE HTML>

<!--Converted with LaTeX2HTML 2019 (Released January 1, 2019) -->
<HTML lang="EN">
<HEAD>
<TITLE>Gradient Descent Method</TITLE>
<META NAME="description" CONTENT="Gradient Descent Method">
<META NAME="keywords" CONTENT="ch3">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="viewport" CONTENT="width=device-width, initial-scale=1.0">
<META NAME="Generator" CONTENT="LaTeX2HTML v2019">

<LINK REL="STYLESHEET" HREF="ch3.css">

<LINK REL="next" HREF="node8.html">
<LINK REL="previous" HREF="node6.html">
<LINK REL="next" HREF="node8.html">
</HEAD>

<BODY >

<DIV CLASS="navigation"><!--Navigation Panel-->
<A
 HREF="node8.html">
<IMG WIDTH="37" HEIGHT="24" ALT="next" SRC="next.png"></A> 
<A
 HREF="node2.html">
<IMG WIDTH="26" HEIGHT="24" ALT="up" SRC="up.png"></A> 
<A
 HREF="node6.html">
<IMG WIDTH="63" HEIGHT="24" ALT="previous" SRC="prev.png"></A>   
<BR>
<B> Next:</B> <A
 HREF="node8.html">Line minimization</A>
<B> Up:</B> <A
 HREF="node2.html">Unconstrained Optimization</A>
<B> Previous:</B> <A
 HREF="node6.html">Newton's method</A>
<BR>
<BR></DIV>
<!--End of Navigation Panel-->

<H2><A ID="SECTION00025000000000000000">
Gradient Descent Method</A>
</H2>

<P>
Newton's method discussed above is based on the Hessian <!-- MATH
 ${\bf H}_f({\bf x})$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.78ex; " SRC="img266.svg"
 ALT="${\bf H}_f({\bf x})$"></SPAN>
of the function <!-- MATH
 $f({\bf x})$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img4.svg"
 ALT="$f({\bf x})$"></SPAN> to be minimized as well as its gradient 
<!-- MATH
 ${\bf g}_f({\bf x})$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.78ex; " SRC="img46.svg"
 ALT="${\bf g}_f({\bf x})$"></SPAN>. The method is not applicable if the Hessian 
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.78ex; " SRC="img28.svg"
 ALT="${\bf H}_f$"></SPAN> is not available, or the cost of computing the inverse 
<!-- MATH
 ${\bf H}^{-1}_f$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 3.48ex; vertical-align: -1.13ex; " SRC="img267.svg"
 ALT="${\bf H}^{-1}_f$"></SPAN> is too high. In such a case, the gradient descent method
can be used without using the Hessian matrix.

<P>
We first consider the minimization of a single-variable function 
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img60.svg"
 ALT="$f(x)$"></SPAN>. From any inital point <SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.46ex; " SRC="img64.svg"
 ALT="$x_0$"></SPAN>, we can move to a nearby point
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
x_1=x_0+\Delta x_0=x_0-\delta f'(x_0),\;\;\;\;\;
  \mbox{where}\;\;\;\;\Delta x_0=-\delta f'(x_0)
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.70ex; " SRC="img268.svg"
 ALT="$\displaystyle x_1=x_0+\Delta x_0=x_0-\delta f'(x_0),\;\;\;\;\;$">&nbsp; &nbsp;where<IMG STYLE="height: 2.79ex; vertical-align: -0.70ex; " SRC="img269.svg"
 ALT="$\displaystyle \;\;\;\;\Delta x_0=-\delta f'(x_0)$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">46</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
No matter whether <SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img270.svg"
 ALT="$f'(x_0)$"></SPAN> is positive or negative, the function value 
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img271.svg"
 ALT="$f(x_1)$"></SPAN> (approximated by the first two terms of its Taylor series) is 
always reduced if the positive step size <SPAN CLASS="MATH"><IMG STYLE="height: 2.09ex; vertical-align: -0.21ex; " SRC="img223.svg"
 ALT="$\delta&gt;0$"></SPAN> is small enough:
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
f(x_1)\approx f(x_0)+f'(x_0)\Delta x_0=f(x_0)-|f'(x_0)|^2\delta<f(x_0)
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 3.02ex; vertical-align: -0.70ex; " SRC="img272.svg"
 ALT="$\displaystyle f(x_1)\approx f(x_0)+f'(x_0)\Delta x_0=f(x_0)-\vert f'(x_0)\vert^2\delta&lt;f(x_0)$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">47</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
This process can be carried out iteratively 
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
x_{n+1}=x_n-\delta_n \; f'(x_n)
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.70ex; " SRC="img273.svg"
 ALT="$\displaystyle x_{n+1}=x_n-\delta_n \; f'(x_n)$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">48</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
until eventually reaching a point <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img165.svg"
 ALT="$x^*$"></SPAN> at which <SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img274.svg"
 ALT="$f'(x^*)=0$"></SPAN> and no 
further progress can be made, i.e. a local minimum of the function is 
obtained.

<P>
This simple method can be generalized to minimize a multi-variable 
objective function <!-- MATH
 $f({\bf x})=f(x_1,\cdots,x_N)$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img197.svg"
 ALT="$f({\bf x})=f(x_1,\cdots,x_N)$"></SPAN> in N-D space. The 
derivative of the 1-D case is generalized to the gradient vector 
<!-- MATH
 ${\bf g}({\bf x})=df({\bf x})/d{\bf x}$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img275.svg"
 ALT="${\bf g}({\bf x})=df({\bf x})/d{\bf x}$"></SPAN> of function <!-- MATH
 $f({\bf x})$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img4.svg"
 ALT="$f({\bf x})$"></SPAN>,
which is in the direction along which the function increases most rapidly 
with the steepest slope, perpendicular to the contour or iso-lines of the
function <!-- MATH
 $f({\bf x})$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img4.svg"
 ALT="$f({\bf x})$"></SPAN>. The fastest way to reduce <!-- MATH
 $f({\bf x})$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img4.svg"
 ALT="$f({\bf x})$"></SPAN> is to go 
down hill along the opposite direction of the gradient vector. 

<P>
Specifically the gradient descent method (also called steepest 
descent or down hill method) carries out the following approximation
<!-- MATH
 ${\bf x}_{n+1}={\bf x}_n+\Delta{\bf x}_n$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.66ex; " SRC="img276.svg"
 ALT="${\bf x}_{n+1}={\bf x}_n+\Delta{\bf x}_n$"></SPAN> (the first two terms of the 
Taylor series) with <!-- MATH
 $\Delta{\bf x}=-\delta{\bf g}\;(\delta>0)$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img277.svg"
 ALT="$\Delta{\bf x}=-\delta{\bf g}\;(\delta&gt;0)$"></SPAN>:
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
f({\bf x}_1)=f({\bf x}_0+\Delta{\bf x})
  \approx f({\bf x}_0)+{\bf g}^T_0 \Delta{\bf x}
  =f({\bf x}_0)-\delta  {\bf g}^T_0 {\bf g}_0
  =f({\bf x}_0)-\delta  ||{\bf g}||^2 < f({\bf x}_0)
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 3.02ex; vertical-align: -0.70ex; " SRC="img278.svg"
 ALT="$\displaystyle f({\bf x}_1)=f({\bf x}_0+\Delta{\bf x})
\approx f({\bf x}_0)+{\bf...
...T_0 {\bf g}_0
=f({\bf x}_0)-\delta \vert\vert{\bf g}\vert\vert^2 &lt; f({\bf x}_0)$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">49</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
iteratively:
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
{\bf x}_{n+1}={\bf x}_n-\delta_n \, {\bf g}_n
  =({\bf x}_{n-1}-\delta_{n-1}\,{\bf g}_{n-1})-\delta_n \, {\bf g}_n=\cdots=
  {\bf x}_0-\sum_{i=0}^n \delta_n\,{\bf g}_i
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 7.20ex; vertical-align: -3.09ex; " SRC="img279.svg"
 ALT="$\displaystyle {\bf x}_{n+1}={\bf x}_n-\delta_n \, {\bf g}_n
=({\bf x}_{n-1}-\de...
...{n-1})-\delta_n \, {\bf g}_n=\cdots=
{\bf x}_0-\sum_{i=0}^n \delta_n\,{\bf g}_i$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">50</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
untill eventually reaching a point at which <!-- MATH
 ${\bf g}={\bf0}$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.32ex; vertical-align: -0.57ex; " SRC="img280.svg"
 ALT="${\bf g}={\bf0}$"></SPAN> and the 
minimum of the function <!-- MATH
 $f({\bf x})$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img4.svg"
 ALT="$f({\bf x})$"></SPAN> is reached.

<P>
Comparing the gradient descent method with Newton's method we see that
here the Hessian matrix is no longer used. The iteration simply follows 
a search direction <!-- MATH
 ${\bf d}_n=-{\bf g}_n$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.32ex; vertical-align: -0.57ex; " SRC="img281.svg"
 ALT="${\bf d}_n=-{\bf g}_n$"></SPAN>, which is different from the 
search direction <!-- MATH
 ${\bf d}_n=-{\bf H}^{-1}_n{\bf g}_n$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.69ex; " SRC="img282.svg"
 ALT="${\bf d}_n=-{\bf H}^{-1}_n{\bf g}_n$"></SPAN> of Newton's method,
based on <SPAN CLASS="MATH"><IMG STYLE="height: 2.32ex; vertical-align: -0.46ex; " SRC="img283.svg"
 ALT="${\bf H}_n$"></SPAN> as well as <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.57ex; " SRC="img256.svg"
 ALT="${\bf g}_n$"></SPAN>. Specially, when <!-- MATH
 ${\bf H}={\bf I}$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img284.svg"
 ALT="${\bf H}={\bf I}$"></SPAN>, 
the two methods become the same.

<P>
As the gradient descent method relies only on the gradient vector of the
objective function without any information contained in the second order 
derivatives in the Hessian matrix, it does not have as much information as
Newton's method and therefore may not be as efficient. For example, when 
the function is quadratic, as discussed before, Newton's method can find 
the solution in a single step from any initial guess, but it may take the 
gradient descent method many steps to reach the solution, because it always 
follows the negative direction of the local gradient, which typically does 
not point to the solution directly. However, for the same reason, the 
gradient descent method is computationally less expensive and will be 
effective when the Hessian matrix is not used.

<P>
<B>Example:</B> Consider a two-variable quadratic function in the following
general form:
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
f({\bf x})={\bf x}^T{\bf Ax}+{\bf b}^T{\bf x}+c
  =[x_1\;x_2]\left[\begin{array}{cc}a_{11} & a_{12}\\a_{21} & a_{22}\end{array}\right]
  \left[\begin{array}{c}x_1\\x_2\end{array}\right]
  +[b_1\;b_2]\left[\begin{array}{c}x_1\\x_2\end{array}\right]+c
  \nonumber
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 5.81ex; vertical-align: -2.32ex; " SRC="img285.svg"
 ALT="$\displaystyle f({\bf x})={\bf x}^T{\bf Ax}+{\bf b}^T{\bf x}+c
=[x_1\;x_2]\left[...
...nd{array}\right]
+[b_1\;b_2]\left[\begin{array}{c}x_1\\ x_2\end{array}\right]+c$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
&nbsp;&nbsp;&nbsp;</TD></TR>
</TABLE></DIV>
<P></P>
where <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img286.svg"
 ALT="${\bf A}$"></SPAN> is a symmetric positive semidefinite matrix,
i.e., <!-- MATH
 $a_{12}=a_{21}$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.46ex; " SRC="img287.svg"
 ALT="$a_{12}=a_{21}$"></SPAN> and <!-- MATH
 ${\bf A}\ge 0$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.32ex; vertical-align: -0.43ex; " SRC="img288.svg"
 ALT="${\bf A}\ge 0$"></SPAN>. Specially, if we let 
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
{\bf A}=\left[\begin{array}{cc}2 & 1\\1 & 1\end{array}\right],\;\;\;\;\;
  {\bf b}=\left[\begin{array}{c}0 \\0\end{array}\right],\;\;\;\;\;  c=0
  \nonumber
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 5.81ex; vertical-align: -2.32ex; " SRC="img289.svg"
 ALT="$\displaystyle {\bf A}=\left[\begin{array}{cc}2 &amp; 1\\ 1 &amp; 1\end{array}\right],\;\;\;\;\;
{\bf b}=\left[\begin{array}{c}0 \\ 0\end{array}\right],\;\;\;\;\; c=0$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
&nbsp;&nbsp;&nbsp;</TD></TR>
</TABLE></DIV>
<P></P>
then we have the following function:
<P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
f(x_1,x_2)=\frac{1}{2}[x_1\;x_2]\left[\begin{array}{cc}2&1\\1&1\end{array}\right]
  \left[\begin{array}{c}x_1\\x_2\end{array}\right]=\frac{1}{2}(2x_1^2+2x_1x_2+x_2^2)
  \nonumber
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 5.81ex; vertical-align: -2.32ex; " SRC="img290.svg"
 ALT="$\displaystyle f(x_1,x_2)=\frac{1}{2}[x_1\;x_2]\left[\begin{array}{cc}2&amp;1\\ 1&amp;1\...
...t[\begin{array}{c}x_1\\ x_2\end{array}\right]=\frac{1}{2}(2x_1^2+2x_1x_2+x_2^2)$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
&nbsp;&nbsp;&nbsp;</TD></TR>
</TABLE></DIV>
<P></P>
which has a minimum <!-- MATH
 $f(x_1,\,x_2)=0$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img291.svg"
 ALT="$f(x_1,\,x_2)=0$"></SPAN> at <SPAN CLASS="MATH"><IMG STYLE="height: 2.32ex; vertical-align: -0.57ex; " SRC="img292.svg"
 ALT="$x=y=0$"></SPAN>. 

<P>
<IMG STYLE="" SRC="figures/gradient1.png"
 ALT="gradient1.png">

<P>
We assume the initial guess is <!-- MATH
 ${\bf x}_0=[1,\;2]^T$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.70ex; " SRC="img293.svg"
 ALT="${\bf x}_0=[1,\;2]^T$"></SPAN>, at which the
gradient is <!-- MATH
 ${\bf g}_0=[4,\;3]^T$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.70ex; " SRC="img294.svg"
 ALT="${\bf g}_0=[4,\;3]^T$"></SPAN>. Now we compare the gradient method
with Newton's method, in terms of the search direction <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img295.svg"
 ALT="${\bf d}$"></SPAN> and 
progress of the iterations:

<UL>
<LI>Newton's method: Here we have
  <P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
{\bf g}=\left[\begin{array}{c}2x_1+x_2\\x_1+x_2\end{array}\right],\;\;\;\;\;
    {\bf H}={\bf A}=\left[\begin{array}{cc}2&1\\1&1\end{array}\right],\;\;\;\;
    {\bf H}^{-1}=\left[\begin{array}{rr}1&-1\\-1&2\end{array}\right]
    \nonumber
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 5.81ex; vertical-align: -2.32ex; " SRC="img296.svg"
 ALT="$\displaystyle {\bf g}=\left[\begin{array}{c}2x_1+x_2\\ x_1+x_2\end{array}\right...
...ght],\;\;\;\;
{\bf H}^{-1}=\left[\begin{array}{rr}1&amp;-1\\ -1&amp;2\end{array}\right]$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
&nbsp;&nbsp;&nbsp;</TD></TR>
</TABLE></DIV>
<P></P>
and the search direction is 
  <!-- MATH
 ${\bf d}_0=-{\bf H}^{-1}{\bf g}_0=-[1,\;2]^T$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.70ex; " SRC="img297.svg"
 ALT="${\bf d}_0=-{\bf H}^{-1}{\bf g}_0=-[1,\;2]^T$"></SPAN> (the red arrow in the figure).
  The iteration is:
  <P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
{\bf x}_1={\bf x}_0-{\bf H}^{-1}{\bf g}_0=\left[\begin{array}{rr}1\\
        2\end{array}\right]
    -\left[\begin{array}{rr}1&-1\\-1&2\end{array}\right]
    \left[\begin{array}{r}4\\3\end{array}\right]
    =\left[\begin{array}{r}0\\0\end{array}\right]
    \nonumber
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 5.81ex; vertical-align: -2.32ex; " SRC="img298.svg"
 ALT="$\displaystyle {\bf x}_1={\bf x}_0-{\bf H}^{-1}{\bf g}_0=\left[\begin{array}{rr}...
...array}{r}4\\ 3\end{array}\right]
=\left[\begin{array}{r}0\\ 0\end{array}\right]$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
&nbsp;&nbsp;&nbsp;</TD></TR>
</TABLE></DIV>
<P></P>
which is the minimum of the function.
</LI>
<LI>The gradient descent method: the search direction is
  <!-- MATH
 ${\bf d}_0=-{\bf g}_0=-[4,\;3]^T$
 -->
<SPAN CLASS="MATH"><IMG STYLE="height: 2.79ex; vertical-align: -0.70ex; " SRC="img299.svg"
 ALT="${\bf d}_0=-{\bf g}_0=-[4,\;3]^T$"></SPAN>, perpendicular to the contour of 
  the function. The first iteration is:
  <P></P>
<DIV CLASS="mathdisplay"><!-- MATH
 \begin{equation}
{\bf x}_1={\bf x}_0-\delta{\bf g}_0
    =\left[\begin{array}{r}1\\2\end{array}\right]
    -\delta\left[\begin{array}{r}4\\3\end{array}\right]
    =\left[\begin{array}{r}1-\delta 4\\2-\delta 3\end{array}\right]
    \nonumber
\end{equation}
 -->
<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><IMG STYLE="height: 5.81ex; vertical-align: -2.32ex; " SRC="img300.svg"
 ALT="$\displaystyle {\bf x}_1={\bf x}_0-\delta{\bf g}_0
=\left[\begin{array}{r}1\\ 2\...
...{array}\right]
=\left[\begin{array}{r}1-\delta 4\\ 2-\delta 3\end{array}\right]$"></SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
&nbsp;&nbsp;&nbsp;</TD></TR>
</TABLE></DIV>
<P></P>
We need to determine the step size <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img253.svg"
 ALT="$\delta$"></SPAN> (to be considered later)
  to find <SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.46ex; " SRC="img301.svg"
 ALT="${\bf x}_1$"></SPAN>, and then continue the iteration.
</LI>
</UL>

<P>
The figure below compares the search directions of the gradient descent
method (blue) based only on the gradient vector <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.57ex; " SRC="img200.svg"
 ALT="${\bf g}_0$"></SPAN>, the local 
information at the point <SPAN CLASS="MATH"><IMG STYLE="height: 1.63ex; vertical-align: -0.46ex; " SRC="img125.svg"
 ALT="${\bf x}_0$"></SPAN>, and Newton's method (red) based on 
the Hessian matrix <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img232.svg"
 ALT="${\bf H}$"></SPAN>, the global of the quadratic function, as well
as the gradient <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.57ex; " SRC="img200.svg"
 ALT="${\bf g}_0$"></SPAN>.

<P>
<IMG STYLE="" SRC="figures/QuadraticContourEx2.png"
 ALT="QuadraticContourEx2.png">

<P>
We see that Newton's method finds the solution in a single step, but
the gradient descent method requires an iteration, and therefore less
effective. This is because the gradient descent method only has available
the local information provided by the first order derivative <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.57ex; " SRC="img302.svg"
 ALT="${\bf g}$"></SPAN>, 
the gradient direction of the function at a single point, while Newton's
method can make use of the second order derivative <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img232.svg"
 ALT="${\bf H}$"></SPAN>, representing
the global information of the elliptical shape of the contour line of the
quadratic function, as well as the local information. 

<P>
Moreover, in the gradient descent method, we still need to determine 
a proper step size <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img253.svg"
 ALT="$\delta$"></SPAN>. If <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img253.svg"
 ALT="$\delta$"></SPAN> is too small, the iteration
may converge very slowly, especially when <SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img60.svg"
 ALT="$f(x)$"></SPAN> reduces slowly toward 
its minimum. On the other hand, if <SPAN CLASS="MATH"><IMG STYLE="height: 1.86ex; vertical-align: -0.12ex; " SRC="img253.svg"
 ALT="$\delta$"></SPAN> is too large but <SPAN CLASS="MATH"><IMG STYLE="height: 2.55ex; vertical-align: -0.70ex; " SRC="img60.svg"
 ALT="$f(x)$"></SPAN> has
some rapid variations in the local region, the minimum of the function 
may be skipped and the iteration may not converge. We will consider how
to find the optimal step size in the next section.

<P>

<DIV CLASS="navigation"><HR>
<!--Navigation Panel-->
<A
 HREF="node8.html">
<IMG WIDTH="37" HEIGHT="24" ALT="next" SRC="next.png"></A> 
<A
 HREF="node2.html">
<IMG WIDTH="26" HEIGHT="24" ALT="up" SRC="up.png"></A> 
<A
 HREF="node6.html">
<IMG WIDTH="63" HEIGHT="24" ALT="previous" SRC="prev.png"></A>   
<BR>
<B> Next:</B> <A
 HREF="node8.html">Line minimization</A>
<B> Up:</B> <A
 HREF="node2.html">Unconstrained Optimization</A>
<B> Previous:</B> <A
 HREF="node6.html">Newton's method</A></DIV>
<!--End of Navigation Panel-->

</BODY>
</HTML>
