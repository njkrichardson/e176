%\documentstyle[12pt]{article}
\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{comment}
\usepackage{html,makeidx}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

%<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
%<script type="text/x-mathjax-config">
%   MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
%</script>

%http://cs229.stanford.edu/notes/cs229-notes1.pdf


\section*{Optimization}

The goal of mathematical optimization or mathematical programming
is to solve an optimization problem of a given real multivariate 
function $y=f({\bf x})=f(x_1,\cdots,x_N)$ by searching its domain, 
an N-dimensional space, to find the optimal point 
${\bf x}^*=[x_1^2,\cdots,x_N^*]^T\in\mathbb{R}^N$ at which the 
corresponding function value $f({\bf x}^*)$ is either maximized or
minimized. As maximizing $f({\bf x})$ is equivalent to minimizing 
$-f({\bf x})$, we can only consider minimization problems.

\begin{equation}
  {\bf x}^*=\argmin_{\bf x} f({\bf x}),\;\;\;\;\;\;\mbox{i.e.,}\;\;\;\;\;
  f({\bf x}^*)=\min_{\bf x} f({\bf x})
\end{equation}
If the independent variables in ${\bf x}$ are allowed to take 
any values inside the function domain $\mathbb{R}^N$, then the 
optimization problem is unconstrained, otherwise if ${\bf x}$ has
to be inside a subset of $\mathbb{R}^N$, the problem is constrained.

Typically the real function $f({\bf x})$ is an {\em objective} or 
{\em cost function} which is to be either minimized or maximized. 
For example, in machine learning, we need to carry out regression 
analysis and classification, and we may need to develop a model 
$y=f({\bf x},{\bf a})$ to fit a set of observed data points 
${\cal D}=\{ ({\bf x}_n,\,y_n),\;\;n=1,\cdots,N \}$, by estimating
the parameters in ${\bf a}=[a_1,\cdots,a_M]^T$ of the function.
Typically the function is assumed to be known, such as the linear
model $y=f(x)=ax+b$, containing two model parameters $a$ (slope)
and $b$ (intercept), to be estimated based on the observed data.

Different methods can be used to solve this parameter estimation
problem. If the least square method is used, the objective function 
can be the squared error, the difference between the model and the 
given data, which is to be minimized:
\begin{equation}
  o({\bf a})=\sum_{n=1}^N [y_n-f(({\bf x}_n,{\bf a}))]^2
\end{equation}
Alternatively, if the Bayesian method is used, the objective 
functioin can be the likelihood function of the parameters in
${\bf a}$, which is to be maximized:
\begin{equation}
  o({\bf a})=L({\bf a}|{\cal D}) =p({\cal D}|{\bf a})
  =\prod_{n=1}^N p(y_n|{\bf x}_n,{\bf a})
\end{equation}
Solving suth optimization problems we get the optimal model 
parameters in ${\bf a}^*$.

If an optimization problem is unconstrained, and the analytical 
expression of the objective function is explicitly given, then the 
solution ${\bf x}^*$ at which the objective function $f({\bf x})$ 
is minimized may be found by solving the following equation system 
obtained by setting the gradient of the $f({\bf x})$ to be zero:
\begin{equation}
  {\bf g}_f=\bigtriangledown_{\bf x} f({\bf x})
  =\frac{d f({\bf x})}{d{\bf x}} 
  =\left[\frac{\partial f({\bf x})}{\partial x_1},\cdots,
    \frac{\partial f({\bf x})}{\partial x_N}\right]^T ={\bf 0}
\end{equation}
where ${\bf g}_f=\bigtriangledown_{\bf x} f({\bf x})=d\,f({\bf x})/d{\bf x}$ 
denotes the gradient vector of a scalar function $f({\bf x})$ with
respect to its vector argument ${\bf x}=[{\bf x}_1,\cdots,{\bf x}_N]^T$.
The root ${\bf x}^*$ of the equation ${\bf g}_f={\bf 0}$ is called a 
{\em critical, stationary, or stable point} of $f({\bf x})$.

Whether the function value $f({\bf x}^*)$ at this stationary point 
is a maximum, minimum, or saddle point, depending on its Hessian 
matrix for its second order derivatives:
\begin{equation}
  {\bf H}_f=\left[ \begin{array}{ccc}
      \frac{\partial^2 f({\bf x})}{\partial x_1^2} & \cdots & 
      \frac{\partial^2 f({\bf x})}{\partial x_1\partial x_N} \\
      \vdots & \ddots & \vdots \\
      \frac{\partial^2 f({\bf x})}{\partial x_N\partial x_1} & \cdots & 
      \frac{\partial^2 f({\bf x})}{\partial x_N^2}
    \end{array} \right]_{{\bf x}={\bf x}^*}
  \nonumber
\end{equation}

\begin{itemize}
\item If ${\bf H}_f>0$ is positive definite (all eigenvalues are positive),
  $f({\bf x}^*)$ is a minimum;
\item If ${\bf H}_f<0$ is negative definite (all eigenvalues are negative), 
  $f({\bf x}^*)$ is a maximum;
\item If ${\bf H}_f$ is indefinite (with both positive and negative 
  eigenvalues), $f({\bf x}^*)$ is a saddle point (maximum in some directions,
  but minimum in others). 
\end{itemize}

For  example, the gradient vectors and Hessian matrices of three 
functions $f_1(x,y)=x^2+y^2$, $f_2(x,y)=-x^2-y^2$, and $f_3(x,y)=x^2-y^2$:
\begin{equation}
  {\bf g}_{f_1}=\left[\begin{array}{c}2x\\2y\end{array}\right], 
  \;\;\;\;
  {\bf g}_{f_2}=\left[\begin{array}{c}-2x\\-2y\end{array}\right], 
  \;\;\;\;
  {\bf g}_{f_3}=\left[\begin{array}{r}2x\\-2y\end{array}\right]
  \nonumber
\end{equation}

\begin{equation}
  {\bf H}_{f_1}=\left[\begin{array}{rr}2 & 0\\0 & 2\end{array}\right]>0;
  \;\;\;\;
  {\bf H}_{f_2}=\left[\begin{array}{rr}-2 & 0\\0 & -2\end{array}\right]<0;
  \;\;\;\;
  {\bf H}_{f_3}=\left[\begin{array}{rr}2 & 0\\0 & -2\end{array}\right]
  \nonumber
\end{equation}
At the common stationary point ${\bf x}^*=[0,\,0]^T$ for all three 
functions at which $f_1(x,y)=f_2(x,y)=f_3(x,y)=0$, $f_1({\bf x}^*)$ is 
a minimum, $f_2({\bf x}^*)$ is a maximum, and $f_3({\bf x}^*)$ is a 
saddle point, i.e., $f_3({\bf x}^*)$ is a minimum in $x$ direction but 
a maximum in $y$ direction.

We show that the problems of minimization and equation solving can be
converted to each other.

\begin{itemize}
\item First, the problem of minimizing an objective function $f({\bf x})$
  is equivalent to solving the following equation system:
  \begin{equation} 
    {\bf g}_f({\bf x})=\bigtriangledown_{\bf x} f({\bf x})
    =\frac{d f({\bf x}) }{d{\bf x}} 
    =\left[\begin{array}{c}\partial f/\partial x_1\\\vdots\\
        \partial f/\partial x_N\end{array}\right] ={\bf 0} 
  \end{equation}
  If its root ${\bf x}^*$ is not a saddle point, it is the solution of the
  optimization problem that either maximizes or minimizes $f({\bf x})$. 

  This equation system can be solved by any of the methods discussed previously,
  such as the \htmladdnormallink{Newton-Raphson method}{../ch2/node6.html},
  which finds the root of a general equation ${\bf f}({\bf x})={\bf 0}$ by the 
  iteration ${\bf x}_{n+1}={\bf x}_n-{\bf J}_f^{-1}({\bf x}_n)\,{\bf f}({\bf x}_n)$. 
  Here, specifically, to solve the equation ${\bf g}_f({\bf x})={\bf 0}$, we
  first get the Jacobian ${\bf J}_g({\bf x})={\bf H}_f({\bf x})$ of the gradient
  ${\bf g}_f({\bf x})$ of $f({\bf x})$, which is the Hessian of $f({\bf x})$, 
  and then carry out the iteration below to eventually find ${\bf x}^*$ that 
  minimizes $f({\bf x})$:
  \begin{equation}
    {\bf x}_{n+1}={\bf x}_n-{\bf J}_g^{-1}({\bf x}_n)\,{\bf g}_f({\bf x}_n)
    ={\bf x}_n-{\bf H}_g^{-1}({\bf x}_n)\,{\bf g}_f({\bf x}_n)
  \end{equation}
  The second equality is due to the fact that the Jacobian of the gradient
  ${\bf g}_f$ of function ${\bf f}({\bf x})$ is the Hessian of the function,
  i.e., ${\bf J}_g={\bf H}_f$.


\item On the other hand, to solve the equation system 
  ${\bf g}({\bf x})=[g_1({\bf x}),\cdots,g_N({\bf x})]^T={\bf 0}$, 
  we can minimize the objective function defined as
  \begin{equation}
    f({\bf x})=\frac{1}{2}{\bf g}^T({\bf x}){\bf g}({\bf x})
    =\frac{1}{2}||{\bf g}({\bf x})||^2 
    =\frac{1}{2}\sum_{i=1}^N |g_i({\bf x})|^2
  \end{equation}
  To solve this minimization problem, we solve the equations:
  \begin{equation}
    \frac{\partial{f({\bf x})}}{\partial x_j}
    =\frac{\partial}{\partial x_j}\left(\frac{1}{2}\sum_{i=1}^N (g_i({\bf x}))^2\right)
    =\sum_{i=1}^N g_i({\bf x})\frac{\partial g_i({\bf x})}{\partial x_j}=0,\;\;\;\;\;\;
    (j=1,\cdots,N)
  \end{equation}
  Combining all $N$ of such equations we get the gradient vector of
  function $f({\bf x})={\bf g}^T{\bf g}$:
  \begin{equation}
    \bigtriangledown_{\bf x} f({\bf x})
    =\frac{d}{d{\bf x}}\left[\frac{1}{2}{\bf g}^T({\bf x}){\bf g}({\bf x}) \right]
    =\frac{d}{d{\bf x}}{\bf g}({\bf x})\;\;{\bf g}({\bf x})
    ={\bf J}_{\bf g} ({\bf x})\,{\bf g}({\bf x})={\bf 0} 
  \end{equation}
  where ${\bf J}_{\bf g} ({\bf x})$ is the Jicobian of ${\bf g}({\bf x})$.
  As in general ${\bf J}_{\bf g}({\bf x})$ has full rank, the homogeneous 
  equation above only has a zero solution ${\bf g}({\bf x})={\bf 0}$. 

\end{itemize}


\section*{Unconstrained Optimization}

\subsection*{Golden section search}

The golden section search algorithm can be used for finding a minimum (or
maximum) of a single-variable function $f(x)$. If it is known that the
function has a minimum between some two points, then we can search for it 
iteratively, in a manner similar to the bisection search for the root of 
an equation $f(x)=0$. Specifically, if in the neighborhood of the minimum
we can find three points $x_0<x_1<x_2$ corresponding to $f(x_0)>f(x_1)<f(x_2)$,
then there exists a minimum between $x_0$ and $x_2$. To search for this
minimum, we can choose another point $x_3$ between $x_1$ and $x_2$ as shown
in the figure below, with the following two possible outcomes:
\begin{itemize}
\item If $f(x_3)=f_{3a}>f(x_1)$, the minimum is inside the interval $x_3-x_0=a+c$ 
  associated with three new points $x_0<x_1<x_3$, i.e., $x_2$ is replaced by 
  $x_3$.
\item If $f(x_3)=f_{3b}<f(x_1)$, the minimum is inside the interval $x_2-x_1=b$
  associated with three new points $x_1<x_3<x_2$, i.e., $x_0$ is replaced by 
  $x_1$.
\end{itemize}
In either case, the new search interval $x_3-x_0=a+c$ or $x_2-x_1=b$ is 
smaller than the old one $x_2-x_0=a+b$, i.e., such an iteration is guaranteed
to converge. Of course $x_3$ can also be between $x_0$ and $x_1$, the 
iteration can be similarly carried out accordingly.

\htmladdimg{figures/GoldenRatio.png}

Similar to the bisection search for the root of $f(x)=0$, here to search
for the minimum of $f(x)$, we also want to avoid the worst possible case 
in which the minimum always happens to be in the larger one of the two 
sections $a+c$ and $b$. We therefore choose $x_3$ in such a way that the
two resulting search intervals will always be the same:
\begin{equation}
  a+c=b,\;\;\;\;\;\;\;\mbox{i.e.}\;\;\;\;\;\;\;c=b-a
\end{equation}
In other words, through out the iteration, the search interval should 
always be partitioned into two sections with the same ratio:
\begin{equation} 
  \frac{a+b}{b}=\frac{a+c}{a}=\frac{b}{a}
  \;\;\;\;\;\mbox{or}\;\;\;\;\;
  \frac{a+b}{b}=\frac{b}{b-c}=\frac{b}{a}
\end{equation}
In either case, we get
\begin{equation} 
  a^2+ab-b^2=0 
\end{equation}
Solving this quadratic equation for $a$ in terms of $b$ we get
\begin{equation}
  a_{1,2}=\frac{-1\pm\sqrt{5}}{2}\;b 
  =\left\{\begin{array}{r}0.618\,b\\-1.618\,b\end{array}\right.
\end{equation}
We keep the positive result and get
\begin{equation} 
 \frac{a}{b}=0.618,\;\;\;\;\;\;\;\;\;
 \frac{b}{a+b}=\frac{b}{0.618b+b}=\frac{1}{1.618}=0.618 
\end{equation}
This is the {\em golden ratio} between the two sections $a$ and $b$ of 
the search interval.

The iteration of the golden section search can terminate when the size 
of the search brackets $|x_2-x_0|$ is small enough, compared with some
tolerance, such as $10^{-9}$, pre-determined based on some prior knowledge 
of the scale of the problem. However, this scale may be unknown, or it may
vary drastically, and an absolute tolerance may be too large for a small 
scale problem but too small for a large scale problem. This problem can 
be resolved by using a relative tolerance with a scale comparable with 
that of the size of the problem:
\begin{equation} 
  |x_2-x_0| < \epsilon(|x_1|+|x_3|)
\end{equation}
where $\epsilon$ is a small value such as $10^{-9}$, and $|x_2-x_0|$
represents the size of the search interval and $|x_1|+|x_3|$ represents
the scale of the problem. When the inequality is satisfied, the iteration 
is terminated.

It is possible that the three initial points chosen are such that either 
$f(x_0)<f(x_1)<f(x_2)$ or $f(x_0)>f(x_1)>f(x_2)$, then we have to search 
toward the descending direction of $f(x)$ until it starts to ascend. 
For example, if $f(x_0)>f(x_1)>f(x_2)$, they can be iteratively updated 
by the following
\begin{equation} 
  x^{new}_0=x^{old}_1,\;\;\;\;x^{new}_1=x^{old}_2,
  \;\;\;\;x^{new}_2=x^{old}_2+c(x^{old}_2-x^{old}_1) 
\end{equation}
where $c>1$ (e.g., $c=1.618$), until we reach a point $x_2$ at which
$f(x_2)>f(x_1)$. Obviously the distance $d=x_2-x_1$ grows exponentially 
$d\propto c^n$. 

\subsection*{Parabolic Interpolation}

If a function $f(x)$ can be approximated by a parabola in the neighborhood 
of its minimum, then the vertex of the parabola can be used to approximate 
the minimum. Assuming we have available three points $a<b<c$ on the x-axis 
corresponding to function values $f(a)>f(b)<f(c)$, then a quadratic function 
$q(x)$ that goes through these points can be uniquely determined by the 
method of \htmladdnormallink{Lagrange interpolation}
{../ch7/node3.html}:
\begin{equation}
  q(x)=f(a)\frac{(x-b)(x-c)}{(a-b)(a-c)}+f(b)\frac{(x-c)(x-a)}{(b-c)(b-a)}
  +f(c)\frac{(x-a)(x-b)}{(c-a)(c-b)} 
\end{equation}
To find the minimum $q(x_{min})$ at the vertex of this quadratic function, 
we first set its derivative to zero:
\begin{equation} 
  q'(x)=f(a)\frac{(x-b)+(x-c)}{(a-b)(a-c)}+f(b)\frac{(x-c)+(x-a)}{(b-c)(b-a)}
  +f(c)\frac{(x-a)+(x-b)}{(c-a)(c-b)}=0 
\end{equation}
and multiply both sides by $(a-b)(b-c)(c-a)$:
\begin{eqnarray}
  &&f(a)(c-b)(2x-b-c)+f(b)(a-c)(2x-c-a)+f(c)(b-a)(2x-a-b)
  \nonumber \\
  &=&2x[f(a)(c-b)+f(b)(a-c)+f(c)(b-a)]
  -[f(a)(c^2-b^2)+f(b)(a^2-c^2)+f(c)(b^2-a^2)]=0 
\end{eqnarray}
and then solve the equation for $x$ to get:
\begin{eqnarray}
  x_{min}&=&\frac{1}{2}\frac{f(a)(c^2-b^2)+f(b)(a^2-c^2)+f(c)(b^2-a^2)}{f(a)(c-b)+f(b)(a-c)+f(c)(b-a)}
  \nonumber \\
  &=&b+\frac{1}{2}\frac{f(a)(c-b)(c+b-2b)+f(b)(a-c)(a+c-2b)+f(c)(b-a)(b+a-2b)}{f(a)(c-b)+f(b)(a-c)+f(c)(b-a)} 
  \nonumber \\
  &=&b+\frac{1}{2}\frac{f(a)(c-b)^2+f(b)(a-c)(a+c-2b)-f(c)(b-a)^2}{f(a)(c-b)+f(b)(a-c)+f(c)(b-a)} 
  \nonumber \\
  &=&b+\frac{1}{2}\frac{[f(a)-f(b)](c-b)^2-[f(c)-f(b)](b-a)^2}{[f(a)-f(b)](c-b)+[f(c)-f(b)](b-a)} 
  \label{vertex}
\end{eqnarray}
Unless $b$ is already at the vertex of $q(x)$, $f(x_{min})$ is smaller than 
$f(b)$, i.e., $x_{min}$ is closer to the minimum of $f(x)$ than $b$. This 
interpolation process can then be repeated using a set of three new points 
of $\{a, x_{min}, b\}$ if $x_{min}<b$ or $\{b, x_{min}, c\}$ if $x_{min}>b$.

In the unlikely case where the numerator of the second term in 
Eq. (\ref{vertex}) is zero, i.e., $[f(a)-f(b)](c-b)^2=[f(c)-f(b)](b-a)^2$,
we get $x_{min}=b$ and the iteration cannot proceed. We could choose some 
different point nearby to proceed.

\htmladdimg{figures/ParabolicInterpolate.png}
\htmladdimg{figures/ParabolicInterpolate1.png}


%\subsection*{Brent's method}

\subsection*{Nelder-Mead method}

%https://www.math.uni-bielefeld.de/documenta/vol-ismp/42_wright-margaret.pdf
%https://www.mathworks.com/help/optim/ug/fminsearch-algorithm.html

{\em There are occasions where it has been spectacularly good ...
Mathematicians hate it because you canâ€™t prove convergence; engineers
seem to love it because it often works.} ---- John Nelder


The {\em Nelder-Mead (NM) method} (also called {\em downhill simplex 
method} is a heuristic (search method for minimizing an objective 
function $f({\bf x}$ given in an N-dimensional space. The key 
concept of the mehtod is the \htmladdnormallink{\em simplex}
{https://en.wikipedia.org/wiki/Simplex}, an $N$ dimensional 
\htmladdnormallink{polytope}{https://en.wikipedia.org/wiki/Polytope}
that is a \htmladdnormallink{convex hull}
{https://en.wikipedia.org/wiki/Convex_hull} of a set of $N+1$ 
linearly independent points ${\bf x}_0,\cdots,{\bf x}_N$. These
points are linearly independent in the sense that the $N$ vectors
${\bf x}_i={\bf x}_0\\;\;(i=1,\cdots,N)$ are linearliy independent
(no three points are colinear):
\begin{equation}
  S=\left\{\sum_{i=0}^Nc_i{\bf x}_i \;\bigg|\;\sum_{i=0}^Nc_i=1,\;\;\;\;c_i\ge 0\right\}
\end{equation}
Intuitively, a simplex is a volumn in the N-D space enclosed by 
flat surfaces and spanned by $N+1$ verticies. For example, a 
simplex is a line sigment spanned by 2 points in 1-D space, a 
triangle spanned by 3 points in 2-D space, and a tetrahedron
spanned by 4 points in 3-D space.

The NM method is an iterative process which transforms an 
initial simplex iteratively into a different one following a
path in the space along which the function values are progressively 
reduced. The initial simplex can be generated based on a single
point ${\bf x}_0$, an initial guess, by adding a constant along
each of the $N$ dimensions spanned by the standard basis
$\{{\bf e}_1,\cdots,{\bf e}_N\}$:
\begin{equation}
  {\bf x}_i={\bf x}_0+c{\bf e}_i
\end{equation}
In the subsequent iterations, the initial simplex is transformed 
based on four parameters:
\begin{itemize}
\item {\em reflection coefficient:} $\alpha>0$, e.g., $\alpha=1$  
\item {\em expansion coefficient:} $0<\beta<1$, e.g., $\beta=1/2$  
\item {\em contraction coefficient:} $\gamma>1$, e.g., $\gamma=2$  
\item {\em shrink coefficient:} $0<\delta<1$, e.g., $\delta=1/2$  
\end{itemize}

Specially here are the steps in each iteration:
\begin{itemize}
\item {\em Order} all $N+1$ vertex points according to their 
  function values:
  \begin{equation}
    f({\bf x}_0)\le f({\bf x}_1)\le\cdots\le f({\bf x}_N)
  \end{equation}
  Denote the best point with lowest function value by ${\bf x}_0={\bf x}_l$,
  the worst point with the highest function value by ${\bf x}_N={\bf x}_h$,
  and the second worst point by ${\bf x}_{N-1}={\bf x}_s$.
\item Find the centroid of all points except the worst one 
  ${\bf x}_N={\bf x}_h$:
  \begin{equation}
    {\bf c}=\frac{1}{N}\sum_{i=1}^N{\bf x}_i
  \end{equation}
\item Find a new {\em reflection} point:
  \begin{equation}
    {\bf x}_r={\bf c}+\alpha({\bf c}-{\bf x}_h)
  \end{equation}
  Depending on the function value $f({\bf x}_r)$, we will do one of
  the following:
  \begin{itemize}
  \item If $f({\bf x}_s<f({\bf x}_r)<f({\bf x}_l)$, then replace 
    ${\bf x}_h$ by ${\bf x}_r$.
  \item If $f({\bf x}_r)<f({\bf x}_l)$, get an {\em expansion} point
    \begin{equation}
      {\bf x}_e={\bf c}+\gamma({\bf x}_r-{\bf c})
    \end{equation}
    Replace ${\bf x}_h$ by the better (with lower function value) of
    ${\bf x}_r$ and ${\bf x}_e$. This is called {\em greedy optimization}.
  \item $f({\bf x}_r)>f({\bf x}_s)$, i.e., ${bf x}_r-{\bf c}$ is a 
    wrong direction, get a {\em contraction} point
    \begin{equation}
      {\bf x}_c={\bf c}+\beta({\bf x}_h-{\bf c})
    \end{equation}
    If $f({\bf x}_c)<f({\bf x}_h)$ is better than ${\bf x}_h$, replace
    ${\bf x}_h$ by ${\bf x}_c$
  \item 

  \item Terminate the iteration when the variance of the $N+1$ 
    function values at the vertices of the current simplex is smaller
    than a tolerance. The best point ${\bf x}_0$ with lowest function
    value is returned as the solution.
  \end{itemize}
\end{itemize}


\subsection*{Newton's method}

Newton's method is based on the Taylor series expansion of the function
$f(x)$ to be minimized near some point $x_0$:
\begin{eqnarray}
  f(x)&=&f(x_0)+f'(x_0)(x-x_0)+\frac{1}{2}f''(x_0)(x-x_0)^2 + \cdots
  +\frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n + \cdots
  \nonumber\\
  &\approx& f(x_0)+f'(x_0)(x-x_0)+\frac{1}{2}f''(x_0)(x-x_0)^2 =q(x)
\end{eqnarray}
If $f(x)=q(t)$ is a quadratic function, then the Taylor series only
contains the first three terms (constant, linear, and quadratic terms).

\htmladdimg{figures/NewtonEx1.png}

In this case, we can find the vertex point at which $f(x)=q(x)$ reaches
its extremum, by first setting its derivative to zero:
\begin{eqnarray}
  q'(x)&=&\frac{d}{dx}q(x)=\frac{d}{dx}\left[ f(x_0)+f'(x_0)(x-x_0)
    +\frac{1}{2}f''(x_0)(x-x_0)^2 \right]
  \nonumber\\
  &=&f'(x_0)+f''(x_0)(x-x_0)=0
\end{eqnarray}
and then solving the resulting equation for $x$ to get:
\begin{equation} 
  x^*=x=x_0-\frac{f'(x_0)}{f''(x_0)}=x_0+\Delta x_0
  \label{OptimalNewton}
\end{equation}
where $\Delta x_0=-f'(x_0)/f''(x_0)$ is the step we need to take to go
from any initial point $x_0$ to the solution $x^*$ in a single step. 
Note that $f(x^*)$ is a minimum if $f''(x^*)>0$ but a maximum if 
$f''(x^*)<0$.

If $f(x)\ne q(x)$ is not quadratic, the result above can still be 
considered as an approximation of the solution, which can be improved 
iteratively from an initial guess $x_0$ to eventually approach the 
solution:
\begin{equation}
  x_{n+1}=x_n+\Delta x_n=x_n-\frac{f'(x_n)}{f''(x_n)}
\end{equation}
We see that in each step $x_n$ of the iteration, the function $f(x)$ is
fitted by a quadratic functions $q(x_n)$ and its vertex at $x_{n+1}$ is 
used as the updated approximated solution, at which the function is again
fitted by another quadratic function $q(x_{n+1})$ for the next iteration. 
Through this process the solution can be approached as $n\rightarrow\infty$.

We note that the iteration $x_{n+1}=x_n-f'(x_n)/f''(x_n)$ above is just 
the \htmladdnormallink{Newton-Raphson method}{../ch2/node5.html} for
solving an equation $f(x)=0$ but now applied to equation $f'(x)=0$, as
a necessary condition for an extremum of $f(x)$.

This quadratic function
\begin{equation}
  q(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{1}{2}f''(x_0)(x-x_0)^2
  \label{qofx}
\end{equation}
can also be expressed as
\begin{equation}
  q(x)=\frac{a}{2}(x-b)^2+c
  \label{qofxnew}
\end{equation}
in terms of the following three parameters:
\begin{equation}
\left\{\begin{array}{l}a=f''(x_0)\\
b=x_0-f'(x_0)/f''(x_0)\\c=f(x_0)-f'^2(x_0)/2f''(x_0)
\end{array}\right.
\end{equation}
(Substituting $a$, $b$ and $c$ into Eq. (\ref{qofxnew}) one gets Eq. (\ref{qofx}).)
Now it becomes obvious that when $x=b$, the function $q(x)=q(b)=c$ reaches its 
minimum at the point $x=b$. Note that the expression of $b$ above is actually the
same as Eq. (\ref{OptimalNewton}).

{\bf Example}

The function to be minimized and its first and second order derivatives are
given below:
\begin{equation}
  f(x)=2x^3-4x^2+x,\;\;\;\;\;f'(x)=6x^2-8x+1,\;\;\;\;\;\;\;f''(x)=12x-8 
  \nonumber
\end{equation}
Solving $f'(x)=0$ we get the optimal solution $x^*=1.333$, at which 
$f(x^*)=-1.037$. But here we will use the Newton's method to obtain the 
solution iteratively based on an initial guess $x_0=3$, at which we have
\begin{equation}
  f(x_0)=f(3)=21,\;\;\;\;\;\;\;f'(x_0)=f'(3)=31,\;\;\;\;\;\;f''(x_0)=f''(3)=28
  \nonumber
\end{equation}
Here $f''(x_0)>0$ indicates that the function $x(x)$ has a minimum in the
neighborhood of $x_0=3$. We use the parabola below to approximate $f(x)$:
\begin{equation}
  q(x)=\frac{a}{2}(x-b)^2+c
  \nonumber
\end{equation}
based on these parameters:
\begin{equation}
  \left\{\begin{array}{l}
  a=f''(3)=28\\
  b=3-f'(3)/f''(3)=1.8929\\
  c=f(3)-f'^2(3)/2f''(3)=3.8393\end{array}\right.
  \nonumber
\end{equation}
At the vertex of $q(x)$ at $x_1=b=1.8929$, we have $f(x_1)=f(1.8929)=1.1251$,
much smaller than $f(x_0)=f(3)=21$. The estimated solution 
$x=b=x_0-f'(x_0)/f''(x_0)$ is used as an iterative step 
$x_1=x_0-f'(x_0)/f''(x_0)$ for further improving the estimate.

We can further generalize Newton's method for single-variable function 
$f(x)$ to mult-variable function $f({\bf x})=f(x_1,\cdots,x_N)$, which 
can be approximated by a quadratic function $q({\bf x})$ containing the 
first three terms of its Taylor series at some point ${\bf x}_0$:
\begin{equation} 
  f({\bf x})\approx  f({\bf x}_0)+{\bf g}_0^T({\bf x}-{\bf x}_0)
  +\frac{1}{2}({\bf x}-{\bf x}_0)^T{\bf H}_0\,({\bf x}-{\bf x}_0)
  =q({\bf x})
\end{equation}
where ${\bf g}_0$ and ${\bf H}_0$ are respectively the gradient vector and 
Hessian matrix of the function $f({\bf x})$ at ${\bf x}_0$:
\begin{eqnarray}
  {\bf g}_0&=&{\bf g}_f({\bf x}_0)=\frac{d}{d{\bf x}} f({\bf x}_0)
  =\left[\begin{array}{c}\frac{\partial f({\bf x}_0)}{\partial x_1}\\
      \vdots \\ \frac{\partial f({\bf x}_0)}{\partial x_N}\end{array}\right],
  \nonumber\\
  {\bf H}_0&=&{\bf H}_f({\bf x}_0)=\frac{d}{d{\bf x}} {\bf g}({\bf x}_0)
  =\frac{d^2}{d{\bf x}^2} f({\bf x}_0)
  =\left[\begin{array}{ccc} \frac{\partial^2 f({\bf x}_0)}{\partial x_1^2} & 
      \cdots & \frac{\partial^2 f({\bf x}_0)}{\partial x_1\partial x_N} \\
      \vdots & \ddots & \vdots \\
      \frac{\partial^2 f({\bf x}_0)}{\partial x_N\partial x_1} & 
      \cdots & \frac{\partial^2 f({\bf x}_0)}{\partial x_N^2}
    \end{array}\right]
\end{eqnarray}
The stationary point of $f({\bf x})=q({\bf x})$ can be found from any point
${\bf x}_0$ by setting its derivative to zero
\begin{equation}
  \frac{d}{d{\bf x}}\left[f({\bf x}_0)+{\bf g}_0^T({\bf x}-{\bf x}_0)
    +\frac{1}{2}({\bf x}-{\bf x}_0)^T{\bf H}_0\,({\bf x}-{\bf x}_0)\right]
  ={\bf g}_0+{\bf H}_0\,({\bf x}-{\bf x}_0)={\bf 0}
\end{equation}
and solving the resulting equation to get
\begin{equation}
  {\bf x}^*={\bf x}={\bf x}_0-{\bf H}({\bf x}_0)^{-1}{\bf g}({\bf x}_0)
\end{equation}
Similar to the 1-D case where whether $f({\bf x}^*)$ is a minimum or 
maximum depends on whether the second order derivative $f''({\bf x}^*)$ 
is greater or smaller than zero, here whether $f({\bf x}^*)$ is a minimum,
maximum, or neither, depends on the second order derivatives, the Hessiam 
matrix ${\bf H}^*={\bf H}({\bf x}^*)$:
\begin{itemize}
\item If ${\bf H}^*>0$ is positive definite (all eigenvalues are positive),
  $f({\bf x}^*)$ is a local minimum;
  
\item If ${\bf H}^*<0$ is negative definite (all eigenvalues are negative),
  $f({\bf x}^*)$ a local maximum;
  
\item If ${\bf H}^*$ is indefinite (has both positive and negative 
  eigenvalues), $f({\bf x}^*)$ is a saddle point (neither minimum nor 
  maximum).  
\end{itemize}

If $f({\bf x})\ne q({\bf x})$ is not quadratic, the result above is not
a minimum or maximum, but it can still be used as an approximation of the
solution, which can be improved to approached the true solution iteratively:
\begin{equation} 
  {\bf x}_{n+1}={\bf x}_n+\Delta{\bf x}_n
  ={\bf x}_n-{\bf H}_n^{-1}{\bf g}_n ={\bf x}_n+{\bf d}_n
\end{equation}
where ${\bf g}_n={\bf g}({\bf x}_n)$, ${\bf H}_n={\bf H}({\bf x}_n)$, and 
${\bf d}_n=\Delta{\bf x}_n=-{\bf H}_n^{-1}{\bf g}_n$ is the increment in 
the nth step, called {\em Newton search direction}.

We note that the iteration above is just a generalization of 
$x_{n+1}=x_n-f'(x_n)/f''(x_n)$ in 1-D case with ${\bf g}_n=f'(x_n)$ 
and ${\bf H}_n=f''(x_n)$. Also, the iteration 
${\bf x}_{n+1}={\bf x}_n-{\bf H}_n^{-1}\,{\bf g}_n$ above is just the
\htmladdnormallink{Newton-Raphson method}{../ch2/node6.html} for 
solving equation ${\bf f}({\bf x})={\bf 0}$ but here applied to
solving ${\bf g}_f({\bf x})={\bf 0}$ as a necessary condition for
an extremum of ${\bf f}$.

The speed of convergence of the iteration can be controlled by a parameter 
$\delta>0$ that controls the step size:
\begin{equation} 
  {\bf x}_{n+1}={\bf x}_n+\delta_n \; \Delta{\bf x}_n
  ={\bf x}_n-\delta_n \; {\bf H}_n^{-1}{\bf g}_n 
\end{equation}

This quadratic function can also be expressed as (show it as 
\htmladdnormallink{homework}{../sub1/sub1.html}):
\begin{equation}
  q({\bf x})=\frac{1}{2}({\bf x}-{\bf b})^T{\bf A}({\bf x}-{\bf b})+c
\end{equation}
where
\begin{equation}
  \left\{\begin{array}{l}
           {\bf A}={\bf H}_0\\
           {\bf b}={\bf x}_0-{\bf H}^{-1}{\bf g}_0\\
           c=f({\bf x}_0)-{\bf g}_0^T{\bf H}_0^{-1}{\bf g_0}/2
\end{array}\right.
\end{equation}
and we have
\begin{equation}
  {\bf g}_0={\bf g}_f({\bf x}_0)={\bf A}({\bf x}_0-{\bf b})={\bf A}{\bf e}_0,
  \;\;\;\;\;\;\;
  {\bf H}_0={\bf H}_f({\bf x}_0)={\bf A}
\end{equation}
where ${\bf e}_0={\bf x}_0-{\bf b}$ is the error, and $||{\bf e}_0||$ is the
distance from ${\bf x}_0$ to the vertex ${\bf x}={\bf b}$ of the quadratic 
function, at which it may take the extremum $q({\bf b})=c$ (if ${\bf H}$ is
either positive or negative definite).



{\bf Example 0:} Consider the following quadratic function:
\begin{equation}
  q(x,y)=\frac{1}{2} [x,\;y]\left[\begin{array}{cc}a&b/2\\b/2&c\end{array}\right]
  \left[\begin{array}{c}x\\y\end{array}\right]=\frac{1}{2}(ax^2+bxy+cy^2)
  \nonumber
\end{equation}
We have
\begin{equation}
  {\bf g}=\left[\begin{array}{c}ax+by/2\\bx/2+cy\end{array}\right],
  \;\;\;\;\;\;\;\;\;\;\;\;\;\;
  {\bf H}=\left[\begin{array}{cc}a&b/2\\b/2&c\end{array}\right],
  \;\;\;\;\;\;\det{\bf H}=ac-b^2/4
  \nonumber
\end{equation}
We let $a=1$, $c=2$, and consider the following values of $b$:
\begin{itemize}
\item $b=-2$, $\lambda_1=2.618$, $\lambda_2=0.382$, $\det{\bf H}=\lambda_1\lambda_2=1$, 
  ${\bf H}>0$ is positive definite, $f(0,\,0)=0$ is the minimum.
\item $b=0$, $\lambda_1=2$, $\lambda_2=1$, $\det{\bf H}=\lambda_1\lambda_2=2$, 
  ${\bf H}>0$ is positive definite, $f(0,\,0)=0$ is the minimum.
\item $b=2$, $\lambda_1=2.618$, $\lambda_2=0.382$, $\det{\bf H}=\lambda_1\lambda_2=1$, 
  ${\bf H}>0$ is positive definite, $f(0,\,0)=0$ is the minimum.
\item $b=4$, $\lambda_1=3.562$, $\lambda_2=-0.562$, $\det{\bf H}=\lambda_1\lambda_2=-2$, 
  ${\bf H}$ is indefinite, $f(0,\,0)=0$ is a saddle point (minimum in one direction
  but maximum in another).
\end{itemize}

\htmladdimg{figures/QuadraticSurf.png}


We can speed up the convergence by a bigger step size $\delta>1$. However, 
if $\delta$ is too big, the solution may be skipped and the iteration may 
not converge if it gets into an oscillation around the solution. Even worse,
the iteration may become divergent. For such reasons, a smaller step size 
$\delta<1$ may be preferred sometimes.

In summary, Newton's method approximates the function $f({\bf x})$ at an
estimated solution ${\bf x}_n$ by a quadratic equation (the first three 
terms of the Taylor's series) based on the gradient ${\bf g}_n$ and Hessian 
${\bf H}$ of the function at ${\bf x}_n$, and treat the vertex of the 
quadratic equation as the updated estimate ${\bf x}_{n+1}$.

Newton's method requires the Hessian matrix as well as the gradient to be 
available. Moreover, it is necessary calculate the inverse of the Hessian 
matrix in each iteration, which may be computationally expensive. 


{\bf Example 1} 

The Newton's method is applied to solving the following non-linear equation 
system of $N=3$ variables:
\begin{equation}
  \left\{\begin{array}{l}
  f_1(x_1,\,x_2,\,x_3)=3x_1-(x_2x_3)^2-3/2\\
  f_2(x_1,\,x_2,\,x_3)=4x_1^2-625\,x_2^2+2x_2-1\\
  f_3(x_1,\,x_2,\,x_3)=exp(-x_1x_2)+20x_3+9\end{array}\right.
  \nonumber
\end{equation}
with the exact solution $(x_1=0.5,\;x_2=0,\;x_3=-0.5)$. These equations can 
be expressed in vector form as ${\bf f}({\bf x})={\bf 0}$ and solved as an 
optimization problem with the objective function 
$o({\bf x})={\bf f}^T({\bf x}){\bf f}({\bf x})$. The iteration from an 
initial guess ${\bf x}_0={\bf 0}$ is shown below. 
\begin{equation}
\begin{array}{c||c|c}\hline 
n & {\bf x}=[x_1,\,x_2,\,x_3]  & ||{\bf f}({\bf x})|| \\\hline\hline
0 &	0.000000,\;\; 0.000000,\;\;  0.000000 &	1.016120e+01  \\\hline
1 &	0.500000,\;\; 0.500000,\;\; -0.500000 &	1.552502e+02  \\\hline
2 &	0.499550,\;\; 0.250800,\;\; -0.493801 &	3.881300e+01  \\\hline
3 &	0.500096,\;\; 0.126206,\;\; -0.496852 &	9.702208e+00  \\\hline
4 &	0.500025,\;\; 0.063914,\;\; -0.498405 &	2.425198e+00  \\\hline
5 &	0.500010,\;\; 0.032778,\;\; -0.499181 &	6.059054e-01  \\\hline
6 &	0.500005,\;\; 0.017231,\;\; -0.499570 &	1.510777e-01  \\\hline
7 &	0.500003,\;\; 0.009498,\;\; -0.499763 &	3.737330e-02  \\\hline
8 &	0.500002,\;\; 0.005712,\;\; -0.499857 &	8.959365e-03  \\\hline
9 &	0.500001,\;\; 0.003968,\;\; -0.499901 &	1.900145e-03  \\\hline
10 &	0.500001,\;\; 0.003326,\;\; -0.499917 &	2.577603e-04  \\\hline
11 &	0.500001,\;\; 0.003206,\;\; -0.499920 &	8.932714e-06  \\\hline
12 &	0.500001,\;\; 0.003202,\;\; -0.499920 &	1.238536e-08  \\\hline
13 &	0.500001,\;\; 0.003202,\;\; -0.499920 &	2.371437e-14  \\\hline
\end{array}
\end{equation}
We see that after 13 iterations the algorithm converges to 
the following approximated solution with accuracy of 
$o({\bf x})=||{\bf f}({\bf x}^*)||^2\approx 10^{-28}$:
\begin{equation}
  {\bf x}^*=\left[\begin{array}{r}0.5000008539707297\\0.0032017070323056\\
      -0.4999200212218281\end{array}\right]
\end{equation}


\subsection*{Gradient Descent Method}

Newton's method discussed above is based on the Hessian ${\bf H}_f({\bf x})$
of the function $f({\bf x})$ to be minimized as well as its gradient 
${\bf g}_f({\bf x})$. The method is not applicable if the Hessian 
${\bf H}_f$ is not available, or the cost of computing the inverse 
${\bf H}^{-1}_f$ is too high. In such a case, the gradient descent method
can be used without using the Hessian matrix.

We first consider the minimization of a single-variable function 
$f(x)$. From any inital point $x_0$, we can move to a nearby point
\begin{equation}
  x_1=x_0+\Delta x_0=x_0-\delta f'(x_0),\;\;\;\;\;
  \mbox{where}\;\;\;\;\Delta x_0=-\delta f'(x_0)
\end{equation} 
No matter whether $f'(x_0)$ is positive or negative, the function value 
$f(x_1)$ (approximated by the first two terms of its Taylor series) is 
always reduced if the positive step size $delta >0$ is small enough:
\begin{equation}
  f(x_1)\approx f(x_0)+f'(x_0)\Delta x_0=f(x_0)-|f'(x_0)|^2\delta<f(x_0)
\end{equation}
This process can be carried out iteratively 
\begin{equation}
  x_{n+1}=x_n-\delta_n \; f'(x_n) 
\end{equation}
until eventually reaching a point $x^*$ at which $f'(x^*)=0$ and no 
further progress can be made, i.e. a local minimum of the function is 
obtained.

This simple method can be generalized to minimize a multi-variable 
objective function $f({\bf x})=f(x_1,\cdots,x_N)$ in N-D space. The 
derivative of the 1-D case is generalized to the gradient vector 
${\bf g}({\bf x})=df({\bf x})/d{\bf x}$ of function $f({\bf x})$,
which is in the direction along which the function increases most rapidly 
with the steepest slope, perpendicular to the contour or iso-lines of the
function $f({\bf x})$. The fastest way to reduce $f({\bf x})$ is to go 
down hill along the opposite direction of the gradient vector. 

Specifically the gradient descent method (also called steepest 
descent or down hill method) carries out the following approximation
${\bf x}_{n+1}={\bf x}_n+\Delta{\bf x}_n$ (the first two terms of the 
Taylor series) with $\Delta{\bf x}=-\delta{\bf g}\;(\delta>0)$:
\begin{equation}
  f({\bf x}_1)=f({\bf x}_0+\Delta{\bf x})
  \approx f({\bf x}_0)+{\bf g}^T_0 \Delta{\bf x}
  =f({\bf x}_0)-\delta  {\bf g}^T_0 {\bf g}_0
  =f({\bf x}_0)-\delta  ||{\bf g}||^2 < f({\bf x}_0)
\end{equation}
iteratively:
\begin{equation} 
  {\bf x}_{n+1}={\bf x}_n-\delta_n \, {\bf g}_n
  =({\bf x}_{n-1}-\delta_{n-1}\,{\bf g}_{n-1})-\delta_n \, {\bf g}_n=\cdots=
  {\bf x}_0-\sum_{i=0}^n \delta_n\,{\bf g}_i 
\end{equation}
untill eventually reaching a point at which ${\bf g}={\bf 0}$ and the 
minimum of the function $f({\bf x})$ is reached.


Comparing the gradient descent method with Newton's method we see that
here the Hessian matrix is no longer used. The iteration simply follows 
a search direction ${\bf d}_n=-{\bf g}_n$, which is different from the 
search direction ${\bf d}_n=-{\bf H}^{-1}_n{\bf g}_n$ of Newton's method,
based on ${\bf H}_n$ as well as ${\bf g}_n$. Specially, when ${\bf H}={\bf I}$, 
the two methods become the same.

As the gradient descent method relies only on the gradient vector of the
objective function without any information contained in the second order 
derivatives in the Hessian matrix, it does not have as much information as
Newton's method and therefore may not be as efficient. For example, when 
the function is quadratic, as discussed before, Newton's method can find 
the solution in a single step from any initial guess, but it may take the 
gradient descent method many steps to reach the solution, because it always 
follows the negative direction of the local gradient, which typically does 
not point to the solution directly. However, for the same reason, the 
gradient descent method is computationally less expensive and will be 
effective when the Hessian matrix is not used.

{\bf Example:} Consider a two-variable quadratic function in the following
general form:
\begin{equation}
  f({\bf x})={\bf x}^T{\bf Ax}+{\bf b}^T{\bf x}+c
  =[x_1\;x_2]\left[\begin{array}{cc}a_{11} & a_{12}\\a_{21} & a_{22}\end{array}\right]
  \left[\begin{array}{c}x_1\\x_2\end{array}\right]
  +[b_1\;b_2]\left[\begin{array}{c}x_1\\x_2\end{array}\right]+c
  \nonumber
\end{equation}
where ${\bf A}$ is a symmetric positive semidefinite matrix,
i.e., $a_{12}=a_{21}$ and ${\bf A}\ge 0$. Specially, if we let 
\begin{equation}
  {\bf A}=\left[\begin{array}{cc}2 & 1\\1 & 1\end{array}\right],\;\;\;\;\;
  {\bf b}=\left[\begin{array}{c}0 \\0\end{array}\right],\;\;\;\;\;  c=0
  \nonumber
\end{equation} 
then we have the following function:
\begin{equation}
  f(x_1,x_2)=\frac{1}{2}[x_1\;x_2]\left[\begin{array}{cc}2&1\\1&1\end{array}\right]
  \left[\begin{array}{c}x_1\\x_2\end{array}\right]=\frac{1}{2}(2x_1^2+2x_1x_2+x_2^2)
  \nonumber
\end{equation}
which has a minimum $f(x_1,\,x_2)=0$ at $x=y=0$. 

\htmladdimg{figures/gradient1.png}

We assume the initial guess is ${\bf x}_0=[1,\;2]^T$, at which the
gradient is ${\bf g}_0=[4,\;3]^T$. Now we compare the gradient method
with Newton's method, in terms of the search direction ${\bf d}$ and 
progress of the iterations:
\begin{itemize}
\item Newton's method: Here we have
  \begin{equation}
    {\bf g}=\left[\begin{array}{c}2x_1+x_2\\x_1+x_2\end{array}\right],\;\;\;\;\;
    {\bf H}={\bf A}=\left[\begin{array}{cc}2&1\\1&1\end{array}\right],\;\;\;\;
    {\bf H}^{-1}=\left[\begin{array}{rr}1&-1\\-1&2\end{array}\right]
    \nonumber
  \end{equation}
  and the search direction is 
  ${\bf d}_0=-{\bf H}^{-1}{\bf g}_0=-[1,\;2]^T$ (the red arrow in the figure).
  The iteration is:
  \begin{equation}
    {\bf x}_1={\bf x}_0-{\bf H}^{-1}{\bf g}_0=\left[\begin{array}{rr}1\\
        2\end{array}\right]
    -\left[\begin{array}{rr}1&-1\\-1&2\end{array}\right]
    \left[\begin{array}{r}4\\3\end{array}\right]
    =\left[\begin{array}{r}0\\0\end{array}\right]
    \nonumber
  \end{equation}
  which is the minimum of the function.
\item The gradient descent method: the search direction is
  ${\bf d}_0=-{\bf g}_0=-[4,\;3]^T$, perpendicular to the contour of 
  the function. The first iteration is:
  \begin{equation}
    {\bf x}_1={\bf x}_0-\delta{\bf g}_0
    =\left[\begin{array}{r}1\\2\end{array}\right]
    -\delta\left[\begin{array}{r}4\\3\end{array}\right]
    =\left[\begin{array}{r}1-\delta 4\\2-\delta 3\end{array}\right]
    \nonumber
  \end{equation}
  We need to determine the step size $\delta$ (to be considered later)
  to find ${\bf x}_1$, and then continue the iteration.
\end{itemize}

The figure below compares the search directions of the gradient descent
method (blue) based only on the gradient vector ${\bf g}_0$, the local 
information at the point ${\bf x}_0$, and Newton's method (red) based on 
the Hessian matrix ${\bf H}$, the global of the quadratic function, as well
as the gradient ${\bf g}_0$.

\htmladdimg{figures/QuadraticContourEx2.png}


We see that Newton's method finds the solution in a single step, but
the gradient descent method requires an iteration, and therefore less
effective. This is because the gradient descent method only has available
the local information provided by the first order derivative ${\bf g}$, 
the gradient direction of the function at a single point, while Newton's
method can make use of the second order derivative ${\bf H}$, representing
the global information of the elliptical shape of the contour line of the
quadratic function, as well as the local information. 

Moreover, in the gradient descent method, we still need to determine 
a proper step size $\delta$. If $\delta$ is too small, the iteration
may converge very slowly, especially when $f(x)$ reduces slowly toward 
its minimum. On the other hand, if $\delta$ is too large but $f(x)$ has
some rapid variations in the local region, the minimum of the function 
may be skipped and the iteration may not converge. We will consider how
to find the optimal step size in the next section.

\subsection*{Line minimization}

In both Newton's method and the radient descent method, an iteration 
${\bf x}_{n+1}={\bf x}_n+\delta_n{\bf d}_n$ is carried out to gradually 
reduce the value of the objective function $f({\bf x})$. Here both the
search direction ${\bf d}_n$ and the step size $\delta$ need to be 
determined to maximally reduce the function value $f({\bf x}_{n+1})$.

First, we realize that to reduce $f({\bf x}_{n+1})$ the search 
direction ${\bf d}$ needs to point away from the gradient ${\bf g}_n$ 
along which $f({\bf x}_n)$ increases most rapidly, i.e., the angle 
$\theta$ between ${\bf d}_n$ and ${\bf g}_n$ should be greater than 
$\pi/2$:
\begin{equation}
  \theta=\cos^{-1}\frac{{\bf d}_n^T{\bf g}_n}{||{\bf d}_n||\;||{\bf g}_n||}
  >\frac{\pi}{2}
\;\;\;\;\;{i.e.,}\;\;\;\;\;  {\bf d}_n^T{\bf g}_n<0
\end{equation}
so that $f({\bf x})$ will decrease along ${\bf d}_n$. 
We see that this condition is indeed satisfied in both the gradient
descent method with ${\bf d}_n=-{\bf g}_n$ and Newton's method with
${\bf d}_n=-{\bf H}_n^{-1}{\bf g}_n$ (with ${\bf H}_n$ being positive 
definite):
\begin{equation}
  {\bf d}_n^T{\bf g}_n=-{\bf g}_n^T{\bf H}_n^{-1}{\bf g}_n <0,\;\;\;\;\;\;\;\;\;\;
  {\bf d}_n^T{\bf g}_n=-{\bf g}_n^T{\bf g}_n=-||{\bf g}_n||^2 <0
\end{equation}

%From an initial guess ${\bf x}_0$ we arrive at ${\bf x}_{n+1}$ after $n+1$ 
%such iterations:
%\begin{equation}
%  {\bf x}_{n+1}={\bf x}_n+\delta_n{\bf d}_n
%  =({\bf x}_{n-1}+\delta_{n-1}{\bf d}_{n-1})+\delta_n{\bf d}_n
%  =\dots={\bf x}_0+\sum_{i=0}^n\delta_i{\bf d}_i
%\end{equation}
%We define the error associated with the nth iteration to be the difference 
%between ${\bf x}_n$ and the solution ${\bf x}^*$:
%\begin{equation}
%  {\bf e}_n={\bf x}_n-{\bf x}^*
%\end{equation}
%and subtract ${\bf x}^*={\bf x}_{n+1}-{\bf e}_{n+1}={\bf x}_0-{\bf e}_0$ 
%from both sides of the equation above to get
%\begin{equation}
%  {\bf e}_{n+1}={\bf e}_n+\delta_n{\bf d}_n={\bf e}_0+\sum_{i=0}^n\delta_i{\bf d}_i
%\end{equation}

Second, the optimal step size $\delta_n$ needs to be determined so that 
the function velue at the next step $f({\bf x}_{n+1})=f({\bf x}_n+\delta_n{\bf d}_n)$ 
is minimized along the search direction ${\bf d}_n$. To find such a $\delta$,
we set to zero the derivative of the function with respect to $\delta_n$, the 
{\em directional derivative} along the direction of ${\bf d}_n$, and get 
(by chain rule):
\begin{equation}
  \frac{d}{d\delta_n} f({\bf x}_{n+1})
  =\frac{d}{d\delta_n}f({\bf x}_n+\delta_n{\bf d}_n)
  =\left(\frac{d\,f({\bf x}_{n+1})}{d{\bf x}}\right)^T\;
  \frac{d({\bf x}_n+\delta_n{\bf d}_n)}{d\delta_n}
  ={\bf g}^T_{n+1}{\bf d}_n=0
\end{equation}
This result indicates that the gradient ${\bf g}_{n+1}=f'({\bf x}_{n+1})$ 
at ${\bf x}_{n+1}$ needs to be perpendicular to the previous search 
direction ${\bf d}_n$. In other words, when traversing along the 
search direction ${\bf d}_n$, we should stop at the point 
${\bf x}_{n+1}={\bf x}_n+\delta{\bf d}_n$ at which the gradient 
${\bf g}_{n+1}$ has zero component along the direction ${\bf d}_n$, 
and the corresponding $\delta$ is the optimal step size.

Finding the optimal step size $\delta$ is a 1-D optimization problem, 
which can be solved by Newton's method. Specifically, we treat
$f({\bf x}_{n+1})=f({\bf x}_n+\delta{\bf d}_n)$ as a function of the signle
variable $\delta$, and approximate it by the first three terms of its 
Taylor series at $\delta=0$:
\begin{equation}
  f({\bf x}_{n+1})=f({\bf x}_n+\delta{\bf d}_n)\approx
  \left[f({\bf x}_n+\delta{\bf d}_n)\right]_{\delta=0}
  +\delta\;\left[\frac{d}{d\delta}f({\bf x}_n+\delta{\bf d}_n)\right]_{\delta=0}
  +\frac{\delta^2}{2}\;\left[\frac{d^2}{d\delta^2}f({\bf x}_n+\delta{\bf d}_n)\right]_{\delta=0}
  \label{OptimalStepTaylor}
\end{equation}
where
\begin{eqnarray}
  \left[f({\bf x}_n+\delta{\bf d}_n)\right]_{\delta=0}&=&f({\bf x}_n)  \\
  \left[\frac{d}{d\delta}f({\bf x}_n+\delta{\bf d}_n)\right]_{\delta=0}
  &=&{\bf g}({\bf x}_n+\delta{\bf d}_n)^T{\bf d}_n\bigg|_{\delta=0}
  ={\bf g}^T_n{\bf d}_n\\
  \left[\frac{d^2}{d\delta^2}f({\bf x}_n+\delta{\bf d}_n)\right]_{\delta=0}
  &=&\left[\frac{d}{d\delta}{\bf g}({\bf x}_n
  +\delta{\bf d}_n)^T\right]_{\delta=0}{\bf d}_n
  =\left[\frac{d}{d{\bf x}} {\bf g}({\bf x})\;
    \frac{d}{d\delta}({\bf x}_n+\delta{\bf d}_n)\right]^T_{\delta=0}
  {\bf d}_n
  \nonumber\\
  &=&({\bf H}_n{\bf d}_n)^T{\bf d}_n={\bf d}^T_n{\bf H}_n{\bf d}_n
\end{eqnarray}
where ${\bf H}_n={\bf H}_n^T$ is the Hessian matrix of $f({\bf x})$
at ${\bf x}_n$. Substituting these back into the Taylor series above
we get:
\begin{equation} 
  f({\bf x}_n+\delta{\bf d}_n)\approx 
  f({\bf x}_n)+\delta\,{\bf g}^T_n\,{\bf d}_n
  +\frac{\delta^2}{2}\,{\bf d}^T_n{\bf H}_n\,{\bf d}_n 
\end{equation}
To find $\delta$ that minimizes $f({\bf x}_n+\delta{\bf d}_n)$, we 
set its derivative with respect to $\delta$ to zero:
\begin{equation} 
  \frac{d}{d\delta}f({\bf x}_n+\delta{\bf d}_n)
  \approx\frac{d}{d\delta}\left(
  f({\bf x}_n)+\delta{\bf g}^T_n\,{\bf d}_n
  +\frac{\delta^2}{2}{\bf d}^T_n{\bf H}_n\,{\bf d}_n \right)
  ={\bf g}^T_n\,{\bf d}_n+\delta\,{\bf d}^T_n{\bf H}_n\,{\bf d}_n=0 
  \label{OptimalDelta1}
\end{equation}
and solve the resulting equation for $\delta$ to get the optimal 
step size based on both ${\bf g}_n$ and ${\bf H}_n$:
\begin{equation} 
  \delta_n=-\frac{{\bf g}^T_n{\bf d}_n}{{\bf d}^T_n{\bf H}_n{\bf d}_n} 
  \label{OptimalDelta}
\end{equation}

For example, consider the following two methods for minimization:

\begin{itemize}
\item {\bf Newton's method:} 

  The search direction is ${\bf d}_n=-{\bf H}_n^{-1}{\bf g}_n$, and the 
  optimal step size is
  \begin{equation} 
    \delta_n=-\frac{{\bf g}_n^T{\bf d}_n}{{\bf d}_n^T{\bf H}_n{\bf d}_n} 
    =\frac{{\bf g}^T_n({\bf H}_n^{-1}{\bf g}_n)}
    {({\bf H}_n^{-1}{\bf g}_n)^T{\bf H}_n({\bf H}^{-1}_n{\bf g}_n)}\;
    =\frac{{\bf g}^T_n({\bf H}_n^{-1}{\bf g}_n)}
    {({\bf H}_n^{-1}{\bf g}_n)^T{\bf g}_n}\;=1
  \end{equation}
  This is the iteration obtained before, which is indeed optimal:
  \begin{equation} 
    {\bf x}_{n+1}={\bf x}_n+\delta_n{\bf d}_n
    =x_n-\delta_n{\bf H}^{-1}_n{\bf g}_n={\bf x}_n-{\bf H}^{-1}_n{\bf g}_n
  \end{equation}

\item {\bf Gradient descent method:} 

  The search direction is ${\bf d}_n=-{\bf g}_n$, we have
  \begin{equation}
    {\bf g}^T_{n+1}{\bf d}_n=-{\bf g}^T_{n+1}{\bf g}_n=0,
    \;\;\;\;\;\mbox{i.e.,}\;\;\;\;\;\; {\bf g}_{n+1}\perp{\bf g}_n
  \end{equation}
  i.e., the search direction ${\bf d}_{n+1}=-{\bf g}_{n+1}$ is always
  perpendicular to the previous one ${\bf d}_n=-{\bf g}_n$, i.e., the
  iteration follows a zigzag path composed of a sequence of segments 
  from the initial guess to the final solution. The optimal step size 
  is
  \begin{equation} 
    \delta_n=-\frac{{\bf g}_n^T{\bf d}_n}{{\bf d}_n^T{\bf H}_n{\bf d}_n} 
    =\frac{{\bf g}_n^T{\bf g}_n}{{\bf g}_n^T{\bf H}_n{\bf g}_n} 
    =\frac{||{\bf g}_n||^2}{{\bf g}_n^T{\bf H}_n{\bf g}_n} 
    \label{GradientStepSizeOptimal}
  \end{equation}
\end{itemize}

However, as the Hessian ${\bf H}_n$ is not assumed to be available in 
the gradient descent method, the optimal step size above cannot actually 
be computed. We can instead approximate $f''(\delta)\big|_{\delta=0}$ in 
the third term of Eq. (\ref{OptimalStepTaylor}) at two nearby points 
at $\delta=0$ and $\delta=\sigma$, where $\sigma$ is a small value:
\begin{eqnarray}
  \left[\frac{d^2}{d\delta^2} f({\bf x}+\delta{\bf d})\right]_{\delta=0}
  &=&\left[\frac{d}{d\delta} f'({\bf x}+\delta{\bf d})\right]_{\delta=0}
%  =\lim\limitation_{\sigma\rightarrow 0}
  =\lim_{\sigma\rightarrow 0}
  \frac{f'({\bf x}+\sigma{\bf d})-f'({\bf x})}{\sigma}
  \nonumber\\
  &\approx &\frac{{\bf g}^T({\bf x}+\sigma{\bf d})\,{\bf d}-{\bf g}^T({\bf x})\,{\bf d}}
      {\sigma}
      =\frac{{\bf g}^T_\sigma{\bf d}-{\bf g}^T{\bf d}}{\sigma}
\end{eqnarray}
where ${\bf g}={\bf g}({\bf x})$ and ${\bf g}_\sigma={\bf g}({\bf x}+\sigma{\bf d})$.
This approximation can be used to replace ${\bf d}_n^T{\bf H}_n{\bf d}_n$ 
in Eq. (\ref{OptimalDelta1}) abve:
\begin{equation}
  \frac{d}{d\delta}f({\bf x}_n+\delta{\bf d}_n)
  \approx {\bf g}_n^T\,{\bf d}_n+\frac{\delta}{\sigma}
  \left({\bf g}_{\sigma n}^T{\bf d}_n-{\bf g}_n^T{\bf d}_n\right)=0
\end{equation}
Solving for $\delta$ we get the estimated optimal step size:
\begin{equation}
  \delta_n=-\frac{\sigma\,{\bf g}_n^T{\bf d}_n}
        {({\bf g}_{\sigma n}^T{\bf d}_n-{\bf g}_n^T{\bf d}_n)} 
        =-\frac{\sigma\,{\bf g}_n^T{\bf d}_n}{({\bf g}_{\sigma n}-{\bf g}_n)^T{\bf d}_n} 
\end{equation}
For example, in the gradient descent method, the search 
direction is ${\bf d}_n=-{\bf g}_n$, and the optimal step size
$\delta=||{\bf g}_n||^2/{\bf g}_n^T{\bf H}_n{\bf g}_n$ depends on
${\bf H}$, which is not assumed to be available. Therefore the 
secant method can be used to approximate the optimal step size:  
\begin{equation}
  \delta_n=-\frac{\sigma\,{\bf g}_n^T{\bf d}_n}{({\bf g}_{\sigma n}-{\bf g}_n)^T{\bf d}_n} 
  =-\frac{\sigma\,{\bf g}_n^T{\bf g}_n}{({\bf g}_{\sigma n}-{\bf g}_n)^T{\bf g}_n} 
  =\frac{\sigma\,||{\bf g}_n||^2}{||{\bf g}_n||^2-{\bf g}_{\sigma n}^T{\bf g}_n} 
  \label{GradientStepSizeSecant}
\end{equation}
The iteration becomes
\begin{equation}
  {\bf x}_{n+1}={\bf x}_n-\delta_n{\bf g}_n
  ={\bf x}_n+\frac{\sigma\,||{\bf g}_n||^2}{{\bf g}_{\sigma n}^T{\bf g}_n-||{\bf g}_n||^2}\,
  {\bf g}_n
\end{equation}

{\bf Example: } The gradient descent method applied to solve the same
three-variable equation system previously solved by Newton's method:
\begin{equation}
  \left\{\begin{array}{l}
  f_1(x_1,\,x_2,\,x_3)=3x_1-(x_2x_3)^2-3/2\\
  f_2(x_1,\,x_2,\,x_3)=4x_1^2-625\,x_2^2+2x_2-1\\
  f_3(x_1,\,x_2,\,x_3)=exp(-x_1x_2)+20x_3+9\end{array}\right.
  \nonumber
\end{equation}

The step size $\delta_n$ is determined by the secant method with 
$\sigma=10^{-6}$. The iteration from an initial guess ${\bf x}_0={\bf 0}$ 
is shown below: 
\begin{equation}
  \begin{array}{c||c|c}\hline 
    n & {\bf x}=[x_1,\,x_2,\,x_3]  & ||{\bf f}({\bf x})|| \\\hline\hline
    0  &	0.0000,\;\; 0.0000,\;\;  0.0000	&1.032500e+02 \\\hline
    10 &	0.4246,\;\;-0.0073,\;\;-0.5002	&1.535939e-01 \\\hline
    20 &	0.5015,\;\; 0.0064,\;\; -0.4998	&2.448241e-05 \\\hline
    30 &	0.5009,\;\; 0.0057,\;\; -0.4998	&9.178209e-06 \\\hline
    40 &	0.5006,\;\; 0.0052,\;\; -0.4998	&4.17587e-06 \\\hline
    50 &	0.5004,\;\; 0.0049,\;\; -0.4999	&2.122594e-06 \\\hline
    60 &	0.5003,\;\; 0.0047,\;\; -0.4999	&1.182466e-06 \\\hline
    100&	0.5001,\;\; 0.0043,\;\; -0.4999	&1.805871e-07 \\\hline
    150&	0.5000,\;\; 0.0041,\;\; -0.4999	&2.720744e-08 \\\hline
    200&	0.5000,\;\; 0.0041,\;\; -0.4999	&4.934027e-09 \\\hline
    250&	0.5000,\;\; 0.0040,\;\; -0.4999	&9.630085e-10 \\\hline
    300&	0.5000,\;\; 0.0040,\;\; -0.4999	&1.939747e-10 \\\hline
    350&	0.5000,\;\; 0.0040,\;\; -0.4999	&3.961646e-11 \\\hline
    400&	0.5000,\;\; 0.0040,\;\; -0.4999	&8.140393e-12 \\\hline
    450&	0.5000,\;\; 0.0040,\;\; -0.4999	&1.677968e-12 \\\hline
    500&	0.5000,\;\; 0.0040,\;\; -0.4999	&3.462695e-13 \\\hline
    550&	0.5000,\;\; 0.0040,\;\; -0.4999	&7.136628e-14 \\\hline
    600&	0.5000,\;\; 0.0040,\;\; -0.4999	&1.474205e-14 \\\hline
  \end{array}
  \nonumber
\end{equation}
We see that after the first 100 iterations the error is reduced
to about $o({\bf x})=||{\bf f}({\bf x}^*)||^2\approx 10^{-14}$, and
With 500 additional iterations the algorithm converges to the 
following approximated solution with accuracy of 
$o({\bf x})\approx 10^{-28}$:
\begin{equation}
  {\bf x}^*=\left[\begin{array}{r}
      0.5000013623816102\\
      0.0040027495837189\\
     -0.4999000311539049\end{array}\right]
\end{equation}
Although the gradient descent method requires many more iterations
than Newton's method to converge, the computational cost in each 
iteration is much reduced, as no more matrix inversion is needed.

When it is difficult or too computationally costly to find the 
optimal step size along the search direction, some suboptimal 
step size may be acceptable, such as in the 
\htmladdnormallink{\em quasi-Newton methods}{node8.html}
for minimization problems. In this case, although the step size 
$\delta$ is no longer required to be such that the function value 
at the new position $f({\bf x}_{n+1})=f({\bf x}_n+\delta{\bf d}_n)$ 
is minimized along the search direction ${\bf d}_n$, the step size 
$\delta$ still has to satisfy the following
\htmladdnormallink{\em Wolfe conditions}{https://en.wikipedia.org/wiki/Wolfe_conditions}:
\begin{itemize}
\item {\em Sufficient decrease (Armijo rule):}
  \begin{equation}
    f({\bf x}_{n+1})=f({\bf x}_n+\delta{\bf d}_n)
    \le f({\bf x}_n)+c_1\,\delta{\bf d}^T {\bf g}_n
  \end{equation}
\item {\em Curvature condition:}
  \begin{equation}
    {\bf g}_{n+1}^T {\bf d}_n\ge c_2\;  {\bf g}_n^T {\bf d}_n
  \end{equation}
  As ${\bf g}_n^T{\bf d}_n<0$, this condition can also be written 
  in the following alternative form:
  \begin{equation}
    |{\bf g}_{n+1}^T {\bf d}_n| < |c_2\;{\bf g}_n^T {\bf d}_n|
  \end{equation}
\end{itemize}
Here the two constants $c_1$ and $c_2$ above satisfy $0 < c_1 < c_2 < 1$. 

In general, these conditions are motivated by the desired effect 
that after each iterative step, the function should have a shallower 
slope along ${\bf d}_n$, as well as a lower value, so that eventually 
the solution can be approached where $f({\bf x})$ is minimum and 
the gradient is zero.

Specifically, to understand the first condition above, we represent
the function to be minimized as a single-variable function of the 
step size $\phi(\delta)=f({\bf x}_n+\delta{\bf d}_n)$, and its 
tangent line at the point $\delta=0$ as a linear function 
$L_0(\delta)=a+b\delta$, where the intercept $a$ can be found
as $a=L_0(0)=\phi(\delta)\big|_{\delta=0}=f({\bf x}_n)$, and the
slope $b$ can be found as the derivative of 
$\phi(\delta)=f({\bf x}_n+\delta{\bf d}_n)$ at $\delta=0$:
\begin{equation}
  b=\frac{d}{d\delta}\;f({\bf x}_n+\delta{\bf d}_n)\bigg|_{\delta=0}
  ={\bf g}^T_n{\bf d}_n<0
\end{equation}
which is required to be negative for the function value to be
reduced, $f({\bf x}_n+\delta{\bf d}_n)<f({\bf x}_n)$. Now the
function of the tangent line can be written as
\begin{equation}
  L_0(\delta)=a+b\delta=f({\bf x}_n)+{\bf g}^T_n{\bf d}_n\delta
\end{equation}
Comparing this with a constant line $L_1(\delta)=f({\bf x}_n)$ of
slope zero, we see that any straight line between $L_0(\delta)$
and $L_1(\delta)$ can be described by 
$L(\delta)=f({\bf x}_n)+c_1\,{\bf g}^T_n{\bf d}_n\delta$ with 
$0<c_1<1$, with a slope $0< c_1 {\bf g}_n^T{\bf d}_n< {\bf g}_n^T{\bf d}_n$.
The Armijo rule is to find any $\delta>0$ that satisfies
\begin{equation}
  f({\bf x}_{n+1})=f({\bf x}_n+\delta{\bf d}_n) 
  \le f({\bf x}_n)+c_1\,{\bf g}^T_n{\bf d}_n\delta <f({\bf x}_n)
\end{equation}
We see that the value of $f({\bf x})$ is guaranteed to be reduced.

The second condition requires that at the new position 
${\bf x}_{n+1}$ the slope of the gradient ${\bf g}_{n+1}$ along 
the search direction ${\bf d}_n$ be sufficiently reduced to be 
less than a specified value (determined by $c_2$), in comparison 
to the slope at the old position ${\bf x}_n$. 

\htmladdimg{figures/GradientDescent.png}

The reason why ${\bf g}_{n+1}\perp{\bf g}_n$ can also be explained geometrically.
As shown in the figure the gradient vectors at various points along the direction 
of $-{\bf g}_n$ are shown by the blue arrows, and their projections onto the direction
represent the slopes of the function $f(\delta_n)=f({\bf x}_n-\delta_n{\bf g}_n)$. 
Obviously at ${\bf x}_{n+1}$ where $f(\delta_n)$ reaches its minimum, its slope 
is zero. In other words, the projection of the gradient ${\bf g}_{n+1}$ onto the 
direction of $-{\bf g}_n$ is zero, i.e., ${\bf g}_{n+1}\perp {\bf g}_n$ or
${\bf g}_{n+1}^T{\bf g}_n=0$.

The gradient descent method gradually approaches a solution ${\bf x}$ of an N-D
minimization problem by moving from the initial guess ${\bf x}_0$ along a zigzag 
path composed of a set of segments with any two consecutive segments
perpendicular to each other. The number of steps depends greatly on the initial 
guess. As illustrated in the example below in an N=2 dimensional case, the best 
possible case is that the solution happens to be on the gradient direction of the
initial guess, which could be reached in a single step, while the worst possible 
case is that the gradient direction of the initial guess happens to be 45 degrees
off from the gradient direction of the optimal case, and it takes many zigzag 
steps to go around the optimal path to reach the solution. Many of the steps 
are in the same direction as some of the previous steps. 

To improve the performance of the gradient descent method we can include in
the iteration a momentum term representing the search direction previously
traversed:
\begin{equation}
  {\bf x}_{n+1}={\bf x}_n-\delta_n{\bf g}_n+\alpha_n({\bf x}_n-{\bf x}_{n-1})
\end{equation}
Now two consecutive search directions are no longer perpendicular to each 
other and the resulting search path is smoother than the zigzag path  without
the momentum term. The parameter $\alpha_m$ controls how much momentum is to 
be added.

Obviously it is most desirable not to repeat any of the previous directions 
traveled so that the solution can be reached in N steps, each in a unique 
direction in the N-D space. In other words, the subsequent steps are independent 
of each other, never interfering with the results achieved in the previous steps.
Such a method will be discussed in the next section.


\htmladdimg{figures/GradientDescent1.png}

{\bf Example:} The Rosenbrock function 
\begin{equation}
  f(x,y)=(a-x)^2+b(y-x^2)^2\;\;\;\;\;\;\;\;\mbox{typically $a=1,\;
    b=100$}
  \nonumber
\end{equation}
is a two-variable non-convex function with a global minimum 
$f(1,\;1)=0$ at the point $(1,\;1)$, which is inside a long parabolic 
shaped valley as shown in the figure below. As the slope along the 
valley is very shallow, it is difficult for an algorithm to converge 
quickly to the minimum. For this reason, the Rosenbrock function is 
often used to test various minimization algorithms.

\htmladdimg{figures/Rosenbrock.png}

The figure below shows the search path of the gradient 
descent method based on the optimal step size given in Eq. 
(\ref{GradientStepSizeOptimal}). We see that the search path 
is composed a long sequence of 90 degree turns between consecutive 
segments.

\htmladdimg{figures/RosenbrockGradient0.png}

\htmladdimg{figures/RosenbrockGradient01.png}

When the Newton's method is applied to this minimization problem,
it takes only four iterations for the algorithm to converge to the
minimum, as shown in the figure below:

\htmladdimg{figures/RosenbrockNewton.png}


%If the function $f({\bf x})$ to be minimized is quadratic:
%\begin{equation}
%  f({\bf x})=\frac{1}{2}{\bf x}^T{\bf A}{\bf x}-{\bf b}^T{\bf x}+c 
%\end{equation}
%and ${\bf A}={\bf A}^T$ is symmetric and positive definite, the derivative 
%of $f({\bf x})$ is its gradient vector:
%\begin{equation}
%  {\bf g}=\frac{d}{d{\bf x}}f({\bf x})
%%  =\frac{d}{d{\bf x}}\left( \frac{1}{2}{\bf x}^T{\bf A}{\bf x}-{\bf b}^T{\bf x}+c \right)
%  ={\bf A}{\bf x}-{\bf b}
%\end{equation}
%which is zero at the optimal solution ${\bf x}*$ at which $f({\bf x})$ 
%is minimized, i.e., by solving the linear equation system 
%${\bf A}{\bf x}={\bf b}$, we can find the optimal solution to be
%\begin{equation}
%  {\bf x}^*={\bf A}^{-1}{\bf b} 
%\end{equation}
%If ${\bf A}\ne{\bf A}^T$ is not symmetric, then
%\begin{equation}
%  {\bf g}=\frac{d}{d{\bf x}}f({\bf x})
%  =\frac{d}{d{\bf x}}\left( \frac{1}{2}{\bf x}^T{\bf A}{\bf x}-{\bf b}^T{\bf x}+c \right)
%  =\frac{1}{2}\left({\bf A}+{\bf A}^T\right){\bf x}-{\bf b}
%\end{equation}
%and the optimal solution is
%\begin{equation}
%  {\bf x}^*=2({\bf A}+{\bf A}^T)^{-1}{\bf b} 
%\end{equation}
%If $f({\bf x})$ is not quadratic, we can still approximate it by 
%the first three terms of its Taylor series as a quadratic equation 
%(with ${\bf A}={\bf H}$), and its optimal solution can be found 
%iteratively by the gradient descent method 
%${\bf x}_{n+1}={\bf x}_n-\delta{\bf g}_n$. The optimal step size 
%$\delta_n$ that minimizes $f({\bf x}_{n+1})=f({\bf x}_n-\delta_n{\bf g}_n)$ 
%along the search direction ${\bf d}_n=-{\bf g}_n$ can be found based 
%on the fact that ${\bf g}_{n+1}^T\;{\bf g}_n=0$:
%\begin{eqnarray}
%  {\bf g}_{n+1}^T {\bf g}_n&=&({\bf A}{\bf x}_{n+1}-{\bf b})^T{\bf g}_n
%  =[{\bf A}({\bf x}_n-\delta_n{\bf g}_n)-{\bf b}]^T{\bf g}_n
%  \nonumber \\
%  &=&({\bf A}{\bf x}_n-{\bf b})^T{\bf g}_n-\delta_n({\bf A}{\bf g}_n)^T{\bf g}_n
%  ={\bf g}_n^T{\bf g}_n-\delta_n{\bf g}_n^T{\bf A}{\bf g}_n =0
%  \nonumber
%\end{eqnarray}
%Solving $\delta_n$ we get
%\begin{equation}
%  \delta_n=\frac{{\bf g}_n^T{\bf g}_n}{{\bf g}_n^T{\bf A}{\bf g}_n} 
%  =\frac{||{\bf g}_n||^2}{{\bf g}_n^T{\bf A}{\bf g}_n} 
%  =\frac{{\bf g}_n^T{\bf A}{\bf e}_n}{{\bf g}_n^T{\bf A}{\bf g}_n} 
%\end{equation}
%which is of course the same as the generic optimal step size 
%$\delta=-{\bf g}^T{\bf d}/{\bf d}^T{\bf H}{\bf d}$ obtained before, when
%${\bf A}={\bf H}$ and ${\bf d}_n=-{\bf g}_n$. 

%The Taylor series expansion of function $f({\bf x})$ at point ${\bf 0}$ is 
%\begin{equation} f({\bf x})=f({\bf 0})+\sum_{i=1}^N \frac{\partial f}{\partial x_i}x_i
%=f({\bf 0})+{\bf g}\cdot{\bf x}+\frac{1}{2}{\bf x}^T{\bf H}{\bf x}
%\end{equation}
%where ${\bf g}$ and ${\bf H}$ are respectively the gradient vector and Hessian matrix
%of function $f({\bf x})$ (first and second derivative for a 1-D function $f(x)$) 
%evaluated at ${\bf 0}$:
%\begin{equation} {\bf g}=\bigtriangledown f({\bf 0}),
%\;\;\;\;\;\;
%{\bf H}={\bf H}(f({\bf 0}))=\left[\begin{array}{ccc}
%    \frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1\partial x_N} \\
%    \vdots & \ddots & \vdots \\
%    \frac{\partial^2 f}{\partial x_N\partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_N^2}
%    \end{array}\right]_{\bf 0}
%\end{equation}
 
%The gradient vector of $f({\bf x})$ is:
%\begin{equation} \bigtriangledown f({\bf x})=\frac{d}{d{\bf x}} f({\bf x})={\bf g}+{\bf H}{\bf x} \end{equation}
%and the difference between the gradients of $f({\bf 0})$ and $f({\bf x})$x is
%\begin{equation} \bigtriangledown f({\bf x})-\bigtriangledown f({\bf 0})
%={\bf g}+{\bf H}{\bf x}-{\bf g}={\bf H}{\bf x} \end{equation}
%We see that when the argument of the function changes by ${\bf x}$ (from ${\bf 0}$
%to ${\bf x}$), the corresponding change in its gradient vector is ${\bf H}{\bf x}$.



\subsection*{Quasi-Newton Methods}

As we have seen above, Newton's methods can be used to solve both 
nonlinear systems to find roots of a set of simultaneous equations,
and optimization problems to minimize a scalar-valued objective function
based on the iterations of the same form. Specifically, 
\begin{itemize}
\item Solving equations ${\bf f}({\bf x})={\bf 0}$:
  \begin{equation}
    {\bf x}_{n+1}={\bf x}_n-\delta {\bf J}({\bf x}_n)^{-1}{\bf f}({\bf x}_n) 
    ={\bf x}_n-\delta\;{\bf J}_n^{-1}{\bf f}_n
  \end{equation}
  where ${\bf J}_n={\bf J}({\bf x}_n)$ is the Jacobian matrix of 
  ${\bf f}({\bf x}_n)$, 
\item Minimizing $f({\bf x})$:
  \begin{equation}
  {\bf x}_{n+1}={\bf x}_n-\delta {\bf H}({\bf x}_n)^{-1}{\bf g}({\bf x}_n)
  ={\bf x}_n-\delta\;{\bf H}_n^{-1}{\bf g}_n
  \end{equation}
  where ${\bf g}_n={\bf g}({\bf x}_n)$ and 
  ${\bf H}_n={\bf H}({\bf x}_n)$ are the gradient vector and 
  Hessian matrix of $f({\bf x})$ at ${\bf x}_n$, respectively. 
\end{itemize}

If the Jacobian ${\bf J}({\bf x})$ or the Hessian matrix ${\bf H}({\bf x})$
is not available, or if it is too computationally costly to calculate
its inverse (of complexity $O(N^3)$), the quasi-Newton methods can be 
used to approximate the Hessian matrix or its inverse based only on 
the first order derivative, the gradient ${\bf g}$ of $f({\bf x})$ 
(with complexity $O(N^2)$), similar to the Broyden's method considered 
\htmladdnormallink{here}{../ch2/node6.html}.

In the following, we consider the minimization of a function $f({\bf x})$.
Its Taylor expansion around point ${\bf x}_{n+1}$ is
\begin{equation}
  f({\bf x})=f({\bf x}_{n+1})+({\bf x}-{\bf x}_{n+1})^T {\bf g}_{n+1}
  +\frac{1}{2}({\bf x}-{\bf x}_{n+1})^T {\bf H}_{n+1}
  ({\bf x}-{\bf x}_{n+1})+O(||{\bf x}-{\bf x}_{n+1}||^3)
\end{equation}
Taking derivetive with respect to ${\bf x}$, we get
\begin{equation}
  \frac{d}{d{\bf x}}f({\bf x})={\bf g}({\bf x})
  ={\bf g}_{n+1}+{\bf H}_{n+1} ({\bf x}-{\bf x}_{n+1})
  +O(||{\bf x}-{\bf x}_{n+1}||^2)
\end{equation}
Evaluating at ${\bf x}={\bf x}_n$, we have ${\bf g}({\bf x}_n)={\bf g}_n$, 
and the above can be written as
\begin{eqnarray}
  {\bf g}_{n+1}-{\bf g}_n&=&{\bf H}_{n+1}({\bf x}_{n+1}-{\bf x}_n)
  +O(||{\bf x}_{n+1}-{\bf x}_n||^2)  
  \nonumber\\
  &=&{\bf B}_{n+1}({\bf x}_{n+1}-{\bf x}_n)
\end{eqnarray}
where matrix ${\bf B}_n$ is the secant approximation of the Hessian
matrix ${\bf H}_n$, and the last eqality is called the 
{\em secant equation}. For convenience, we further define:
\begin{equation}
{\bf s}_n={\bf x}_{n+1}-{\bf x}_n,\;\;\;\;\;\;
{\bf y}_n={\bf g}_{n+1}-{\bf g}_n
\end{equation}
so that the equation above can be written as
\begin{equation}
  {\bf B}_{n+1}{\bf s}_n={\bf y}_n,\;\;\;\;\;\mbox{or}\;\;\;\;\;
  {\bf B}_{n+1}^{-1}{\bf y}_n={\bf s}_n
\end{equation}
This is the {\em quasi-Newton equation}, which is the 
{\em secant condition} that must be satisfied by matrix 
${\bf B}_n$, or its inverse ${\bf B}_n^{-1}$, in any of the 
quasi-Newton algorithms, all taking the follow general steps:
\begin{enumerate}
\item Initialize ${\bf x}_0$ and ${\bf B}_0$, set $n=0$; 
\item Compute gradient ${\bf g}_n$ and the search direction 
  ${\bf d}_n=-{\bf B}_n^{-1}{\bf g}_n$;
\item Get ${\bf x}_{n+1}={\bf x}_n+\delta {\bf d}_n$ with step
  size $\delta$ satisfying the 
  \htmladdnormallink{Wolfe conditions}{https://en.wikipedia.org/wiki/Wolfe_conditions}.
\item update ${\bf B}_{n+1}={\bf B}_n+\Delta{\bf B}_n$ or
  ${\bf B}_{n+1}^{-1}={\bf B}_n^{-1}+\Delta{\bf B}_n^{-1}$,
  that satisfies the quasi-Newton equation;
\item If termination condition is not satisfied, $n=n+1$, go back 
  to second step.
\end{enumerate}
For this iteration to converge to a local minimum of $f({\bf x})$,
${\bf B}_n$ must be positive definite matrix, same as the Hessian
matrix ${\bf H}$ it approximates. Specially, if ${\bf B}_n={\bf I}$, 
then ${\bf d}_n=-{\bf g}_n$, the algorithm becomes the gradient 
descent method; also, if ${\bf B}_n={\bf H}_n$ is the Hessian matrix, 
then the algorithm becomes the Newton's method.

In a quasi-Newton method, we can choose to update either ${\bf B}_n$ 
or its inverse ${\bf B}_n^{-1}$ based on one of the two forms of 
the quasi-Newton equation. We note that there is a dual relationship 
between the two ways to update, i.e., by swapping ${\bf y}_n$ and 
${\bf s}_m$, an update formula for ${\bf B}_n$ can be directly 
applied to its inverse and vice versa.

\begin{itemize}
\item The {\em Symmetric Rank 1 (SR1)} Algorithm

  Here matrix ${\bf B}_n$ is updated by an additional term:
  \begin{equation}
    {\bf B}_{n+1}={\bf B}_n+{\bf uu}^T
  \end{equation}
  where ${\bf u}$ a vector and ${\bf uu}^T$ is a matrix of rank 1.
  Imposing the secant condition, we get
  \begin{equation}
    {\bf B}_{n+1}{\bf s}_n
    ={\bf B}_n{\bf s}_n+{\bf uu}^T{\bf s}_n={\bf y}_n,
    \;\;\;\;\mbox{i.e.,}\;\;\;\;\;
    {\bf u}({\bf u}^T{\bf s}_n)={\bf y}_n-{\bf B}_n{\bf s}_n
  \end{equation}
  This equation indicats ${\bf u}$ is in the same direction as 
  ${\bf w}={\bf y}_n-{\bf B}_n{\bf s}_n$, and can therefore
  be written as ${\bf u}=c {\bf w}$. Substituting this back we get
  \begin{equation}
    {\bf u}{\bf u}^T{\bf s}_n=c^2{\bf w}{\bf w}^T{\bf s}_n={\bf w}
  \end{equation}
  Solving this we get $c =1/({\bf w}^T {\bf s}_n)^{1/2}$,
  \begin{equation}
    {\bf u}=c{\bf w}=\frac{{\bf y}_n-{\bf B}_n{\bf s}_n}{({\bf w}^T{\bf s}_n)^{1/2}}
    =\frac{{\bf y}_n-{\bf B}_n{\bf s}_n}{( ({\bf y}_n-{\bf B}_n{\bf s}_n)^T{\bf s}_n)^{1/2}}
  \end{equation}
  and the iteration of ${\bf B}_n$ becomes
  \begin{equation}
    {\bf B}_{n+1}={\bf B}_n+{\bf uu}^T
    ={\bf B}_n
    +\frac{ ({\bf y}_n-{\bf B}_n{\bf s}_n)({\bf y}_n-{\bf B}_n{\bf s}_n)^T }
    { ({\bf y}_n-{\bf B}_n{\bf s}_n)^T {\bf s}_n }
  \end{equation}
  Applying the
  \htmladdnormallink{Sherman-Morrison formula}{../appendices/node7.html},
  we can further get the inverse ${\bf B}_{n+1}$:
  \begin{eqnarray}
    {\bf B}_{n+1}^{-1}&=&\left( {\bf B}_n+{\bf uu}^T \right)^{-1}
    ={\bf B}_n^{-1}-\frac{ {\bf B}_n^{-1}{\bf uu}^T{\bf B}_n^{-1}}{1+{\bf u}^T{\bf B}_n^{-1}{\bf u}}
    \nonumber\\
    &=&{\bf B}_n^{-1}
    +\frac{ ({\bf s}_n-{\bf B}_n^{-1}{\bf y}_n)({\bf s}_n-{\bf B}_n^{-1}{\bf y}_n)^T }
    { ({\bf s}_n-{\bf B}_n^{-1}{\bf y}_n)^T {\bf y}_n }
  \end{eqnarray}
  We note that this equation is dual to the previous one, and it
  can also be obtained based on the duality between the secant 
  conditions for ${\bf B}_n$ and ${\bf B}_n^{-1}$. Specifically, 
  same as above, we impose the secant condition for the inverse 
  ${\bf B}_{n+1}^{-1}$ of the following form
  \begin{equation}
    {\bf B}_{n+1}^{-1}={\bf B}_n^{-1}+{\bf uu}^T
  \end{equation}
  and get
  \begin{equation}
    {\bf B}_{n+1}^{-1}{\bf y}_n
    ={\bf B}_n^{-1}{\bf y}_n+{\bf uu}^T{\bf y}_n={\bf s}_n,
  \end{equation}
  Then by repeating the same process above, we get the same update
  formula for ${\bf B}_n^{-1}$.

  This is the formula for directly updating ${\bf B}_n^{-1}$. 
  For the iteration to converge to a local minimum, matrix 
  ${\bf B}_{n+1}^{-1}$ needs to be positive definite as well as 
  ${\bf B}_n^{-1}$, we therefore require
  \begin{equation}
  {\bf w}^T{\bf y}_n=({\bf s}_n-{\bf B}_n^{-1}{\bf y}_n)^T {\bf y}_n 
  ={\bf s}_n^T{\bf y}_n -{\bf y}_n^T {\bf B}_n^{-1}{\bf y}_n >0
  \end{equation}
  It may be difficult to maintain this requirement through out the
  iteration. This problem can be avoided in the following DFP and 
  BFGS methods.

\item The {\em BFGS (Broyden-Fletcher-Goldfarb-Shanno)} algorithm

  This is a rank 2 algorithm in which matrix ${\bf B}_n$ is updated 
  by two rank-1 terms:
  \begin{equation}
    {\bf B}_{n+1}={\bf B}_n+\alpha{\bf uu}^T+\beta{\bf vv}^T
  \end{equation}
  Imposing the secant condition for ${\bf B}_n$, we get
  \begin{equation}
    {\bf B}_{n+1} {\bf s}_n
    =({\bf B}_n+\alpha{\bf uu}^T+\beta{\bf vv}^T){\bf s}_n
    ={\bf B}_n{\bf s}_n+{\bf u}(\alpha{\bf u}^T{\bf s}_n)
    +{\bf v}(\beta{\bf v}^T{\bf s}_n)={\bf y}_n  
  \end{equation}
  or
  \begin{equation}
    {\bf u}(\alpha{\bf u}^T{\bf s}_n)
    +{\bf v}(\beta{\bf v}^T{\bf s}_n)={\bf y}_n-{\bf B}_n{\bf s}_n
  \end{equation}
  which can be satisfied if we let
  \begin{equation}
    {\bf u}={\bf y}_n,\;\;\;\;\;\;\alpha=\frac{1}{{\bf s}_n^T{\bf y}_n},
    \;\;\;\;\;{\bf v}={\bf B}_n{\bf s}_n,\;\;\;\;\;
    \beta=-\frac{1}{{\bf v}^T{\bf s}_n}=-\frac{1}{{\bf s}_n^T{\bf B}_n{\bf s}_n}
  \end{equation}
  Substituting these into 
  ${\bf B}_{n+1}={\bf B}_n+\alpha{\bf uu}^T+\beta{\bf vv}^T$
  we get
  \begin{equation}
    {\bf B}_{n+1}={\bf B}_n+\alpha{\bf uu}^T+\beta{\bf vv}^T
    ={\bf B}_n+\frac{{\bf y}_n{\bf y}_n^T}{{\bf y}_n^T{\bf s}_n}
    -\frac{{\bf B}_n{\bf s}_n{\bf s}_n^T{\bf B}_n}{{\bf s}_n^T{\bf B}_n{\bf s}_n}
    \label{BFGSupdateB}
  \end{equation}
  Given this update formula for ${\bf B}_n$, we can further find its
  inverse ${\bf B}_n^{-1}$ and then the search direction 
  ${\bf d}_n=-{\bf B}_n^{-1}{\bf g}_n$.

  Alternatively, given ${\bf B}_n$, we can further derive an update 
  formula directly for the inverse matrix ${\bf B}_n^{-1}$, so that 
  the search direction ${\bf d}_n$ can be obtained without carrying 
  out the inversion computation. Specifically, we first define 
  ${\bf U}=[{\bf u}_1\;{\bf u}_2]$ and ${\bf V}=[{\bf v}_1\;{\bf v}_2]$,
  where
  \begin{equation}
    {\bf u}_1={\bf v}_1=\frac{{\bf y}_n}{({\bf s}_n^T{\bf y}_n)^{1/2}},
    \;\;\;\;\;\;
    {\bf u}_2=-{\bf v}_2=\frac{{\bf B}_n{\bf s}_n}{({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2}}
  \end{equation}
  so that the expression above can be written as:
  \begin{equation}
    {\bf B}_{n+1}={\bf B}_n+{\bf u}_1{\bf v}_1^T+{\bf u}_2{\bf v}_2^T
    =({\bf B}_n+{\bf UV}^T)^{-1}
  \end{equation}
  and then apply the
  \htmladdnormallink{Sherman-Morrison formula}{../appendices/node7.html},
  to get:
  \begin{eqnarray}
    {\bf B}_{n+1}^{-1}&=&({\bf B}_n+{\bf UV}^T)^{-1}
    ={\bf B}_n^{-1}-{\bf B}_n^{-1}{\bf U}({\bf I}+{\bf V}^T{\bf B}_n^{-1}{\bf U})^{-1}{\bf V}^T{\bf B}_n^{-1}
    \nonumber\\
    &=&{\bf B}_n^{-1}-{\bf B}_n^{-1}{\bf U}{\bf C}^{-1}{\bf V}^T{\bf B}_n^{-1}  
    ={\bf B}_n^{-1}-{\bf B}_n^{-1}[{\bf u}_1\;{\bf u}_2]{\bf C}^{-1}
    ({\bf B}_n^{-1}[{\bf v}_1\;{\bf v}_2])^T
  \end{eqnarray}
  where we have defined
  \begin{equation}
    {\bf C}=\left[\begin{array}{cc}c_{11} & c_{12}\\c_{21} & c_{22}\end{array}\right]
    ={\bf I}+{\bf V}^T{\bf B}_n^{-1}{\bf U}
    ={\bf I}+[{\bf v}_1\;{\bf v}_2]^T{\bf B}^{-1}_n[{\bf u}_1\;{\bf u}_2]
  \end{equation}
  with
  \begin{equation}
  c_{11}=1+{\bf v}^T_1{\bf B}_n^{-1}{\bf u}_1
  =1+\frac{{\bf y}^T_n{\bf B}_n^{-1}{\bf y}_n}{{\bf s}_n^T{\bf y}_n}
  \end{equation}
  \begin{equation}
  c_{22}=1+{\bf v}_2{\bf B}_n^{-1}{\bf u}_2
  =1-\frac{{\bf s}_n^T{\bf B}_n{\bf B}_n^{-1}{\bf B}_n{\bf s}_n}{{\bf s}_n^T{\bf B}_n{\bf s}_n}
  =0
  \end{equation}
  \begin{equation}
  c_{12}={\bf v}_1^T{\bf B}_n^{-1}{\bf u}_2
  =\frac{{\bf y}_n^T{\bf B}_n^{-1}{\bf B}_n{\bf s}_n}
  { ({\bf s}_n^T{\bf y}_n)^{1/2} ({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2} }
  =\frac{({\bf s}_n^T{\bf y}_n)^{1/2}}{({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2} }
  \end{equation}
  \begin{equation}
  c_{21}={\bf v}_2^T{\bf B}_n^{-1}{\bf u}_1=-c_{12}
  \end{equation}
  \begin{equation}
    {\bf C}=\left[\begin{array}{cc}c_{11} & c_{12}\\-c_{12} & 0\end{array}\right],
    \;\;\;\;\;\;\;\;\;\;\;\;
    {\bf C}^{-1}=
    \left[\begin{array}{cc}0 & -1/c_{12}\\1/c_{12} & c_{11}/c_{12}^2\end{array}\right]
  \end{equation}
  Substituting this ${\bf C}^{-1}$ into the expression for
  ${\bf B}_{n+1}^{-1}$, we get:
  \begin{eqnarray}
    {\bf B}_{n+1}^{-1}&=&
    {\bf B}_n^{-1}-[{\bf B}_n^{-1}{\bf u}_1\;{\bf B}_n^{-1}{\bf u}_2]
    \left[\begin{array}{cc}0 & -1/c_{12}\\1/c_{12} & c_{11}/c_{12}^2\end{array}\right]
    [{\bf B}_n^{-1}{\bf v}_1\;{\bf B}_n^{-1}{\bf v}_2]^T
    \nonumber\\
    &=& {\bf B}_n^{-1}-\frac{1}{c_{12}}
        [{\bf B}_n^{-1}{\bf u}_2{\bf v}_1^T{\bf B}_n^{-1}
          -{\bf B}_n^{-1}{\bf u}_1{\bf v}_2^T{\bf B}_n^{-1}]
        -\frac{c_{11}}{c_{12}^2}{\bf B}_n^{-1}{\bf u}_2{\bf v}_2^T{\bf B}_n^{-1}
        \label{BFGSinverseBexpression}
  \end{eqnarray}
  where
  \begin{equation}
  \frac{1}{c_{12}}{\bf B}_n^{-1}{\bf u}_2{\bf v}_1^T{\bf B}_n^{-1}
  =\frac{({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2} }{({\bf s}_n^T{\bf y}_n)^{1/2}}
  {\bf B}_n^{-1}
  \frac{{\bf B}_n{\bf s}_n}{({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2}}
  \frac{{\bf y}^T_n}{({\bf s}_n^T{\bf y}_n)^{1/2}} {\bf B}_n^{-1}
  =\frac{{\bf s}_n{\bf y}_n^T{\bf B}_n^{-1}}{{\bf s}_n^T{\bf y}_n}
  \end{equation}
  \begin{equation}
  -\frac{1}{c_{12}}{\bf B}_n^{-1}{\bf u}_1{\bf v}_2^T{\bf B}_n^{-1}
  =\frac{({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2} }{({\bf s}_n^T{\bf y}_n)^{1/2}}
  {\bf B}_n^{-1}
  \frac{{\bf y}_n}{({\bf s}_n^T{\bf y}_n)^{1/2}}
  \frac{{\bf s}_n^T {\bf B}_n}{({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2}}{\bf B}_n^{-1}
  =\frac{{\bf B}_n^{-1}{\bf y}_n{\bf s}_n^T}{{\bf s}_n^T{\bf y}_n}
  \end{equation}
  \begin{eqnarray}
    \frac{c_{11}}{c_{12}^2}{\bf B}_n^{-1}{\bf u}_2{\bf v}_2^T{\bf B}_n^{-1}
    &=&-\left(1+\frac{{\bf y}^T_n{\bf B}_n^{-1}{\bf y}_n}{{\bf s}_n^T{\bf y}_n}\right)
    \frac{{\bf s}_n^T{\bf B}_n{\bf s}_n}{{\bf s}_n^T{\bf y}_n} {\bf B}_n^{-1}
    \frac{{\bf B}_n{\bf s}_n}{({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2}}
    \frac{{\bf s}_n^T {\bf B}_n}{({\bf s}_n^T{\bf B}_n{\bf s}_n)^{1/2}} {\bf B}_n^{-1}
    \nonumber\\
    &=&-\left(1+\frac{{\bf y}^T_n{\bf B}_n^{-1}{\bf y}_n}{{\bf s}_n^T{\bf y}_n}\right)
    \frac{{\bf s}_n{\bf s}_n^T}{{\bf s}_n^T{\bf y}_n}
  \end{eqnarray}
  Substituting these three terms back into Eq. (\ref{BFGSinverseBexpression}),
  we get the update formula for ${\bf B}_n^{-1}$:
  \begin{equation}
    {\bf B}_{n+1}^{-1}=
    {\bf B}_n^{-1}-\frac{{\bf B}_n^{-1}{\bf y}_n{\bf s}_n^T+{\bf s}_n{\bf y}_n^T{\bf B}_n^{-1}}{{\bf s}_n^T{\bf y}_n}
    +\left(1+\frac{{\bf y}^T_n{\bf B}_n^{-1}{\bf y}_n}{{\bf s}_n^T{\bf y}_n}\right)
    \frac{{\bf s}_n{\bf s}_n^T}{{\bf s}_n^T{\bf y}_n}
    \label{BFGSupdateInvB}
  \end{equation}
  In summary, ${\bf B}_{n+1}^{-1}$ can be found by either Eq. 
  (\ref{BFGSupdateB}) for ${\bf B}_{n+1}$ followed by an additional 
  inversion operation, or Eq. (\ref{BFGSupdateInvB}) directly for 
  ${\bf B}_{n+1}^{-1}$. The results obtained by these methods are
  equivalent.

\item The {\em DFP (Davidon-Fletcher-Powell)} Algorithm

  Same as the BFGS algorithm, the DFP algorithm is also a rank 2 
  algorithm, where instead of ${\bf B}_n$, the inverse matrix 
  ${\bf B}^{-1}_n$ is updated by two rank-1 terms:
  \begin{equation}
    {\bf B}^{-1}_{n+1}={\bf B}_n^{-1}+\alpha{\bf uu}^T+\beta{\bf vv}^T
  \end{equation}
  where $\alpha$ and $\beta$ are real scalars, ${\bf u}$ and ${\bf v}$
  are vectors. Imposing the secant condition for the inverse of 
  ${\bf B}_n$, we get
  \begin{equation}
    {\bf B}_{n+1}^{-1} {\bf y}_n
    =({\bf B}_n^{-1}+\alpha{\bf uu}^T+\beta{\bf vv}^T){\bf y}_n
    ={\bf B}_n^{-1}{\bf y}_n+{\bf u}(\alpha{\bf u}^T{\bf y}_n)
    +{\bf v}(\beta{\bf v}^T{\bf y}_n)={\bf s}_n  
  \end{equation}
  or
  \begin{equation}
    {\bf u}(\alpha{\bf u}^T{\bf y}_n)
    +{\bf v}(\beta{\bf v}^T{\bf y}_n)={\bf s}_n-{\bf B}_n^{-1}{\bf y}_n
  \end{equation}
  which can be satisfied if we let
  \begin{equation}
    {\bf u}={\bf s}_n,\;\;\;\;\;\;\alpha=\frac{1}{{\bf s}^T{\bf y}_n},
    \;\;\;\;\;{\bf v}={\bf B}_n^{-1}{\bf y}_n,\;\;\;\;\;
    \beta=-\frac{1}{{\bf v}^T{\bf y}_n}=-\frac{1}{{\bf y}_n^T{\bf B}_n^{-1}{\bf y}_n}
  \end{equation}
  Substituting these into 
  ${\bf B}^{-1}_{n+1}={\bf B}^{-1}_n+\alpha{\bf uu}^T+\beta{\bf vv}^T$
  we get
  \begin{equation}
    {\bf B}^{-1}_{n+1}={\bf B}^{-1}_n+\alpha{\bf uu}^T+\beta{\bf vv}^T
    ={\bf B}^{-1}_n+\frac{{\bf s}_n{\bf s}_n^T}{{\bf s}_n^T{\bf y}_n}
    -\frac{{\bf B}_n^{-1}{\bf y}_n{\bf y}_n^T{\bf B}_n^{-1}}{{\bf y}_n^T{\bf B}_n^{-1}{\bf y}_n}
    \label{DFPupdateInvB}
  \end{equation}
  Following the same procedure used for the BFGS method, we can also
  obtain the update formular of matrix ${\bf B}_n$ for the DFP method:
  \begin{equation}
    {\bf B}_{n+1}=
    {\bf B}_n-\frac{{\bf B}_n{\bf s}_n{\bf y}_n^T+{\bf y}_n{\bf s}_n^T{\bf B}_n}{{\bf y}_n^T{\bf s}_n}
    +\left(1+\frac{{\bf s}^T_n{\bf B}_n{\bf s}_n}{{\bf y}_n^T{\bf s}_n}\right)
    \frac{{\bf y}_n{\bf y}_n^T}{{\bf y}_n^T{\bf s}_n}
    \label{DFPupdateB}
  \end{equation}

  We note that Eqs. (\ref{DFPupdateInvB}) and (\ref{BFGSupdateB}) form 
  a duality pair, and Eqs. (\ref{DFPupdateB}) and (\ref{BFGSupdateInvB}) 
  form another duality pair. In other words, the BFGS and FDP methods
  are dual of each other.

%http://www.ing.unitn.it/~bertolaz/2-teaching/2004-2005/AA-2004-2005-PHD/lucidi/slides-mQN-1x2.pdf


\end{itemize}

In both the DFP and BFGS methods, matrix ${\bf B}_n^{-1}$, as 
well as ${\bf B}_n$, must be positive definite, i.e., 
${\bf z}^T{\bf B}_n{\bf z} > 0$ must hold for any ${\bf z}\ne{\bf 0}$.
We now prove that this requirement is satisfied if the curvature
condition of the \htmladdnormallink{Wolfe conditions}{node7.html}
is satisfied:
\begin{equation}
  {\bf g}_{n+1}^T {\bf d}_n \ge c_2\;{\bf g}_n^T {\bf d}_n
  \;\;\;\;\;\;\;\mbox{i.e.}\;\;\;\;\;
  \left( {\bf g}_{n+1}-c_2\;{\bf g}_n\right)^T {\bf d}_n \ge 0
\end{equation}
Replacing the search direction ${\bf d}_n$ by
$\delta{\bf d}_n={\bf x}_{n+1}-{\bf x}_n={\bf s}_n$, we get:
\begin{equation}
({\bf g}_{n+1}-c_2{\bf g}_n)^T {\bf s}_n 
={\bf g}^T_{n+1}{\bf s}_n-c_2{\bf g}_n^T {\bf s}_n \ge 0
\end{equation}
We also have
\begin{equation}
{\bf y}_n^T{\bf s}_n=({\bf g}_{n+1}-{\bf g}_n)^T {\bf s}_n
={\bf g}_{n+1}^T{\bf s}_n-{\bf g}_n^T {\bf s}_n
\end{equation}
Substracting the first equation from the second, we get
\begin{equation}
  {\bf y}_n^T{\bf s}_n-
  ({\bf g}^T_{n+1}{\bf s}_n-c_2{\bf g}_n^T {\bf s}_n)
  =(c_2-1){\bf g}_n^T{\bf s}_n > 0
\end{equation}
The last inequality is due to the fact that${\bf g}_n^T{\bf s}_n<0$ 
and $c_2<1$. We therefore have
\begin{equation}
{\bf y}_n^T{\bf s}_n\ge {\bf g}^T_{n+1}{\bf s}_n-c_2{\bf g}_n^T {\bf s}_n \ge 0
\end{equation}
Given  ${\bf y}_n^T{\bf s}_n>0$, we can further prove by 
induction that both ${\bf B}_{n+1}$ and ${\bf B}_{n+1}^{-1}$ 
based repectively on the update formulae in Eqs. 
(\ref{BFGSupdateB}) and (\ref{DFPupdateInvB}) are positive 
definite. We first assume ${\bf B}_0$ and ${\bf B}_n$ are both 
positive definite, and express ${\bf B}_n$ in terms of its 
Cholesky decomposition, ${\bf B}_n={\bf LL}^T$. We further 
define ${\bf a}={\bf L}^T{\bf z}$, ${\bf b}={\bf L}^T{\bf s}$, 
and get
\begin{equation}
{\bf a}^T{\bf a}={\bf z}^T{\bf B}_n{\bf z},\;\;\;\;
{\bf b}^T{\bf b}={\bf s}^T{\bf B}_n{\bf s},\;\;\;\;
{\bf a}^T{\bf b}={\bf z}^T{\bf B}_n{\bf s}
\end{equation}
Now we can show that the sum of the first and third terms of
Eq. (\ref{BFGSupdateInvB}) is positive definite:
\begin{equation}
  {\bf z}^T\left[ {\bf B}_n-\frac{{\bf B}_n{\bf s}_n{\bf s}_n^T{\bf B}_n}{{\bf s}_n^T{\bf B}_n{\bf s}_n}\right] {\bf z}
  ={\bf z}^T{\bf B}_n{\bf z}
  -\frac{({\bf z}^T{\bf B}_n{\bf s}_n)^2}{{\bf s}_n^T{\bf B}_n{\bf s}_n}
  ={\bf a}^T{\bf a}-\frac{({\bf a}^T{\bf b})^2}{{\bf b}^T{\bf b}}\ge 0
\end{equation}
The last step is due to the Cauchy-Schwarz inequality. Also, as
${\bf s}_n^T{\bf y}_n\ge 0$, the second term of Eq. (\ref{BFGSupdateB}) 
is positive definite
\begin{equation}
{\bf z}^T\left(\frac{{\bf s}_n{\bf s}_n^T}{{\bf s}_n^T{\bf y}_n}\right){\bf z}\ge 0
\end{equation}
Combining these two results, we get
\begin{equation}
{\bf z}^T{\bf B}_{n+1}{\bf z}
=  {\bf z}^T\left( {\bf B}_n-\frac{{\bf B}_n{\bf s}_n{\bf s}_n^T{\bf B}_n}{{\bf s}_n^T{\bf B}_n{\bf s}_n}\right) {\bf z}
+{\bf z}^T\left(\frac{{\bf y}_n{\bf y}_n^T}{{\bf s}_n^T{\bf y}_n}\right){\bf z}\ge 0
\end{equation}
i.e., ${\bf B}_{n+1}$ based on the update formular Eq. (\ref{BFGSupdateB}) 
is positive definite. Following the same steps, we can also show that
${\bf B}_{n+1}^{-1}$ based on Eq. (\ref{DFPupdateInvB}) is positive definite.

{\bf Examples:}

The figures below shows the search path of the DFP and BPGS methods
when applied to find the minimum of the Rosenbrock function.

\htmladdimg{figures/RosenbrockSR1.png}
\htmladdimg{figures/RosenbrockDFP.png}
\htmladdimg{figures/RosenbrockBFGS.png}


\subsection*{Conjugate gradient method}

\htmladdnormallink{reference}{http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf}

The gradient descent method can be used to solve the minimization problem
when the Hessian matrix of the objective function is not available. However, 
this method may not be efficient if it gets into a zigzag search pattern and 
repeat the same search directions many times. This problem can be avoided 
in the {\em conjugate gradient (CG)} method. If the objective function is
quadratic, the CG method converges to the solution in $N$ iterations without 
repeating any of the directions previously traversed. If the objective function 
is not quadratic, the CG method can still significantly improve the performance
in comparison to the gradient descent method. 

Here we will first assume the function $f({\bf x})$ to be minimized is quadratic:
\begin{equation} 
  f({\bf x})=\frac{1}{2}{\bf x}^T{\bf A}{\bf x}-{\bf b}^T{\bf x}+c
\end{equation}
where ${\bf A}={\bf A}^T$ is symmetric. Later we will relax the condition 
for $f({\bf x})$ and consider the minimization of arbitrary functions, which
can be approximated by the first three terms of it Taylor expansion, with 
the symmetric Hessian matrix in the third term.

The gradient and Hessian of the quadratic function given above are respectively:
\begin{equation} 
  {\bf g}({\bf x})=\frac{d}{d{\bf x}}f({\bf x})
  =\frac{d}{d{\bf x}}\left(\frac{1}{2}{\bf x}^T{\bf A}{\bf x}
  -{\bf b}^T{\bf x}+c\right)  ={\bf A}{\bf x}-{\bf b}
\end{equation}
and 
\begin{equation}
  {\bf H}({\bf x})=\frac{d^2}{d{\bf x}^2}f({\bf x})=\frac{d}{d{\bf x}}{\bf g}
  =\frac{d}{d{\bf x}}({\bf Ax}-{\bf b})={\bf A}
\end{equation}
We further assume the Hessian matrix is positive definite, so that $f({\bf x})$ 
has a minimum. 

Solving ${\bf g}({\bf x})={\bf A}{\bf x}-{\bf b}={\bf 0}$, we get the
solution ${\bf x}^*={\bf A}^{-1}{\bf b}$, at which the function is minimized 
to 
\begin{equation}
  f({\bf x}^*)=\frac{1}{2}({\bf A}^{-1}{\bf b})^T{\bf A}({\bf A}^{-1}{\bf b})
  -{\bf b}^T({\bf A}^{-1}{\bf b})+c=-\frac{1}{2}{\bf b}^T{\bf A}^{-1}{\bf b}+c
\end{equation}

At the solution ${\bf x}^*$ that minimizes $f({\bf x})$, its gradient
${\bf g}({\bf x}^*)={\bf A}{\bf x}^*-{\bf b}={\bf 0}$. At the nth estimate 
${\bf x}_n$ the gradient ${\bf g}_n={\bf g}({\bf x}_n)$ can be considered 
as the residual of the nth iteration, and $\varepsilon=||{\bf g}_n||^2$ can 
be used as an error measurement representing how close ${\bf x}_n$ is to 
the true solution ${\bf x}^*$. 

We also see that the minimization of the quadratic function $f({\bf x})$ 
is equivalent to solving a linear equation ${\bf A}{\bf x}={\bf b}$ with 
a symmetric positive definite coefficient matrix ${\bf A}$. The CG method 
considered here can therefore be used for solving both problems. 

{\bf Conjugate basis vectors}

Before continuing to discuss the CG algorithm, we first review the concept of 
\htmladdnormallink{conjugate vectors}{http://fourier.eng.hmc.edu/e176/lectures/algebra/node2.html},
which is the key to the CG method. Two vectors ${\bf u}$ and ${\bf v}$ are
{\em mutually conjugate} (or A-orthogonal or A-conjugate) to each other 
with respect to a symmetric matrix ${\bf A}={\bf A}^T$, if they satisfy:
\begin{equation}
  {\bf u}^T{\bf A}{\bf v}=({\bf A}{\bf v})^T{\bf u}
  ={\bf v}^T{\bf A}{\bf u}=0
\end{equation}
Specially, if ${\bf A}={\bf I}$, the two conjugate vectors become orthogonal 
to each other, i.e., ${\bf v}^T{\bf u}=0$.

Similar to a set of $N$ orthogonal vectors that can be used as the basis 
spanning an N-D space, a set of $N$ mutually conjugate vectors 
$\{{\bf d}_0,\cdots,{\bf d}_{N-1}\}$ satisfying ${\bf d}_i^T{\bf A}{\bf d}_j=0$
($i\ne j$) can also be used as a basis to span the N-D space. Any vector in 
the space can be expressed as a linear combination of these basis vectors.

Also we note that any set of $N$ independent vectors can be converted by the
\htmladdnormallink{Gram-Schmidt process}{http://fourier.eng.hmc.edu/e176/lectures/algebra/node2.html} 
to a set of $N$ basis vectors that are either orthogonal or A-orthogonal.

{\bf Example:}  Given two independent basis vectors of the 2-D space, and
a positive-definite matrix:
\begin{equation}
  {\bf v}_1=\left[\begin{array}{r}1\\0\end{array}\right],\;\;\;\;\;
  {\bf v}_2=\left[\begin{array}{r}0\\1\end{array}\right],\;\;\;\;\;
  {\bf A}=\left[\begin{array}{rr}3&1\\1&2\end{array}\right]
  \nonumber
\end{equation}
we can construct two A-orthogonal basis vectors by the Gram-Schmidt method:
\begin{equation}
  {\bf u}_1={\bf v}_1=\left[\begin{array}{r}1\\0\end{array}\right],
  \;\;\;\;\;\;\;\;
  {\bf u}_2={\bf v}_2-\frac{{\bf u}_1^T{\bf A}{\bf v}_2}{{\bf u}_1^T{\bf A}{\bf u}_1}{\bf u}_1
  =\left[\begin{array}{c}-1/3\\1\end{array}\right]
  \nonumber
\end{equation}
The projections of a vector ${\bf x}=[2,\;3]^T$ onto ${\bf v}_1$ and ${\bf v}_2$ 
are:
\begin{equation}
  {\bf p}_{{\bf v}_1}({\bf x})=\frac{{\bf v}_1^T{\bf x}}{{\bf v}_1^T{\bf v}_1}{\bf v}_1
  =2\left[\begin{array}{c}1\\0\end{array}\right]
  =\left[\begin{array}{c}2\\0\end{array}\right],\;\;\;\;\;
  {\bf p}_{{\bf v}_2}({\bf x})=\frac{{\bf v}_2^T{\bf x}}{{\bf v}_2^T{\bf v}_2}{\bf v}_2
  =3\left[\begin{array}{c}0\\1\end{array}\right]
  =\left[\begin{array}{c}0\\3\end{array}\right]
  \nonumber
\end{equation}
The A-projections of the same vector ${\bf x}$ onto ${\bf u}_1$ and ${\bf u}_2$
are:
\begin{equation}
  {\bf p}_{{\bf u}_1}({\bf x})=\frac{{\bf u}_1^T{\bf A}{\bf x}}
  {{\bf u}_1^T{\bf A}{\bf u}_1}{\bf u}_1
  =3\left[\begin{array}{c}1\\0\end{array}\right]
  =\left[\begin{array}{c}3\\0\end{array}\right],
  \;\;\;\;\;\;\;\;\;
  {\bf p}_{{\bf u}_2}({\bf x})=\frac{{\bf u}_2^T{\bf A}{\bf x}}
  {{\bf u}_2^T{\bf A}{\bf u}_2}{\bf u}_2
  =3\left[\begin{array}{c}-1/3\\1\end{array}\right]
  =\left[\begin{array}{c}-1\\3\end{array}\right]
  \nonumber
\end{equation}
The original vector ${\bf x}$ can be represented in either of the two bases:
\begin{eqnarray}
  {\bf x}=\left[\begin{array}{c}2\\3\end{array}\right]
  &=&2\left[\begin{array}{c}1\\0\end{array}\right]
  +3\left[\begin{array}{c}0\\1\end{array}\right]
  =2{\bf v}_1+3{\bf v}_2
  ={\bf p}_{{\bf v}_1}({\bf x})+{\bf p}_{{\bf v}_2}({\bf x})
  \nonumber \\
  &=&3\left[\begin{array}{c}1\\0\end{array}\right]
  +3\left[\begin{array}{c}-1/3\\1\end{array}\right]
  =3{\bf u}_1+3{\bf u}_2
  ={\bf p}_{{\bf u}_1}({\bf x})+{\bf p}_{{\bf u}_2}({\bf x})
  \nonumber 
\end{eqnarray}


\htmladdimg{figures/Aorthogonal.png}


{\bf Search along a conjugate basis}

Similar to the gradient descent method, which iteratively improves 
the estimated solution by following a sequence of orthogonal search 
directions $\{ -{\bf g}_0,\cdots,-{\bf g}_n,\cdots\}$, the CG method
also follows a sequence of search directions $\{{\bf d}_0,\cdots,
{\bf d}_{N-1}\}$ A-orthogonal to each other, i.e., 
${\bf d}_i^T{\bf Ad}_j=0\;(i\ne j)$:
\begin{equation}
  {\bf x}_{n+1}={\bf x}_n+\delta_n{\bf d}_n
  =\cdots ={\bf x}_0+\sum_{i=0}^n \delta_i{\bf d}_i
  \label{CGiteration}
\end{equation}
Subtracting ${\bf x}^*={\bf x}_{n+1}-{\bf e}_{n+1}={\bf x}_n-{\bf e}_n$
from both sides, we get the iteration in terms of the errors:
\begin{equation}
  {\bf e}_{n+1}={\bf e}_n+\delta_n{\bf d}_n
  =\cdots={\bf e}_0+\sum_{i=0}^n \delta_i{\bf d}_i
  \label{CGerror}
\end{equation}

As it is assumed that the function to be minimized is quadratic 
$f({\bf x})={\bf x}^T{\bf Ax}/2-{\bf b}^T{\bf x}+c$, its
gradient at ${\bf x}_i$ is:
\begin{equation} 
  {\bf g}_i=f'({\bf x}_i)
  ={\bf A}{\bf x}_i-{\bf b}={\bf A}({\bf x}^*+{\bf e}_i)-{\bf b}
  ={\bf A}{\bf x}^*-{\bf b}+{\bf A}{\bf e}_i={\bf A}{\bf e}_i
\end{equation}
where ${\bf x}^*$ is the solution satisfying
${\bf g}({\bf x}^*)={\bf A}{\bf x}^*-{\bf b}={\bf 0}$. The optimal 
step size in Eq. (\ref{OptimalDelta}) can be written as
\begin{eqnarray}
  \delta_i&=&-\frac{{\bf g}_i^T{\bf d}_i}{{\bf d}_i^T{\bf A}{\bf d}_i}
  =-\frac{{\bf e}_i^T{\bf A}{\bf d}_i}{{\bf d}_i^T{\bf A}{\bf d}_i}
  =-\frac{{\bf d}_i^T{\bf A}{\bf e}_i}{{\bf d}_i^T{\bf A}{\bf d}_i}
  =-\frac{ {\bf d}^T_i{\bf A} \left( {\bf e}_0+\sum_{j=0}^{i-1}\delta_j{\bf d}_j \right)}{{\bf d}^T_i{\bf A}{\bf d}_i}
  \nonumber\\
  &=&-\frac{ {\bf d}^T_i{\bf A}{\bf e}_0+\sum_{j=0}^{i-1}\delta_j{\bf d}^T_i{\bf A}{\bf d}_j}{{\bf d}^T_i{\bf A}{\bf d}_i} 
  =-\frac{ {\bf d}^T_i{\bf A}{\bf e}_0}{ {\bf d}^T_i{\bf A}{\bf d}_i}
  \label{delta_gd}
\end{eqnarray}
The last equality is due to the fact that ${\bf d}_i$ and ${\bf d}_j$
are A-orthogonal, i.e., ${\bf d}^T_i{\bf A}{\bf d}_j=0$.
Substituting this into Eq. (\ref{CGerror}) we get
\begin{equation} 
  {\bf e}_{n+1}={\bf e}_n+\delta_n{\bf d}_n
  ={\bf e}_n-\left(\frac{{\bf d}_n^T{\bf A}{\bf e}_0}{{\bf d}_n^T{\bf A}{\bf d}_n}\right){\bf d}_n
\end{equation}

On the other hand, we represent the error ${\bf e}_0={\bf x}_0-{\bf x}^*$ 
associated with the initial guess ${\bf x}_0$ as a linear combination of 
the search vectors $\{{\bf d}_0,\cdots,{\bf d}_{N-1}\}$ treated as $N$ basis 
vectors that span the N-D vector space:
\begin{equation}
  {\bf e}_0=\sum_{i=0}^{N-1} c_i{\bf d}_i
  =\sum_{i=0}^{N-1}{\bf p}_{{\bf d}_i}({\bf e}_0) 
  =\sum_{i=0}^{N-1} \left(\frac{ {\bf d}^T_i{\bf A}{\bf e}_0}{ {\bf d}^T_i{\bf A}{\bf d}_i}\right){\bf d}_i
  \label{cine0}
\end{equation}
where ${\bf p}_{{\bf d}_i}({\bf e}_0)$ is the A-projection of ${\bf e}_0$ 
onto the ith basis vector ${\bf d}_i$:
\begin{equation}
  {\bf p}_{{\bf d}_i}({\bf e}_0)=c_i{\bf d}_i
  =\left(\frac{ {\bf d}^T_i{\bf A}{\bf e}_0}{ {\bf d}^T_i{\bf A}{\bf d}_i}\right)
  {\bf d}_i
\end{equation}
We note that the coefficient $c_i$ happens to be the negative optimal step
size in Eq. (\ref{delta_gd}):
\begin{equation}
  c_i=\frac{{\bf d}^T_i{\bf A}{\bf e}_0}{{\bf d}^T_i{\bf A}{\bf d}_i}=-\delta_i
\end{equation}
Now the expression of ${\bf e}_{n+1}$ in Eq. (\ref{CGerror}) can be written as
\begin{equation} 
  {\bf e}_{n+1}={\bf e}_0+\sum_{i=0}^n\delta_i{\bf d}_i
  =\sum_{i=0}^{N-1}c_i{\bf d}_i-\sum_{i=0}^n c_i{\bf d}_i=\sum_{i=n+1}^{N-1}c_i{\bf d}_i
  =\sum_{i=n+1}^{N-1}{\bf p}_{{\bf d}_i}({\bf e}_0)
\end{equation}
We see that in each iteration from $n$ to $n+1$, the number of terms
in the summation is reduced by one, i.e., the nth component 
${\bf p}_{{\bf d}_n}({\bf e}_0)$ of ${\bf e}_n$ along the direction of 
${\bf d}_n$ is completely eliminated. After $N$ such iterations, the
error is reduced from ${\bf e}_0$ to ${\bf e}_N={\bf 0}$, and the true 
solution is obtained ${\bf x}_N={\bf x}^*+{\bf e}_N={\bf x}^*$. 
                                   
Pre-multiplying ${\bf d}_k^T{\bf A}\;(k\le n)$ on both sides of the
equation above, we get:
\begin{equation} 
  {\bf d}_k^T{\bf A}{\bf e}_{n+1}={\bf d}_k^T{\bf g}_{n+1}
  =\sum_{i=n+1}^{N-1}c_i{\bf d}_k^T{\bf A}{\bf d}_j=0 
  \label{gd0}
\end{equation}
We see that after $n+1$ iterations the remaining error ${\bf e}_{n+1}$ is 
A-orthogonal to all previous directions ${\bf d}_0,\cdots,{\bf d}_n$, and 
the gradient ${\bf g}_{n+1}$ is orthogonal to these directions.

In the figure below, the conjugate gradient method is compared with the gradient 
descent method for the case of $N=2$. We see that the first search direction is
the same $-{\bf g}_0$ for both methods. However, the next search direction 
${\bf d}_1$ is A-orthogonal to ${\bf d}_0$, same as the next error ${\bf e}_1$, 
different from the search direction $-{\bf g}_1$ in gradient descent method. 
The conjugate gradient method finds the solution ${\bf x}$ in $N=2$ steps, 
while the gradient descent method has to go through many more steps all 
orthogonal to each other before it finds the solution.

\htmladdimg{figures/ConjugateGradient.png}.

{\bf Find the A-orthogonal basis}

The $N$ A-orthogonal search directions $\{{\bf d}_0,\cdots,{\bf d}_{N-1}\}$ 
can be constructed based on any set of $N$ independent vectors 
$\{{\bf v}_0,\cdots,{\bf v}_{N-1}\}$ by the 
\htmladdnormallink{Gram-Schmidt process}{../algebra/node2.html}:
\begin{equation}
  {\bf d}_n={\bf v}_n-\sum_{j=0}^{n-1} {\bf p}_{{\bf d}_j}({\bf v}_n)
  ={\bf v}_n-\sum_{m=0}^{n-1} \left(\frac{{\bf d}^T_m{\bf A}{\bf v}_n}{{\bf d}^T_m{\bf A}{\bf d}_m}\right)  {\bf d}_m
  ={\bf v}_n-\sum_{m=0}^{n-1} \beta_{nm}{\bf d}_m
\end{equation}
where $\beta_{nm}={\bf d}^T_m{\bf A}{\bf v}_n/({\bf d}^T_m{\bf A}{\bf d}_m)$ 
and $\beta_{nm}{\bf d}_m$ is the A-projection of ${\bf v}_n$ onto each
of the previous direction ${\bf d}_m$.

However, we specifically choose to use ${\bf v}_n=-{\bf g}_n$, to gain
some significant computational advantage as shows below. Now the
Gram-Schmidt process above becomes:
\begin{equation}
  {\bf d}_n =-{\bf g}_n-\sum_{m=0}^{n-1} \beta_{nm}{\bf d}_m
  \label{GSCG}
\end{equation}
where
\begin{equation}
  \beta_{nm}=\frac{{\bf d}_m^T{\bf A}{\bf v}_n}{{\bf d}^T_m{\bf A}{\bf d}_m}
  =-\frac{{\bf d}_m^T{\bf A}{\bf g}_n}{{\bf d}^T_m{\bf A}{\bf d}_m}
  \;\;\;\;\;\;(m<n)
  \label{GSbeta}
\end{equation}
The equation above also indicates that ${\bf g}_n$ can be written 
as a linear combination of all previous search directions
${\bf d}_0,\cdots,{\bf d}_n$:
\begin{equation}
  {\bf g}_n=\sum_{i=0}^n \alpha_i{\bf d}_i
\end{equation}
Premultiplying ${\bf g}_{n+1}^T$ on both sides, we get
\begin{equation}
  {\bf g}_{n+1}^T{\bf g}_k={\bf g}_{n+1}^T\left( \sum_{i=0}^k \alpha_i{\bf d}_i\right)
  =\sum_{i=0}^k \alpha_i\;{\bf g}_{n+1}^T{\bf d}_i=0
\end{equation}
The last equality is due to ${\bf g}_{n+1}^T{\bf d}_k=0$ (Eq. (\ref{gd0})).
We see that ${\bf g}_{n+1}$ is also orthogonal to all previous gradients 
${\bf g}_0,\cdots,{\bf g}_n$.

Pre-multiplying ${\bf g}_k^T (k\ge n)$ on both sides of Eq. (\ref{GSCG}),
we get
\begin{equation} 
  {\bf g}_k^T{\bf d}_n=-{\bf g}_k^T{\bf g}_n-\sum_{m=0}^{n-1} \beta_{mn}\,{\bf g}_k^T{\bf d}_m
  =-{\bf g}_k^T{\bf g}_n
  =\left\{ \begin{array}{cc}-||{\bf g}_n||^2&n=k\\ 0&n<k\end{array}\right.
\end{equation}
Note that all terms in the summation are zero as ${\bf g}_k^T{\bf d}_m=0$ 
for all $m<n\le k$ (Eq. (\ref{gd0})). Substituting 
${\bf g}_n^T{\bf d}_n=-||{\bf g}_n||^2$ into Eq. (\ref{delta_gd}), we get
\begin{equation}
  \delta_n=-\frac{{\bf g}_n^T{\bf d}_n}{{\bf d}_n^T{\bf A}{\bf d}_n}
  =\frac{||{\bf g}_n||^2}{{\bf d}_n^T{\bf A}{\bf d}_n} 
  \label{CGstepsize}
\end{equation}
Next we consider
\begin{equation}
  {\bf g}_{m+1}={\bf A}{\bf x}_{m+1}-{\bf b}
  ={\bf A}({\bf x}_m+\delta_m{\bf d}_m)-{\bf b}
  =({\bf A}{\bf x}_m-{\bf b})+\delta_m{\bf A}{\bf d}_m
  ={\bf g}_m+\delta_m{\bf A}{\bf d}_m
\end{equation}
Pre-multiplying ${\bf g}_n^T$ with $n>m$ on both sides we get
\begin{equation} 
  {\bf g}_n^T{\bf g}_{m+1}={\bf g}_n^T{\bf g}_m+\delta_m{\bf g}_n^T{\bf A}{\bf d}_m 
  =\delta_m{\bf g}_n^T{\bf A}{\bf d}_m 
\end{equation}
where ${\bf g}_n^T{\bf g}_m=0$ ($m\ne n$). Solving for ${\bf g}_n^T{\bf A}{\bf d}_m$ 
we get
\begin{equation} 
  {\bf g}_n^T{\bf A}{\bf d}_m
  =\frac{1}{\delta_m} {\bf g}_n^T{\bf g}_{m+1}
  =\left\{\begin{array}{cl}
  ||{\bf g}_n||^2/\delta_{n-1}&m=n-1\\  0&m<n-1\end{array}\right.
\end{equation}
Substituting this into Eq. (\ref{GSbeta}) we get
\begin{equation}
  \beta_{nm}=-\frac{{\bf d}_m^T{\bf A}{\bf g}_n}{{\bf d}^T_m{\bf A}{\bf d}_m}
  =\left\{\begin{array}{cl}-||{\bf g}_n||^2/\delta_{n-1}{\bf d}_{n-1}^T{\bf A}{\bf d}_{n-1}
&m=n-1
  \\0&m<n-1\end{array}\right.
\end{equation}

which is non-zero only when $m=n-1$, i.e., there is only one non-zero term
in the summation of the Gram-Schmidt formula for ${\bf d}_n$. This is the 
reason why we choose ${\bf v}_n=-{\bf g}_n$. We can now drop the second 
subscript $m$ in $\beta_{nm}$. Substituting the step size
$\delta_{n-1}=||{\bf g}_{n-1}||^2/{\bf d}_{n-1}^T{\bf A}{\bf d}_{n-1}$ 
(Eq. (\ref{CGstepsize})) into the above expression for $\beta_{nm}=\beta_m$,
we get
\begin{equation} 
  \beta_n=-\frac{||{\bf g}_n||^2}{||{\bf g}_{n-1}||^2} 
\end{equation}
We note that matrix ${\bf A}$ no longer appears in the expression.

{\bf The CG algorithm}

In summary here are the steps of the conjugate gradient algorithm:
\begin{enumerate}
\item Set $n=0$ and initialize the search direction (same as gradient descent):
  \begin{equation} 
    {\bf d}_0=-{\bf g}_0
  \end{equation}
\item Terminate if the error $\varepsilon=||{\bf g}_n||^2$ is smaller 
  than a preset threshold. Otherwise, continue with the following:
\item Find optimal step size and step forward:
  \begin{equation}
  \delta_n=\frac{||{\bf g}_n||^2}{{\bf d}_n^T{\bf A}{\bf d}_n},
  \;\;\;\;\;\;\;\;
  {\bf x}_{n+1}={\bf x}_n+\delta_n{\bf d}_n 
  \end{equation}
\item Update gradient:
  \begin{equation}
  {\bf g}_{n+1}=\frac{d}{d{\bf x}}f({\bf x}_{n+1})
  \end{equation}
%  \begin{equation} {\bf g}_{n+1}={\bf g}_n+\delta_n{\bf A}{\bf d}_n \end{equation}
\item Find coefficient for the Gram-Schmidt process:
  \begin{equation}
  \beta_{n+1}=\frac{||{\bf g}_{n+1}||^2}{||{\bf g}_n||^2} 
%  =-\frac{||{\bf r}_{n+1}||^2}{||{\bf r}_n||^2} 
  \end{equation}
\item Update search direction:
  \begin{equation} 
    {\bf d}_{n+1}=-{\bf g}_{n+1}+\beta_{n+1}{\bf d}_n 
%    ={\bf r}_{n+1}-\beta_{n+1}{\bf d}_n 
  \end{equation}
  Set $n=n+1$ and go back to step 2.
\end{enumerate}

The algorithm above assumes the objective function $f({\bf x})$ to be quadratic 
with known ${\bf A}$. But when $f({\bf x})$ is not quadratic, ${\bf A}$ is no 
longer available, and the Hessian ${\bf H}$ may not be a good approximation of 
${\bf A}$. However, we can still assume that $f({\bf x})$ can be approximated 
as a quadratic function in the neighborhood of its minimum. In this case, the 
algorithm can be modified so that it does not depend on ${\bf A}$. Specifically,
in step 3 above, the optimal step size $\delta_n$ is calculated based on ${\bf A}$.
When ${\bf A}$ is unavailable, $\delta$ can also be found by line minimization
based on any suitable algorithms for 1-D optimization. 

%Also, the coefficient $\beta_{n+1}$, which is derived based on the assumption 
%that $f({\bf x})$ is quadratic, may no longer be valid. Some alternative formulas
%for $\beta_{n+1}$ can be used:
%\begin{equation} 
%  \beta_{n+1}=\frac{{\bf g}^T_{n+1}({\bf g}_{n+1}-{\bf g}_n)}{||{\bf g}_n||^2}
%  \;\;\;\;\;\mbox{or}\;\;\;\;\;
%  \beta_{n+1}=-\frac{{\bf g}^T_{n+1}({\bf g}_{n+1}-{\bf g}_n)}
%       { {\bf d}_n^T({\bf g}_{n+1}-{\bf g}_n)} 
%\end{equation}
%These expressions are identical to $\beta_{n+1}=||{\bf g}_{n+1}||^2/||{\bf g}_n||^2$
%when $f({\bf x})$ is indeed quadratic. Note that it is now possible for $\beta_{n+1}<0$.
%If this happens to be the case, we will use $\beta_{n+1}=0$, i.e., the next search
%direction is simply ${\bf d}_{n+1}=-{\bf g}_n-\beta_n{\bf d}_n=-{\bf g}_n$, same
%as the gradient descent method.

If it is known that the current ${\bf x}_n$ is close enough to the solution 
${\bf x}^*$, we can approximate $f({\bf x})$ as a quadratic function, and
its Hessian matrix ${\bf H}$ can be used to approximate ${\bf A}$ so that the 
original algorithm may still be used. 

{\bf Example 1:} 
To compare the conjugate method and the gradient descent method, consider a very 
simple 2-D quadratic function
\begin{equation}
  f(x,y)={\bf x}^T{\bf A}{\bf x}
  =[x_1,\,x_2]\left[\begin{array}{cc}3&1\\1&2\end{array}\right]
  \left[\begin{array}{c}x_1\\x_2\end{array}\right]
  \nonumber
\end{equation}
The performance of the gradient descent method depends significantly on
the initial guess. For the specific initial guess of ${\bf x}_0=[1.5,\;-0.75]^T$,
the iteration gets into a zigzag pattern and the convergence is very slow,
as shown below:

\begin{equation}
\begin{array}{c|c|c}\hline
    n & {\bf x}=[x_1,\,x_2] & f({\bf x}) \\\hline
    0 & 1.500000, -0.750000 & 2.812500 \\
    1 & 0.250000, -0.750000 & 0.468750e-01 \\
    2 & 0.250000, -0.125000 & 7.812500e-02 \\
    3 & 0.041667, -0.125000 & 1.302083e-02 \\
    4 & 0.041667, -0.020833 & 2.170139e-03 \\
    5 & 0.006944, -0.020833 & 3.616898e-04 \\
    6 & 0.006944, -0.003472 & 6.028164e-05 \\
    7 & 0.001157, -0.003472 & 1.004694e-05 \\	
    8 & 0.001157, -0.000579 & 1.674490e-06 \\
    9 & 0.000193, -0.000579 & 2.790816e-07 \\	
    10 & 0.000193, -0.000096 & 4.651361e-08 \\	
    11 & 0.000032, -0.000096 & 7.752268e-09 \\
    12 & 0.000032, -0.000016 & 1.292045e-09 \\
    13 & 0.000005, -0.000016 & 2.153408e-10 \\\hline
  \end{array}
  \nonumber
\end{equation}
However, as expected, the conjugate gradient method takes exactly
$N=2$ steps from any initial guess to reach at the solution:

\begin{equation}
  \begin{array}{c|c|c}\hline
    n & {\bf x}=[x_1,\,x_2] & f({\bf x}) \\\hline
    0 & 1.500000, -0.750000 & 2.812500e+00 \\
    1 & 0.250000, -0.750000 & 4.687500e-01 \\
    2 & 0.000000, -0.000000 & 1.155558e-33 \\\hline
  \end{array}
  \nonumber
\end{equation}

\htmladdimg{figures/GDvsCG2.png}

For an $N=3$ example of $f({\bf x})={\bf x}^T{\bf A}{\bf x}$ with
\begin{equation}
  {\bf A}=\left[\begin{array}{ccc}5 & 3 & 1\\3 & 4 & 2\\1 & 2 & 3
    \end{array}\right]
  \nonumber
\end{equation}
from an initial guess ${\bf x}_0=[1,\;2,\;3]^T$, it takes the gradient 
descent method 41 iterations to reach 
${\bf x}_{41}=[3.5486e-06,\;-7.4471e-06,\;4.6180e-06]^T$ corresponding
to $f({\bf x})=8.5429e-11$. From the same initial guess, it takes the 
conjugate gradient method only $N=3$ iterations to converge to the 
solution:

\begin{equation}
  \begin{array}{c|c|c}\hline
    n & {\bf x}=[x_1,\,x_2,\,x_3] & f({\bf x}) \\\hline
    0 &  1.000000,  2.000000, 3.000000 & 4.500000e+01 \\
    1 & -0.734716, -0.106441, 1.265284 & 2.809225e+00 \\
    2 &  0.123437, -0.209498, 0.136074 & 3.584736e-02 \\
    3 & -0.000000,  0.000000, 0.000000 & 3.949119e-31 \\\hline
  \end{array}
  \nonumber
\end{equation}

For an $N=9$ example, it takes over 4000 iterations for the gradient
descent method to converge with $||{\bf e}||\approx 10^{-10}$, but exactly 9
iterations for the CG method to converge with $||{\bf e}||\approx 10^{-16}$.

{\bf Example 2:} 

The figure below shows the search path of the conjugate gradient method
applied to the minimization of the Rosenbrock function:

\htmladdimg{figures/RosenbrockCG.png}


{\bf Example 3:} 

Solve the following $N=3$ non-linear equation system by the CG method:
\begin{equation}
  \left\{\begin{array}{l}
  f_1(x_1,\,x_2,\,x_3)=3x_1-(x_2x_3)^2-3/2\\
  f_2(x_1,\,x_2,\,x_3)=4x_1^2-625\,x_2^2+2x_2-1\\
  f_3(x_1,\,x_2,\,x_3)=exp(-x_1x_2)+20x_3+9\end{array}\right.
  \nonumber
\end{equation}
The solution is known to be $x_1=0.5,\;x_2=0,\;x_3=-0.5$.

This equation system can be represented in vector form as ${\bf f}({\bf x})={\bf 0}$
and the objective function is $o({\bf x})={\bf f}^T({\bf x}){\bf f}({\bf x})$. 
The iteration of the CG method with an initial guess ${\bf x}_0={\bf 0}$ is 
shown below:
\begin{equation}
  \begin{array}{c|c|c}\hline
    n & {\bf x}=[x_1,\,x_2,\,x_3] & o({\bf x}) \\\hline
    0 &	0.0000,  0.0000,  0.0000 &	1.032500e+02 \\
    1 &	0.0113,  0.0050, -0.5001 &	3.160163e+00 \\
    2 &	0.0188, -0.0021, -0.5004 &	3.095894e+00 \\
    3 &	0.5009, -0.0018, -0.5004 &	7.268252e-05 \\
    4 &	0.5009, -0.0017, -0.5000 &	1.051537e-05 \\
    5 &	0.5008, -0.0012, -0.5000 &	6.511151e-06 \\
    6 &	0.5001, -0.0005, -0.5000 &	6.365321e-07 \\
    7 &	0.5001, -0.0005, -0.5000 &	5.667357e-07 \\
    8 &	0.5002, -0.0004, -0.5000 &	2.675128e-07 \\
    9 &	0.5001, -0.0003, -0.5000 &	1.344218e-07 \\
    10&	0.5001, -0.0002, -0.5000 &	1.241196e-07 \\
    11&	0.5000, -0.0001, -0.5000 &	2.120969e-08 \\
    12&	0.5000, -0.0001, -0.5000 &	1.541814e-08 \\
    13&	0.5000, -0.0001, -0.5000 &	7.282025e-09 \\
    14&	0.5000, -0.0001, -0.5000 &	4.801781e-09 \\
    15&	0.5000, -0.0000, -0.5000 &	4.463926e-09 \\\hline
  \end{array}
  \nonumber
\end{equation}

In comparison, the gradient descent method would need to take over 200 
iterations (with much reduced complexity though) to reach this level of 
error. 


{\bf Conjugate gradient method used for solving linear equation systems:}

As discussed before, if ${\bf x}$ is the solution that minimizes the 
quadratic function $f({\bf x})={\bf x}^T{\bf A}{\bf x}/2-{\bf b}^T{\bf x}+c$, 
with ${\bf A}={\bf A}^T$ being symmetric and positive definite, it also satisfies
$d\,f({\bf x})/d{\bf x}={\bf A}{\bf x}-{\bf b}={\bf 0}$. In other words, the 
optimization problem is equivalent to the problem of solving the linear system 
${\bf A}{\bf x}-{\bf b}={\bf 0}$, both can be solved by the conjugate gradient 
method.

Now consider solving the linear system ${\bf A}{\bf x}={\bf b}$ with 
${\bf A}={\bf A}^T$. Let ${\bf d}_i,\;(i=1,\cdots,N)$ be a set of $N$ 
A-orthogonal vectors satisfying ${\bf d}_i^T{\bf A}{\bf d}_j=0\;(i\ne j)$,
which can be generated based on any $N$ independent vectors, such as the
standard basis vectors, by the Gram-Smidth method. The solution ${\bf x}$
of the equation ${\bf A}{\bf x}={\bf b}$ can be represented by these $N$ 
vectors as
\begin{equation} 
  {\bf x}=\sum_{i=1}^N c_i{\bf d}_i
\end{equation}
Now we have
\begin{equation}
  {\bf b}={\bf A}{\bf x}={\bf A}\left[\sum_{i=1}^N c_i{\bf d}_i\right]
=\sum_{i=1}^N c_i{\bf A}{\bf d}_i
\end{equation}
Pre-multiplying ${\bf d}_j^T$ on both sides we get
\begin{equation}
  {\bf d}_j^T{\bf b}=\sum_{i=1}^N c_i{\bf d}_j^T{\bf A}{\bf d}_i=c_j{\bf d}_j^T{\bf A}{\bf d}_j
\end{equation}
Solving for $c_j$ we get
\begin{equation}
c_j=\frac{{\bf d}_j^T{\bf b}}{{\bf d}_j^T{\bf A}{\bf d}_j}
\end{equation}
Substituting this back into the expression for ${\bf x}$ we get the solution
of the equation:
\begin{equation} 
  {\bf x}=\sum_{i=1}^N c_i{\bf d}_i
  =\sum_{i=1}^N \left(\frac{{\bf d}_i^T{\bf b}}{{\bf d}_i^T{\bf A}{\bf d}_i}\right)
  {\bf d}_i
\end{equation}
Also note that as ${\bf b}={\bf A}{\bf x}$, the ith term of the summation above
is simply the A-projection of ${\bf x}$ onto the ith direction ${\bf d}_i$:
\begin{equation}
{\bf p}_{{\bf d}_i}({\bf x})
=\left(\frac{{\bf d}_i^T{\bf A}{\bf x}}{{\bf d}_i^T{\bf A}{\bf d}_i}\right){\bf d}_i
\end{equation}

One application of the conjugate gradient method is to solve the normal 
equation to find the least-square solution of an over-constrained equation 
system ${\bf A}{\bf x}={\bf b}$, where the coefficient matrix ${\bf A}$ is $M$ 
by $N$ with rank $R=N<M$. As discussed previously, the normal equation of this 
system is
\begin{equation}
  {\bf A}^T{\bf A}{\bf x}={\bf A}^T{\bf b} 
\end{equation}
Here ${\bf A}^T{\bf A}$ is an $N$ by $N$ symmetric, positive definite matrix.
This normal equation can be solved by the conjugate gradient method.


\subsection*{Issues of Local/Global Minimum}


A main concern in optimization problems is whether the solution obtained by 
the algorithm is a local minimum or a global one. The simulated annealing
(SA) algorithm is typically used to search for the global minimum in a 
high-dimensional space, by allowing up-hill moves in the iteration to avoid
getting stuck by a local minimum. SA mimics the annealing process in metallurgy,
in which the temperature is carefully controlled to decrease slowly so that the
material reaches a state with minimum free energy. 

The way the SA method avoids getting stuck at a local minimum is to allow the 
iteration steps to go up-hill. Specifically, the objective function $f({\bf x})$ 
evaluated at ${\bf x}_n$ is treated as the energy at ${\bf x}_n$, and an up-hill 
move corresponding to an energy increase $\Delta E=f({\bf x}_{n+1})-f({\bf x}_n)_0$ 
can be accepted if some probability function is greater than a random threshold 
value $Th$ within a certain range comparable with $P$:
\begin{equation} 
P(\Delta E, T)=e^{-\Delta E/T}  > Th
\end{equation}
Obviously smaller $\Delta E$ and higher $T$ will cause higher $P$, it is more
probable for a move to be accepted.

During the entire search process, the temperature $T$ gradually reduces so that 
the probability of accepting an up-hill move also gradually reduces. In the early
stage of the search, it is more probable to go up-hill, thereby getting out of a
local minimum; but toward the later part of the search process, such a move is
less likely to happen, as presumably in this late stage of the search, the energy
level is already low, not likely to be at a local minimum with high energy level.
Through out the search, the best result corresponding to he lowest energy level
is always updated, to be returned as the final solution at the end of the process.

Further information can be found at 
\htmladdnormallink{wikipedia}{http://en.wikipedia.org/wiki/Simulated_annealing}
and
\htmladdnormallink{mathworld}{http://mathworld.wolfram.com/SimulatedAnnealing.html}.

A c program for AS can be found 
\htmladdnormallink{here}{http://fourier.eng.hmc.edu/e176/lectures/siman.c}.


\section*{Constrained Optimization}

% http://web.stanford.edu/class/msande310/310trialtext.pdf
% http://web.mit.edu/15.053/www/AMP-Chapter-13.pdf

An optimization problem is more complicated if it is constrained, i.e.,
the arguments ${\bf x}=[x_1,\cdots,x_N]^T$ of the objective function 
$f({\bf x})$ are subject to certain equality or inequality constraints
in terms of what values they can take. Such a constrained optimization
problem can be formulated as:
\begin{equation}
  \begin{array}{ll}
    \mbox{optimize} & f({\bf x})=f(x_1,\cdots,x_N) \\
    \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l} 
      h_i({\bf x})=0,\;\;\;\;\;(i=1,\cdots m)\\
      g_j({\bf x})\le 0,\;\;\;\;\;(j=1,\cdots n)
    \end{array} \right.
  \end{array}
\end{equation}
The set of all ${\bf x}$ satisfying the $m+n$ constraints is called
the feasible region of the problem. The goal is to find a point
${\bf x}^*$ in the feasible region at which $f({\bf x}^*)$ is an 
extremum. 

In the most general case where the objective function $f({\bf x})$ 
and the constraint functions $g_i({\bf x})$ and $h_j({\bf x})$ are 
nonlinear, the process of solving this nonlinear optimization problem 
is called {\em nonlinear programming (NLP)}. Specially, if the 
objective function is quadratic while the constraints are linear, 
(the feasible region is a polytope), the process is called 
{\em quadratic programming (QP)}. More specially, if the objective 
function is linear as well as the constraints, the process is called 
{\em linear programming (LP)}. In the following, we will first 
consider the NLP in general, and then discuss more specifically 
QP and LP.

\subsection*{Optimization with Equality Constraints}

The optimization problems subject to equality constraints can be
generally formulated as:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize/minimize} & f({\bf x})=f(x_1,\cdots,x_N) \\
    \mbox{subject to:} & h_i({\bf x})=h_i(x_1,\cdots,x_N)=0,
    \;\;\;\;\;(i=1,\cdots m)\\
  \end{array}
\end{equation}
where $f({\bf x})$ is the objective function and $h_i({\bf x})=0$
is the ith constraint. Here $m\le N$, as in general there does not
exist a solution that satisfies more than $N$ equations in the N-D 
space.

To visualize the problem, we first consider the special case with 
$N=2$ and $m=1$. In this case, both $f({\bf x})$ and $h({\bf x})$ 
are surfaces defined over the 2-D space spanned by $x_1$ and $x_2$, 
and $h({\bf x})=0$ is the contour line of $h({\bf x})$ on the 2-D 
plane. The solution ${\bf x}^*$ must satisfy the following two 
conditions:
\begin{itemize} 
\item ${\bf x}^*$ is on the contour line $h({\bf x})=0$;
\item ${\bf x}^*$ is on the contour line $f({\bf x})=d$ (for some 
  $d$ as shown in the figure), so that $f({\bf x}^*)$ is an extremum,
  it does not increase/decrease in the neighborhood of ${\bf x}^*$
  along the contour line.
\end{itemize}
In other words, the two contours $h({\bf x})=0$ and $f({\bf x})=d$
must coincide at ${\bf x}^*$, i.e., they must have the same tangents
and therefore parallel gradients (perpendicular to the tangents):
\begin{equation}
  \bigtriangledown_{\bf x}f({\bf x}^*)
  =\lambda^*\,\bigtriangledown_{\bf x} h({\bf x}^*)
\end{equation}
where the constant scaling factor $\lambda^*$ is positive if the
two gradients are in the same direction or negative if they are
in opposite directions. As here we are only interested in whether
the two curves coincide or not, the sign of $\lambda^*$ is of no
concern.

\htmladdimg{figures/Lagrange.png}

To find such a point ${\bf x}^*$, we first construct a new objective 
function, called the {\em Lagrange function} (or simply 
{\em Lagrangian}):
\begin{equation}
  L({\bf x},\lambda)=f({\bf x})-\lambda\, h({\bf x})
\end{equation}
where $\lambda$ is the {\em Lagrange multiplier}, which can be 
either positive or negative (the sign in front of $\lambda$ can 
be either plus or minus), and then set the gradient of this 
objective function with respect to $\lambda$ as well as ${\bf x}$ 
to zero:
\begin{equation}
  \bigtriangledown_{{\bf x},\lambda} L({\bf x},\lambda)
  =\bigtriangledown_{{\bf x},\lambda} [ f({\bf x})-\lambda\, h({\bf x}) ]
  ={\bf 0}
\end{equation}
This equation contains two parts:
\begin{equation}
  \left\{\begin{array}{l}
  \bigtriangledown_{\bf x} f({\bf x})=\lambda\bigtriangledown_{\bf x} h({\bf x})\\
  \bigtriangledown_\lambda L({\bf x},\lambda)=\partial L({\bf x},\lambda)/\partial \lambda
  =-h({\bf x})=0
  \end{array}\right.
\end{equation}
This is an equation system containing $N+m=2+1=3$ equations for 
the same number of unknowns $x_1,\,x_2,\,\lambda$:
\begin{equation}
  \frac{\partial f({\bf x})}{\partial x_i}
  =\lambda \frac{\partial h({\bf x})}{\partial x_i}\;\;\;(i=1,2),
  \;\;\;\;\;h({\bf x})=0
\end{equation}
The solution ${\bf x}^*=[x_1^*,\,x_2^*]^T$ and $\lambda^*$ (either
positive or negative) of this equation system happens to satisfy 
the desired relationships: (a) 
$\bigtriangledown_{\bf x}f({\bf x}^*)=\lambda^*\bigtriangledown_{\bf x}h({\bf x}^*)$,
i.e., the two curves $f({\bf x})=d$ and $h({\bf x})=0$ do coincide 
at ${\bf x}^*$, and (b) $h({\bf x}^*)=0$, i.e., ${\bf x}^*$ does 
indeed satisfy the equality constraints. 

Specially, consider the following two possible cases:
\begin{itemize}
\item If $\lambda^*=0$, then 
  $\bigtriangledown_{\bf x}f({\bf x}^*)=\lambda^*\bigtriangledown_{\bf x}h({\bf x}^*)=0$, 
  indicating $f({\bf x}^*)$ is an extremum independent of the constraint,
  i.e., the constraint $h({\bf x})=0$ is inactive, and the optimization 
  is unconstrained. 
\item If $\lambda^*\ne 0$, then $\bigtriangledown_{\bf x}f({\bf x}^*)\ne 0$
  (as in general $\bigtriangledown_{\bf x}h({\bf x}^*)\ne{\bf 0}$),
  indicating $f({\bf x}^*)$ is not an extremum without the constraint, 
  i.e., the constraint is active, and the optimization is indeed
  constrained.
\end{itemize}

The discussion above can be generalized from 2-D to an $N>2$ dimensional
space, in which the optimal solution ${\bf x}^*$ is to be found to
extremize the objective $f({\bf x})$ subject to $m$ equality 
constraints $h_i({\bf x})=0\;(i=1,\cdots,m)$, each representing a 
contour surface in the N-D space. The solution ${\bf x}^*$ must 
satisfy the following two conditions:
\begin{itemize}
\item ${\bf x}^*$ is on all $m$ contour surfaces so that
  $h_1({\bf x}^*)=\cdots=h_m({\bf x}^*)=0$, i.e., it is on the 
  intersection of these surfaces, a curve in the space (e.g.,
  the intersection of two surfaces in 3-D space is a curve);
\item ${\bf x}^*$ is on a contour surface $f({\bf x}^*)=d$
  (for some value $d$) so that $f({\bf x}^*)$ is an extremum,
  i.e., it does not increase or degrease in the neighborhood of 
  ${\bf x}^*$ along the curve.
\end{itemize}
Combining these conditions, we see that the intersection curve 
of the $m$ surfaces $h_i({\bf x})=0$ must coincide with the contour
surface $f({\bf x})=d$ at ${\bf x}^*$, i.e., the intersection of 
the tangent surfaces of $h_i({\bf x})=0$ through ${\bf x}^*$ must
lie on the tangent surface of $f({\bf x})=d$ at ${\bf x}^*$,
therefore the gradient $\bigtriangledown f({\bf x}^*)$ must 
be a linear combination of the $m$ gradients 
$\bigtriangledown h_i({\bf x}^*)$:
\begin{equation}
\bigtriangledown_{\bf x}f({\bf x}^*)
=\sum_{i=1}^m \lambda_i^* \bigtriangledown_{\bf x} h_i({\bf x}^*)
\end{equation}

\htmladdimg{figures/Lagrange1.png}

To find such a solution ${\bf x}^*$ of the optimization problem, 
we first construct the Lagrange function:
\begin{equation}
  L({\bf x},{\bf\lambda})=f({\bf x})-\sum_{i=1}^m\lambda_i h_i({\bf x})
\end{equation}
where ${\bf\lambda}=[\lambda_1,\cdots,\lambda_m]^T$ is a vector
for $m$ {\em Lagrange multipliers}, and then set its gradient to 
zero: 
\begin{equation}
  \bigtriangledown_{{\bf x},{\bf\lambda}} L({\bf x},{\bf\lambda})
  =\bigtriangledown_{{\bf x},{\bf\lambda}}
  \left[ f({\bf x})-\sum_{i=1}^m \lambda_i h_i({\bf x}) \right]
  ={\bf 0}
\end{equation}
to get the following two equation systems of $N$ and $m$ equations 
repectively:
\begin{equation}
  \bigtriangledown_{\bf x} f({\bf x})
  =\sum_{i=1}^m \lambda_i \bigtriangledown_{\bf x} h_i({\bf x})
  \;\;\;\;\mbox{i.e.}\;\;\;\;
  \frac{\partial f({\bf x})}{\partial x_j}
  =\sum_{i=1}^m\lambda_i \frac{\partial h_i({\bf x})}{\partial x_j}
  \;\;\;(j=1,\cdots,N),
\end{equation}
and
\begin{equation}
  \frac{\partial L({\bf x},{\bf\lambda})}{\partial \lambda_i}
  =h_i({\bf x})=0\;\;\;\;(i=1,\cdots,m)
\end{equation}
The first set of $N$ equations indicates that the gradient 
$\bigtriangledown_{\bf x}f({\bf x}^*)$ at ${\bf x}^*$ is a 
linear combination of the $m$ gradients 
$\bigtriangledown_{\bf x} h_i({\bf x}^*)$, while the second 
set of $m$ equations guarantees that ${\bf x}^*$ also satisfy
the $m$ equality constraints. Solving these equations we get 
the desired soluton ${\bf x}^*$ together with the Lagrange
multipliers $\lambda_1,\cdots,\lambda_m$. This is the 
{\em Lagrange multiplier method}.

Specially, if $\lambda_i=0$, then the ith constraint is not active. 
More specially if $\lambda_i=0$ for all $i=1,\cdots,m$, then 
\begin{equation}
  \bigtriangledown_{\bf x} f({\bf x}^*)
  =\sum_{i=1}^m \lambda_i \bigtriangledown_{\bf x} h_i({\bf x})={\bf 0}
\end{equation}
indicating $f({\bf x}^*)$ is an extremum independent of any of
the constraints, i.e., ${\bf x}^*$ is an unconstrained solution, 
and the optimization problem is unconstrained.

% http://www.ifp.illinois.edu/~angelia/ge330fall09_nlpkkt_l26.pdf
% http://www.math.mtu.edu/~msgocken/ma5630spring2003/lectures/bar/bar.pdf
% http://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf
% http://privatewww.essex.ac.uk/~wangt/Presession%20Math/Lecture%205.pdf
% https://www.cs.cmu.edu/~ggordon/10725-F12/slides/16-kkt.pdf
% https://www.tu-ilmenau.de/fileadmin/media/simulation/Lehre/Vorlesungsskripte/Lecture_materials_Abebe/NLP_Notes.pdf

\subsection*{Optimization with Inequality Constraints}

The optimization problems subject to inequality constraints can
be generally formulated as:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize/minimize} & f({\bf x})=f(x_1,\cdots,x_N) \\
    \mbox{subject to:} & 
      g_j({\bf x})=g_j(x_1,\cdots,x_N)\le \;\mbox{or}\;\;
%      g_j({\bf x})=g_j(x_1,\cdots,x_N)
      \ge 0,\;\;\;\;\;\;\;\;(j=1,\cdots n)
  \end{array}
\end{equation}
Again, to visualize the problem we first consider an example with 
$N=2$ and $n=1$, as shown in the figure below for the minimization 
(left) and maximization (right) of $f({\bf x})$ subject to 
$g({\bf x}) > 0$. The constrained solution ${\bf x}^*=[x^*_1,\,x^*_2]^T$ 
is on the boundary of the feasible region satisfying $g(x^*_1,x^*_2)=0$, 
while the unconstrained extremum is outside the feasible region.

\htmladdimg{figures/KKT0a.png}

Consider the following two possible cases.
\begin{itemize}
\item First, if the unconstrained extremum at which 
  $\bigtriangledown_{\bf x} f({\bf x})=0$ is outside the feasible
  region, i.e., the inequality constraint is active, then the 
  constrained solution ${\bf x}^*$ must be
  \begin{itemize}
  \item on the boundary of the feasible region, i.e., $g({\bf x}^*)=0$,
  \item different from the unconstrained solution, i.e., 
    $\bigtriangledown_{\bf x} f({\bf x}^*)\ne{\bf 0}$;
  \end{itemize}
  The problem becomes the same as an equality constrained problem 
  considered before, which requires the objective function $f({\bf x})$
  and the constraining function $g({\bf x})$ to have the same tangent
  at ${\bf x}^*$, or parallel gradients: 
  \begin{equation}
    \bigtriangledown_{\bf x} f({\bf x}^*)=\mu^*\;\bigtriangledown_{\bf x} g({\bf x}^*)
    \;\;\;\;\mbox{or}\;\;\;\;
    \bigtriangledown_{\bf x} f({\bf x}^*)-\mu^*\;\bigtriangledown_{\bf x} g({\bf x}^*)=0
  \end{equation}
  However, different from before, now we are also concerned with whether
  the two gradients have the same or opposite directions, corresponding to 
  $\mu>0$ or $\mu<0$, respectively, as illustrated in the 1-D examples in 
  the figure below:

  \htmladdimg{figures/KKT1a.png}

  Depending on whether $f({\bf x})$ is to be maximized or minimized, 
  and whether the constraint is $g({\bf x})\ge 0$ or $g({\bf x})\le 0$,
  there exist four possible cases in terms of the sign of $\mu$, as 
  summarized in the table below: 

  \begin{equation}
    \begin{array}{c||c|c}\hline 
      & g({\bf x})\ge 0 & g({\bf x})\le 0 \\\hline\hline
      \max\; f({\bf x}) &\bigtriangledown f({\bf x}_1)=\mu\,\bigtriangledown g({\bf x}_1),\;\mu<0 & 
      \bigtriangledown f({\bf x}_3)=\mu\,\bigtriangledown g({\bf x}_3),\;\mu>0 \\\hline
      \min\; f({\bf x}) & \bigtriangledown f({\bf x}_2)=\mu\,\bigtriangledown g({\bf x}_2),\;\mu>0 & 
      \bigtriangledown f({\bf x}_4)=\mu \,\bigtriangledown g({\bf x}_4),\;\mu<0 \\\hline
    \end{array}
    \label{PolarityTable}
  \end{equation}
  These cases can also be summarized in terms of the product of
  the coeficient $\mu$ and the constraining function $g({\bf x})$:
  \begin{equation}
    \mu\, g({\bf x})\;\left\{\begin{array}{ll} 
    \le 0 & \mbox{for maximization}\\ \ge 0 & \mbox{for minimization}
    \end{array}\right.
    \label{mugminmax}
  \end{equation}

\item Second, if the unconstrained extremum is inside the feasible 
  region, i.e., the inequality constraint is inactive, then the problem 
  is actually unconstrained and its solution ${\bf x}^*$ is 
  \begin{itemize}
  \item the same as the unconstrained solution, i.e.,
    $\bigtriangledown_{\bf x} f({\bf x}^*)={\bf 0}$;
  \item not on the boundary of the feasible region, i.e., 
    $g({\bf x}^*)\ne 0$;
  \end{itemize}
  This solution can still be found by solving the same equation 
  $\bigtriangledown_{\bf x} f({\bf x})
  =\mu\,\bigtriangledown_{\bf x} g({\bf x})={\bf 0}$ with $\mu=0$. 
\end{itemize}

Summarizing the two cases above, we see that $g({\bf x}^*)=0$ but
$\mu^*\ne 0$ in the first case, $g({\bf x}^*)\ne 0$ but $\mu^*=0$
in the second case, i.e., the following holds in either case:
\begin{equation}
  \mu^*\,g({\bf x}^*)=0
  \label{mugeqz}
\end{equation}

The discussion above can be generalized from 2-D to $N>2$ dimensional
space, in which the optimal solution ${\bf x}^*$ is to be found 
to extremize the objective $f({\bf x})$ subject to $n$ inequality 
constraints $g_i({\bf x})=0\;(i=1,\cdots,n)$. To solve this inequality
constrained optimization problem, we first construct the Lagrangian:
\begin{equation}
  L({\bf x},{\bf \mu})=f({\bf x})-\sum_{i=1}^n\mu_i\,g_i({\bf x})
\end{equation}
Here we note that in some literatures, a plus sign is used in front of 
the summation of the second term. This is equivalent to our discussion
here so long as the sign of $\mu$ indicated in Table \ref{PolarityTable}
is negated. 

We now set the gradient of the Lagrangian to zero: 
\begin{equation}
  \bigtriangledown_{{\bf x},{\bf \mu}} L({\bf x},{\bf \mu})
  =\bigtriangledown_{{\bf x},{\bf \mu}}\left[f({\bf x})-\sum_{i=1}^n\mu_i\,g_i({\bf x})\right]
  ={\bf0}
\end{equation}
and get two equation systems of $N$ and $n$ equations,
respectively:
\begin{equation}
  \bigtriangledown_{\bf x}f({\bf x})
  =\sum_{i=1}^n\mu_i \bigtriangledown_{\bf x} g_i({\bf x})
\end{equation}
and
\begin{equation}
  \frac{\partial L({\bf x},{\bf \mu})}{\partial\mu_i}
  =g_i({\bf x})=0\;\;\;\;(i=1,\cdots,n)
\end{equation}

The result above for the inequality constrained problems is the same 
as that for the equality constrained problems considered before. However,
we note that there is an additional requirement regarding the sign of the
scaling coifficients. For an equality constrained problem, the direction 
of the gradient $\bigtriangledown g({\bf x})$ is of no concern, i.e., the
sign of $\lambda$ is unrestricted; but here for an inequality constrained 
problem, $\mu$ is required to be either positive or negative, depending
on whether the gradients of $f({\bf x}^*)$ and $g({\bf x}^*)$ are in the
same or opposite directions, as shown in Table \ref{PolarityTable}. For 
example, to minimize $f({\bf x})$ subject to $g({\bf x})\ge 0$, $\mu^*$ 
needs to be positive, but to maximize $f({\bf x})$ subject to 
$g({\bf x})\ge 0$, $\mu^*$ needs to be negative.

In principle, we can find the solution ${\bf x}^*$ that optimizes 
$f({\bf x})$ while also satisfying the inequality constraints by
solving the equation system above for ${\bf x}$ as well as ${\bf\mu}$
with the required sign.

We can now address the general optimization of an N-D objective function
$f({\bf x})$ subject to multiple constraints of both equalities and 
inequalities:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize/minimize} & f({\bf x})=f(x_1,\cdots,x_N) \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l} 
      h_i({\bf x})=0,\;\;\;\;\;(i=1,\cdots m)\\
      g_j({\bf x})\le 0\;\;\mbox{or}\;\;
      g_j({\bf x})\ge 0,\;\;\;\;\;\;\;\;(j=1,\cdots n)\\
    \end{array} \right.
  \end{array}
\end{equation}
For notational convenience, we represent the $m+n$ equality and 
inequality constraints in vector form as: 
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize/minimize} & f({\bf x})\\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l} 
      {\bf h}({\bf x})={\bf 0}\\
      {\bf g}({\bf x})\le{\bf 0}\;\;\mbox{or}\;\;{\bf g}({\bf x})\ge{\bf 0}
    \end{array} \right.
  \end{array}
\end{equation}
where ${\bf h}({\bf x})=[h_1({\bf x}),\cdots,h_m({\bf x})]^T$ and 
${\bf g}({\bf x})=[g_1({\bf x}),\cdots,g_n({\bf x})]^T$.

To solve this optimization problem, we first construct the Lagrangian
\begin{equation}
  L({\bf x},{\bf\lambda},{\bf\mu})
  =f({\bf x})-\sum_{i=1}^m\lambda_i h_i({\bf x})-\sum_{j=1}^n\mu_j g_j({\bf x})
  =f({\bf x})-{\bf\lambda}^T {\bf h}({\bf x})-{\bf\mu}^T{\bf g}({\bf x})
\end{equation}
where the Lagrange multipliers in ${\bf\lambda}=[\lambda_1,\cdots,\lambda_m]^T$ 
and ${\bf\mu}=[\mu_1,\cdots,\mu_n]^T$ are for the $m$ equality and $n$ 
non-negative constraints, respectively, and then set its gradient with 
respect to both ${\bf\lambda}$ and ${\bf\mu}$ as well as ${\bf x}$ to 
zero. The solution ${\bf x}^*$ can then be obtained by solving the 
resulting equation system, with $\mu_j$ taking the proper sign depending
on whether $f({\bf x})$ is to be maximized or minimized, and whether the
inequality constraints are $g_j({\bf x})\le 0$ or $g_j({\bf x})\ge 0$, 
as specified in Table \ref{PolarityTable}. Also, the solution ${\bf x}^*$
should satisfy the generalized version of Eq. (\ref{mugeqz}):
\begin{equation}
  \mu_j\,g_j({\bf x}^*)=0,\;\;\;\;\;\;(j=1,\cdots,n)
  \label{mugeqz1}
\end{equation}

{\bf Example} 

Find the extremum of $f(x_1,x_2)=x^2_1+x^2_2$ subject to each of 
the three different constraints: $x_1+x_2=1$, $x_1+x_2\le 1$, and 
$x_1+x_2\ge 1$. 
\begin{itemize}
\item $h(x_1,x_2)=x_1+x_2-1=0$:
  \begin{equation}
    \begin{array}{ll}
      \mbox{minimize/maximize:}\;\;\;\;\;\;f(x_1,x_2)=x^2_1+x^2_2\\
      \mbox{subject to:}\;\;\;\;\;\;h(x_1,x_2)=x_1+x_2-1=0\\
    \end{array}
    \nonumber
  \end{equation}
  The Lagrangian is:
  \begin{equation}
    L(x_1,x_2,\lambda)=f(x_1,x_2)-\lambda g(x_1,x_2)
    =x^2_1+x^2_2-\lambda(x_1+x_2-1)
    \nonumber
  \end{equation}
  Solving the following equations 
  \begin{equation}
    \frac{\partial L}{\partial x_1}=2x_1-\lambda=0,\;\;\;
    \frac{\partial L}{\partial x_2}=2x_2-\lambda=0,\;\;\;
    \frac{\partial L}{\partial \lambda}=x_1+x_2-1=0
    \nonumber
  \end{equation}
  we get $\lambda^*=1$ and the optimal solution $x_1^*=x_2^*=0.5$ 
  (satisfying $x_1^*+x_2^*=1$), at which the function is minimized to 
  be $f(0.5,\,0.5)=0.5$. We also note that $f(x_1,x_2)$ and $g(x_1,x_2)$ 
  have the same gradients
  \begin{equation}
    \bigtriangledown f(0.5,0.5)=\bigtriangledown h(0.5,0.5)=[1,1]^T
    \nonumber
  \end{equation}
\item The following two problems
  \begin{equation}
    \begin{array}{ll}
      \mbox{minimize:} & f(x_1,x_2)=x^2_1+x^2_2\\
      \mbox{subject to:} & g(x_1,x_2)=x_1+x_2-1\ge 0
    \end{array}
    \nonumber
  \end{equation}
  and
  \begin{equation}
    \begin{array}{ll}
      \mbox{maximize:} & f(x_1,x_2)=x^2_1+x^2_2\\
      \mbox{subject to:} & g(x_1,x_2)=x_1+x_2-1\le 0
    \end{array}
    \nonumber
  \end{equation}
  have the same Lagrangian:
  \begin{equation}
    L(x_1,x_2,\mu)=f(x_1,x_2)-\mu\, g(x_1,x_2)=x^2_1+x^2_2-\mu(x_1+x_2-1)
  \end{equation}
  Solving the following equations 
  \begin{equation}
    \frac{\partial L}{\partial x_1}=2x_1-\mu=0,\;\;\;
    \frac{\partial L}{\partial x_2}=2x_2-\mu=0,\;\;\;
    \frac{\partial L}{\partial \mu}=x_1+x_2-1=0
    \nonumber
  \end{equation}
  we still get the same results $\mu^*=1>0$ and $x_1^*=x_2^*=\mu^*/2=0.5$, 
  at which the function reaches minimum or maximum $f(0.5,\,0.5)=0.5$ 
  subject to the corresponding constraint $x_1+x_2\ge 1$ or 
  $x_1+x_2\le 1$.
\item We could also consider the following two problems:
  \begin{equation}
    \begin{array}{ll}
      \mbox{minimize:} & f(x_1,x_2)=x^2_1+x^2_2\\
      \mbox{subject to:} & g(x_1,x_2)=x_1+x_2-1\le 0
    \end{array}
    \nonumber
  \end{equation}  
  and
  \begin{equation}
    \begin{array}{ll}
      \mbox{maximize:} & f(x_1,x_2)=x^2_1+x^2_2\\
      \mbox{subject to:} & g(x_1,x_2)=x_1+x_2-1\ge 0
    \end{array}
    \nonumber
  \end{equation}
  The Lagrangian is the same as before, however, now we need to
  consider the case where $\mu^*=0$ and
  \begin{equation}
    \frac{\partial L}{\partial x_1}=2x_1=0,\;\;\;
    \frac{\partial L}{\partial x_2}=2x_2=0
    \nonumber
  \end{equation}
  Solving these we get $x_1^*=x_2^*=0$, corresponding to the solution
  for the first problem, the absolute minimum of $f(0,\,0)=0$,
  independent of any constraint, i.e, the minimization problem is 
  unconstrained. However, as the function is not bounded from above,
  its maximum does not exist, i.e., the second problem has no
  solution.
\end{itemize}

\subsection*{Duality and KKT Conditions}

For a given optimization problem, now called the {\em primal problem},
there exists a {\em dual problem}. To see this, consider the following
problem:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize} & f_p({\bf x})\\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l} 
      {\bf h}({\bf x})={\bf 0}\\
      {\bf g}({\bf x})\le{\bf 0}\;\;\mbox{or}\;\;{\bf g}({\bf x})\ge{\bf 0}
    \end{array} \right.
  \end{array}
\end{equation}
with its Lagrangian:
\begin{equation}
  L({\bf x},{\bf\lambda},{\bf\mu})
  =f_p({\bf x})-{\bf\lambda}^T{\bf h}({\bf x})-{\bf\mu}^T{\bf g}({\bf x})
\end{equation}
As this is a minimization problem, here $\mu^T g({\bf x})\ge 0$ as
indicated in Table \ref{PolarityTable}.

We now consider the {\em dual function} defined as the infimum 
(lower bound) of the Lagrangian over ${\bf x}$:
\begin{equation}
  f_d({\bf\lambda},{\bf\mu})
  =\inf_{\bf x} L({\bf x},{\bf\lambda},{\bf\mu})
  =\inf_{\bf x}\left[ f_p({\bf x})-{\bf\lambda}^T{\bf h}({\bf x})
    -{\bf\mu}^T{\bf g}({\bf x})\right]
\end{equation}
which can be obtained by solving the equation
$\bigtriangledown_{\bf x}L({\bf x},{\bf\lambda},{\bf\mu})={\bf 0}$ to 
get ${\bf x}$ that minimizes the Lagrangian, and then substituting 
it back into the Lagrangian. Moreover, in order to obtain the 
tightest lower bound, the supremum (upper bound) of the dual function 
$f_d({\bf\lambda},{\bf\mu})$ over all possible values for its variables
${\bf\lambda}$ and ${\bf\mu}$, we need to solve the following optimization
problem:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize} & f_d({\bf\lambda},{\bf\mu})
    =\inf_{\bf x}L({\bf x},{\bf\lambda},{\bf\mu})\\
    \mbox{subject to:} & {\bf\mu}\le{\bf 0}\;\;\mbox{or}\;\;
    {\bf\mu}\ge{\bf 0}
  \end{array}
\end{equation}
This is the {\em dual problem} of the original primal problem.
Note that which of the two inequalities of the dual problem is used 
depends on which of the two inequalities of the primal problem is 
used, as indicated in Table \ref{PolarityTable}. Specifically, for 
this minimization primal problem, ${\bf\mu}^T{\bf g}({\bf x})\ge 0$
(Eq. (\ref{mugminmax})), i.e., the inequality constraints of both
the primal and dual problems should be either greater or smaller than 
zero.

For any ${\bf x}$, including the solutions ${\bf x}^*$, in the 
feasible regions defined by ${\bf h}({\bf x})={\bf 0}$ and 
${\bf g}({\bf x})\ge{\bf 0}$, the following holds
\begin{equation}
  f_d({\bf\lambda},{\bf\mu})\le L({\bf x},{\bf\lambda},{\bf\mu})
  =f_p({\bf x})-{\bf\lambda}^T{\bf h}({\bf x})-{\bf\mu}^T{\bf g}({\bf x})
  \le f_p({\bf x})-{\bf\mu}^T{\bf g}({\bf x}) \le f_p({\bf x})
\end{equation}
We further define $d^*=f_d({\bf\lambda}^*,{\bf\mu}^*)$ and 
$f_p({\bf x}^*)=p^*$, and get
\begin{equation}
  d^*=f_d({\bf\lambda}^*,{\bf\mu}^*)\le L({\bf x},{\bf\lambda},{\bf\mu})
  \le f_p({\bf x}^*)=p^*
\end{equation}

Similarly, we can also show that a primal maximization problem with
solution ${\bf x}^*$ that satisfies $p^*=f({\bf x}^*)\ge f({\bf x})$ has
a dual minimization problem with solution ${\bf\lambda}^*,\;{\bf\mu}^*$
that satisfies
\begin{equation}
  d^*=f_d({\bf\lambda}^*,\;{\bf\mu}^*)\ge f_p({\bf x}^*)=p^*
\end{equation}

We can therefore conclude that the solution of the dual problem always 
provides the tightest bound for the solution of the primal problem. The 
difference $p^*-d^*\ge 0$ for the minimization primal problem or 
$d^*-p^*\ge 0$ for the maximization primal problem is called the 
{\em duality gap}. If the duality gap is non-zero, i.e., $d^*\ne p^*$, 
the dual relationship is a {\em weak duality}, otherwise it is a 
{\em strong duality}, for which the following holds:
\begin{eqnarray}
  d^*&=&f_d({\bf\lambda}^*,{\bf\mu}^*)= L({\bf x}^*,{\bf\lambda}^*,{\bf\mu}^*)
  =f_p({\bf x}^*)-{\bf\lambda}^{*T}{\bf h}({\bf x}^*)-{\bf\mu}^{*T}{\bf g}({\bf x}^*)
  \nonumber\\
  &=&f_p({\bf x}^*)-{\bf\mu}^{*T}{\bf g}({\bf x}^*) =f_p({\bf x}^*)=p^*
  \nonumber
\end{eqnarray}
which further implies all following equations must hold:
\begin{equation}
  \left\{\begin{array}{ll}
  \bigtriangledown_{\bf x} L({\bf x}^*,{\bf\lambda},{\bf\mu})
  =\bigtriangledown_{\bf x} f({\bf x}^*)
  -\sum_{i=1}^m\lambda_i \bigtriangledown_{\bf x} h_i({\bf x}^*)
  -\sum_{j=1}^n\mu_j     \bigtriangledown_{\bf x} g_j({\bf x}^*)={\bf 0}
  & \mbox{(stationarity)}  \\
  \left\{\begin{array}{l}
  h_i({\bf x}^*)=0,\;\;\;\;\;(i=1,\cdots,m)\\
  g_j({\bf x}^*)\le 0\;\;\mbox{or}\;\; g_j({\bf x}^*)\ge 0,\;\;\;
  (j=1,\cdots,n)
  \end{array}\right.&\mbox{(primal feasibility)}  \\
  \mu_j^*\le 0,\;\;\mbox{or}\;\;\mu_j^*\ge 0,\;\;\;\;(j=1,\cdots,n)&
  \mbox{(dual feasibility)} \\
  \mu_j^*\,g_j({\bf x}^*)=0,\;\;\;\;\;(j=1,\cdots,n) & 
  \mbox{(complementarity)}
  \end{array}\right.
  \label{KKTConditions}
\end{equation}

These are called the {\em Karush-Kuhn-Tucker (KKT) conditions}, the 
necessary conditions for the solution of a constrained optimization 
problem, i.e., for any ${\bf x}$ to be a solution, it must satisfy
all of the following:
\begin{itemize}
\item Stationarity: ${\bf x}$ maximizes or minimizes the Lagrangian, 
  i.e., $\bigtriangledown_{\bf x}L({\bf x},{\bf\lambda},{\bf\mu})={\bf 0}$;
\item Primal feasibility: ${\bf x}$ must be inside the feasible region;
\item Dual feasibility: all multipliers $\mu_j$ for the inequalities 
  must take the proper sign as specified by Table \ref{PolarityTable};
\item Complementarity: one of $\mu_j^*$ and $g_j({\bf x}^*)$ must be 
  zero for the following two possible cases:
  \begin{itemize}
  \item the jth constraint is inactive, $g_j({\bf x}^*)\ne 0$, $\mu_j^*=0$;
  \item the jth constraint is active if $\mu_j^*\ne 0$, $g_j({\bf x}^*)=0$.
  \end{itemize}
\end{itemize}

% https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture7.pdf
% https://www.cs.cmu.edu/~ggordon/10725-F12/slides/16-kkt.pdf
% https://www.princeton.edu/~chiangm/optimization.pdf
% https://web.stanford.edu/class/ee364a/lectures/duality.pdf

% https://web.stanford.edu/class/ee364a/lectures/duality.pdf



\subsection*{Linear Programming (LP)}

The basic problem in linear programming (LP) is to maximize a linear 
objective function $f(x_1,\cdots,x_N)=\sum_{i=1}^N c_ix_i$ of $N$ variables
$x_1,\cdots,x_N$ (or minimize $f'(x_1,\cdots,x_N)=-\sum_{i=1}^N c_ix_i$), 
under a set of $M$ linear constraints. 
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize}    & 
    f(x_1,\cdots,x_N)=\sum_{i=1}^N c_ix_i \\
    \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l} 
      \sum_{i=1}^n a_{1i}x_i\le b_1\\
      \cdots \cdots \cdots \cdots\\
      \sum_{i=1}^N a_{Mi}x_i \le b_M\\
      x_1\ge 0,\cdots,x_N\ge 0
    \end{array} \right.
  \end{array}
\end{equation}
For example, the objective function could be the total profit, the 
constraints could be some limited resources. Let $x_1,\cdots,x_N$ represent
the quantities of $N$ different types of products, $c_1,\cdots,c_N$ represent 
their unit prices, and $a_{ij}$ represent the consumption of the ith resource 
of quantity $b_i$ by the jth product, the goal is to maximize the profit 
$f(x_1,\cdots,x_N)$ subject to the constraints imposed by the limited 
resources $b_1,\cdots,b_M$.

Here all variables are assumed to be non-negative. If there exists a
variable that is not restricted, it can be eliminated. 
% it can be replaced by $x'_j-x''_j$ with $x'_j\ge 0$ and $x''_j\ge 0$. 
Specifically, we can solve one of the constraining equations for the
variable and use the resulting expression of the variable to replace
all its appearances in the problem. For example:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize}    & f(x,y,z)=2x-y+3z \\
    \mbox{subject to:} & 
    \left\{\begin{array}{l} x-2y+z=3\\3x-y+4z=10\\
    y \ge 0,\;\; z\ge 0\end{array}\right.\\
  \end{array}
\end{equation}
Solving the first constraining equation for the free variable $x$, 
we get $x=3+2y-z$. Substituting this into the objective function and 
the other constraint, we get $2x-y+z=3y+z+6$ and $5y+z=1$, and the
problem can be reformulated as:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize}    & f(y,z)=3y+z+6 \\
    \mbox{subject to:} & 
    \left\{\begin{array}{l} 5y+z=1 \\
    y \ge 0,\;\; z\ge 0\end{array}\right.\\
  \end{array}
\end{equation}

The LP problem can be more concisely represented in matrix form:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize} & {\bf c}^T{\bf x}\\
    \mbox{subject to} & \left\{\begin{array}{l}
         {\bf Ax}-{\bf b}\le {\bf 0}\\
         {\bf x}\ge {\bf 0}\end{array}\right.\\
  \end{array}
\end{equation}
where
\begin{equation}
{\bf x}=\left[\begin{array}{c}x_1\\\vdots\\x_N\end{array}\right]
\;\;\;\;\;\;
{\bf c}=\left[\begin{array}{c}c_1\\\vdots\\c_N\end{array}\right]
\;\;\;\;\;\;
{\bf b}=\left[\begin{array}{c}b_1\\\vdots\\b_M\end{array}\right]
\;\;\;\;\;\;
{\bf A}=\left[\begin{array}{ccc}a_{11}&\cdots&a_{1N}\\
\vdots & \ddots & \vdots \\a_{M1}&\cdots&a_{MN}\end{array}\right]
\end{equation}
and the less or greater than signs in a vector inequality is understood
as an element-wise relationship between to the corresponding elements of
the vectors on the two sides.

Given the LP problem above, now called the {\em primal problem}, we 
can find its {\em dual problem} as discussed before for the general 
constrained optimization problems. We first construct the Lagrangian
of primal problem:
\begin{equation}
  L({\bf x})={\bf c}^T{\bf x}-{\bf y}^T({\bf Ax}-{\bf b})
  =({\bf c}-{\bf A}^T{\bf y})^T{\bf x}+{\bf y}^T{\bf b}
\end{equation}
Here ${\bf y}=[y_1,\cdots,y_M]^T$ contains the Lagrange multipliers 
for the inequality constraints ${\bf Ax}-{\bf b}\le{\bf 0}$, and
${\bf y}\ge{\bf 0}$ according to Table \ref{PolarityTable} given
before. We define the dual function as the maximum of $L({\bf x})$ 
over ${\bf x}$:
\begin{equation}
  f_d({\bf y})=\max_{\bf x} L({\bf x})
  =\max_{\bf x} [ {\bf c}^T{\bf x}-{\bf y}^T({\bf Ax}-{\bf b}) ]
  =\max_{\bf x} [ ({\bf c}-{\bf A}^T{\bf y})^T{\bf x}+{\bf y}^T{\bf b} ]
\end{equation}
To find the value of ${\bf x}$ that maximizes $L({\bf x})$, we set 
its gradient to zero:
\begin{equation}
  \bigtriangledown_{\bf x}L({\bf x})
  =\bigtriangledown_{\bf x}[ ({\bf c}-{\bf A}^T{\bf y})^T{\bf x}+{\bf y}^T{\bf b}]
  ={\bf c}-{\bf A}^T{\bf y}={\bf 0}
\end{equation}
and substitute it into $L_d({\bf y})=\max_{\bf x}L({\bf x})$ to
get: $f_d({\bf y})={\bf y}^T{\bf b}$, which is the maximum of
$L({\bf x},{\bf y})$, if ${\bf c}-{\bf A}^T{\bf y}\le 0$ so that
the term $({\bf c}-{\bf A}^T{\bf y})^T{\bf x}\le 0$ contributes 
negatively to $L({\bf x})$ (as ${\bf x}\ge 0$). Now $f_d({\bf y})$
is an upper bound of $f_p({\bf x})$:
\begin{equation}
f_d({\bf y})={\bf b}^T{\bf y}\ge L({\bf x})
={\bf c}^T{\bf x}-{\bf y}^T({\bf Ax}-{\bf b}) 
\ge{\bf c}^T{\bf x}=f_p({\bf x})
\end{equation} 
We obtain the tightest upper bound by solving the following dual problem: 
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize} & f_d({\bf y})={\bf b}^T{\bf y}\\
    \mbox{subject to} & \left\{\begin{array}{l}
         {\bf A}^T{\bf y}-{\bf c}\ge {\bf 0}\\
         {\bf y}\ge {\bf 0}\end{array}\right.\\
  \end{array}
\end{equation}
Following the same approach, we can also find the dual of a minimization
primal problem. Moreover, we can also show that the dual of a dual problem
is the original primal problem. Such duality relationships are summarized
below:
\begin{equation}
\left\{\begin{array}{ll}\mbox{max}: & {\bf c}^T{\bf x}\\
\mbox{s.t.} & {\bf Ax}-{\bf b}\le{\bf 0},\;\;{\bf x}\ge{\bf 0}\end{array}\right.
\Longleftrightarrow
\left\{\begin{array}{ll}\mbox{min} & {\bf b}^T{\bf y}\\
\mbox{s.t.} & {\bf Ay}-{\bf c}\ge{\bf 0},\;\;{\bf y}\ge{\bf 0}\end{array}\right.
\end{equation}
\begin{equation}
\left\{\begin{array}{ll}\mbox{min}: & {\bf c}^T{\bf x}\\
\mbox{s.t.} & {\bf Ax}-{\bf b}\le{\bf 0},\;\;{\bf x}\ge{\bf 0}\end{array}\right.
\Longleftrightarrow
\left\{\begin{array}{ll}\mbox{max} & {\bf b}^T{\bf y}\\
\mbox{s.t.} & {\bf Ay}-{\bf c}\le{\bf 0},\;\;{\bf y}\le{\bf 0}\end{array}\right.
\Longleftrightarrow
\left\{\begin{array}{ll}\mbox{max} & -{\bf b}^T{\bf y}\\
\mbox{s.t.} & {\bf Ay}+{\bf c}\ge{\bf 0},\;\;{\bf y}\ge{\bf 0}\end{array}\right.
\end{equation}

% http://www.seas.ucla.edu/~vandenbe/ee236a/lectures/duality.pdf

If either the primal or the
dual is feasible and bounded, so is the other, and they form a
strong duality, the solution $d^*$ of the dual problem is the same
as the solution $p^*$ of the primal problem. Also, as the primal and
dual problems are completely symmetric, 



Similarly, we can also get the dual problem of an equality constrained 
standard LP problem:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize} & {\bf b}^T{\bf y}\\
    \mbox{subject to} & {\bf A}^T{\bf y}-{\bf c}\ge {\bf 0}\\
  \end{array}
\end{equation}
Note that the condition ${\bf y}\ge {\bf 0}$ for the multipliers 
is dropped, due to the fact that the primal standard LP problem 
is equality constrained (instead of inequality constrained). The 
inequality constraints in the dual problem above can be further 
converted to equality constraints by introducing a set of 
{\em dual slack variables} ${\bf s}\ge {\bf 0}$:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize} & {\bf b}^T{\bf y}\\
    \mbox{subject to} & \left\{\begin{array}{l}
         {\bf A}^T{\bf y}-{\bf s}={\bf c}\\
         {\bf s}\ge {\bf 0}\end{array}\right.\\
  \end{array}
\end{equation}

As ${\bf c}\le {\bf A}^T{\bf y}$ and ${\bf b}\ge{\bf Ax}$, we have the 
following
\begin{equation}
{\bf c}^T{\bf x}\le({\bf A}^T{\bf y})^T{\bf x}
={\bf y}^T{\bf Ax}={\bf x}^T{\bf A}^T{\bf y}
=({\bf Ax})^T{\bf y}\le{\bf b}^T{\bf y}
\end{equation}
therefore we get the dual gap ${\bf b}^T{\bf y}\ge {\bf c}^T{\bf x}$.



The inequality constrained LP problem can be converted to an equality
constrained problem by introducing {\em slack variables}:
\begin{equation}
  \sum_{i=1}^N a_ix_i\le b \;\;\;\Longrightarrow\;\;\;
  \sum_{i=1}^N a_ix_i + s = b,\;\;\;(s\ge 0)
\end{equation}
Now the inequality constrained LP problem can be reformulated to become 
an equality constrained problem, called the {\em standard form}:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize}    & z=f({\bf x})={\bf c}^T{\bf x}=\sum_{j=1}^N c_jx_j\\
    \mbox{subject to:} & 
    \left\{\begin{array}{llllll}
    \sum_{i=1}^N a_{1i}x_i&+s_1& & & & =b_1\\
    \sum_{i=1}^N a_{2i}x_i& &+s_2& & & =b_2\\
    \cdots & \cdots & \cdots &\cdots & \cdots &\cdots \\
    \sum_{i=1}^N a_{Mi}x_i & & & & +s_M& =b_M\\
    x_1\ge 0,&\cdots,& x_N\ge 0, &s_1\ge 0,&\cdots& s_M\ge 0\end{array}\right.
  \end{array}
\end{equation}
We further redefine the following:
\begin{itemize}
\item ${\bf x}$ is an $N+M$ dimensional augmented variable vector that
  includes the $M$ slack variables $\{s_1,\cdots,s_M\}$ as well as the 
  $N$ original variables $\{x_1,\cdots,x_N\}$:
  \begin{equation}
  {\bf x}=[x_1,\;x_2,\cdots,x_N,\,s_1,\;s_2,\cdots,s_M]^T
  \end{equation}
\item ${\bf A}$ is redefined as an $M\times (N+M)$ augmented coefficient 
  matrix that includes coefficients for both types of variables:
  \begin{equation}
    {\bf A}=\left[\begin{array}{cccc|cccc}
        a_{11} & a_{12} & \cdots & a_{1N} & 1 & 0 & \cdots & 0\\
        a_{21} & a_{22} & \cdots & a_{1N} & 0 & 1 & \cdots & 0\\
        \vdots&\vdots & \ddots &\vdots &\vdots & \vdots & \ddots & \vdots\\
        a_{M1} & a_{M2} & \cdots & a_{MN} & 0 & \cdots & 0 & 1\\
      \end{array}\right]_{M\times (N+M)}
    =[\;{\bf A}_{M\times N}\;|\; {\bf I}_{M\times M}\;]
  \end{equation}
  where ${\bf A}_{M\times N}$ are the coefficients for the $N$ 
  original variables $\{x_1,\cdots,x_N\}$, and the identity matrix 
  ${\bf I}_{M\times M}$ is for the unity coefficients of the $M$ slack
  variables $\{s_1,\cdots,s_M\}$, each of which appears in the $M$ 
  constraints only once.
\end{itemize}

Now the LP problem can be expressed in the standard form (original form 
on the left, with ${\bf x}$ and ${\bf A}$ redefined on the right):
\begin{equation} 
  \begin{array}{ll}
    \mbox{maximize} & {\bf c}^T{\bf x}\\
    \mbox{subject to} &
    \left\{ \begin{array}{l}
      {\bf A}{\bf x}+{\bf s}={\bf b}\\
      {\bf x}\ge{\bf 0},\;{\bf s}\ge{\bf 0}
    \end{array}\right.
  \end{array}
  \;\;\;\;\;\;\;\;\mbox{or}\;\;\;\;\;\;
  \begin{array}{ll}
    \mbox{maximize} & {\bf c}^T{\bf x}\\
    \mbox{subject to} &
    \left\{ \begin{array}{l}
      {\bf A}{\bf x}={\bf b}\\
      {\bf x}\ge{\bf 0}
    \end{array}\right.
  \end{array}
\end{equation}



An LP problem can be viewed geometrically. We can normalize 
${\bf c}$ so that $||{\bf c}||=1$, so that the objective function 
$f({\bf x})={\bf c}^T{\bf x}$ becomes the projection of ${\bf x}$ 
onto vector ${\bf c}$. Each of the $M$ inequality constraints in 
${\bf A}{\bf x}\le{\bf b}$ corresponds to a hyper-plane in the N-D 
vector space perpendicular to its normal direction 
${\bf a}_j=[a_{i1},\cdots,a_{IN}]^T$:
\begin{equation}
  \sum_{j=1}^N a_{ij}x_j={\bf a}_j^T{\bf x}=b_i,\;\;\;\;\;\;\;(i=1,\cdots,M)
\end{equation}
Also, each of the $N$ non-negativity conditions $x_j\ge 0$ in
${\bf x}\ge{\bf 0}$ corresponds to a hyper-plane perpendicular to the 
jth standard basis vector ${\bf e}_j$. In the most general case (none 
of the hyper-planes is parallel to any others, no more than $N$ 
hyper-planes intersect at one point), every $N$ of these $N+M$ 
hyper-planes intersect at a point in the N-D space, and they form
\begin{equation}
  C_{M+N}^N=\frac{(M+N)!}{N!\;M!}
\end{equation}
such intersection points in total.
The polytope enclosed by these $N+M$ hyper-planes is called the 
{\em feasible region}, in which the optimal solution must lie. 
The vertices of the polytope of the feasible region are a subset 
of the $C_{M+N}^N$ intersections at which all constraints in 
${\bf A}{\bf x}\le{\bf b}$ and ${\bf x}\ge{\bf 0}$ are satisfied.

{\bf Fundamental theorem of linear programming}

The optimal solution ${\bf x}^*$ of a linear programming problem formulated 
above is either a vertex of the polytope feasible region $P$, or lies on a 
hyper-surface of the polytope, on which all points all optimal solutions.

\htmladdimg{figures/LPtheorem.png}

{\bf Proof:}

Assume the optimal solution ${\bf x}^*\in P$ is interior to the polytope
feasible region $P$, then there must exist some $\epsilon>0$ such that 
the hyper-sphere of radius $\epsilon$ centered at ${\bf x}^*$ is inside 
$P$. Evaluate the objective function at a point on the hyper-sphere
\begin{equation}
{\bf x}'={\bf x}^*+\epsilon\;{\bf c}/||{\bf c}||\in P
\end{equation}
we get
\begin{equation}
f({\bf x}')={\bf c}^T{\bf x}'
={\bf c}^T({\bf x}^*+\epsilon\;{\bf c}/||{\bf c}||)
={\bf c}^T{\bf x}^*+\epsilon\;{\bf c}^T{\bf c}/||{\bf c}||
={\bf c}^T{\bf x}^*+\epsilon\;||{\bf c}||>{\bf c}^T{\bf x}^*=f({\bf x}^*)
\end{equation}
i.e., ${\bf x}^*$ cannot be the optimal solution as assumed. This contradiction
indicates that an optimal solution ${\bf x}^*$ must be on the surface of $P$,
either at one of its vertices or on one of its surfaces. Q.E.D.

Based on this theorem, the optimization of a linear programming problem 
could be solved by exhaustively checking each of the $C_{N+M}^N$ intersections
formed by $N$ of the $N+M$ hyper-planes to find (a) whether it is feasible 
(satisfying all $N+M$ constraints), and, if so, (b) whether the objective 
function $f({\bf x})$ is maximized at the point. Specifically, we choose 
$N$ of the $N+M$ equations for the the constraints and solve this N-equation 
and N-unknown linear system to get the intersection point of $N$ corresponding
hyper-planes. This brute-force method is most straight forward, but the 
computational complexity is high when $N$ and $M$, and therefore $C_{N+M}^N$, 
become large.

{\bf Example:}

\begin{equation}
  \begin{array}{ll}
    \mbox{maximize}    & f_p(x_1,x_2)=2x_1+3x_2 \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}
      2x_1+ x_2 \le 18\\ 6x_1+5x_2 \le 60 \\ 2x_1+5x_2 \le 40 \\ x_1 \ge 0,\;\; x_2\ge 0
    \end{array}\right.
  \end{array}
  \;\;\;\;\;\;\mbox{or}\;\;\;\;\;\;
  \begin{array}{ll}
    \mbox{maximize}    & f_p({\bf x})={\bf c}^T{\bf x} \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}{\bf A}{\bf x}\le{\bf b}\\{\bf x}\ge {\bf 0}
    \end{array}\right.
  \end{array}
  \nonumber
\end{equation}
where
\begin{equation} 
  {\bf x}=\left[\begin{array}{c}x_1\\x_2\end{array}\right],\;\;\;\;
  {\bf c}=\left[\begin{array}{c}2\\3\end{array}\right],\;\;\;\;
  {\bf b}=\left[\begin{array}{c}18\\60\\40\end{array}\right],\;\;\;\;
  {\bf A}=\left[\begin{array}{cc}2&1\\6&5\\2&5\end{array}\right]
  \nonumber
\end{equation}

\htmladdimg{figures/SimplexEx0.png}

This problem has $N=2$ variables with the same number of non-negativity
constraints and $M=3$ linear inequality constraints. The $n+m=5$ straight 
lines form $C_{N+M}^N=C_5^2=10$ intersections, out of which 5 are the 
vertices of the polygonal feasible region satisfying all constraints. 
The value of the objective function $f_p(x_1,x_2)={\bf c}^T{\bf x}=c_1x_1+c_2x_2$ 
is proportional to the projection of the 2-D variable vector 
${\bf x}=[x_1,\;x_2]^T$ onto the coefficient vector ${\bf c}=[2,\;3]^T$. 
The goal here is to find a point in the polygonal feasible region with 
maximum projection onto vector ${\bf c}$. 

The $C_5^2=10$ intersections are listed in the table below together with
their feasibility (whether a vertex of the polygon or not) and the 
corresponding objective function value (proportional to the projection
in the parentheses). We see that out of the 5 feasible solutions at the
vertices of the polygon, the one at ${\bf x}^*=(5,\;6)$ is optimal with 
maximum objective function value $f_p(x_1,x_2)=2x_1+3x_2=2\times 5+3\times6=28$ 
(proportional to the projection ${\bf c}^T{\bf x}^*/||{\bf c}||=28/3.61=7.77$. 

\begin{equation}
  \begin{array}{c|c|c|c}\hline
    & (x_1,\; x_2) &  & \mbox{objective function (normalized)} \\\hline
    1 & 5.00,\; 6.00 & \mbox{feasible}   & 28\; (7.77) \\
    2 & 6.25,\; 5.50 & \mbox{infeasible} & 29\; (8.04) \\
    3 & 7.50,\; 3.00 & \mbox{feasible}   & 24\; (6.66) \\
    4 &20.00,\; 0.00 & \mbox{infeasible} & 40\; (11.1) \\
    5 &10.00,\; 0.00 & \mbox{infeasible} & 20\; (5.55) \\
    6 & 9.00,\; 0.00 & \mbox{feasible}   & 18\; (4.10) \\
    7 & 0.00,\; 8.00 & \mbox{feasible}   & 24\; (6.66) \\
    8 & 0.00,\; 12.00 & \mbox{infeasible}& 36\; (9.98) \\
    9 & 0.00,\; 18.00 & \mbox{infeasible}& 54\; (14.99) \\
    10& 0.00,\; 0.00 & \mbox{feasible }  &  0\; (0.00) \\\hline
  \end{array}
  \nonumber
\end{equation}

This problem can be converted to the standard form:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize}    & f_p(x_1,x_2)=2x_1+3x_2 \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}
      2x_1+ x_2 \le 18\\ 6x_1+5x_2 \le 60 \\ 2x_1+5x_2 \le 40 \\
      x_1 \ge 0,\;\; x_2\ge 0
    \end{array}\right.
  \end{array}
  \;\;\;\;\;\;\Longrightarrow\;\;\;\;\;\;
  \begin{array}{ll}
    \mbox{maximize}    & f_p(x_1,x_2)=2x_1+3x_2 \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}
      2x_1+ x_2+s_1=18\\ 6x_1+5x_2+s_2=60 \\ 2x_1+5x_2+s_3=40 \\ 
      x_1 \ge 0,\; x_2\ge 0,\; s_1\ge 0,\; s_2\ge 0,\; s_3\ge0
    \end{array}\right.
  \end{array}
  \nonumber
\end{equation}

The goal of this linear programming problem can be intuitively understood 
as to push the hyper-plane ${\bf c}^T{\bf x}=z$, here a line in 2-D space, 
along its normal direction ${\bf c}$, from the origin to as far away as 
possible inside the feasible region, to eventually arrive at one of the 
vertices of the polytope feasible region that is farthest away from the 
origin.

We can also consider the dual problem

\begin{equation}
  \begin{array}{ll}
    \mbox{minimize}    & f_d(y_1,y_2,y_3)=18y_1+60y_2+40y_3 \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}
      2y_1+6y_2+2y_3\ge 2 \\ y_1+5y_2+5y_3\ge 3\\
      y_1\ge 0,\;y_2\ge 0,\;y_3\ge 0\\
    \end{array}\right.
  \end{array}
  \;\;\;\;\;\;\mbox{or}\;\;\;\;\;\;
  \begin{array}{ll}
    \mbox{minimize}    & f_d({\bf y})={\bf b}^T{\bf y} \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}{\bf A}^T{\bf y}\le{\bf b}\\{\bf y}\ge {\bf 0}
    \end{array}\right.
  \end{array}
  \nonumber
\end{equation}

The five planes, $m=2$ from the constraining inequalities, and $n=3$
for the non-negative constraints $y_1\le 0$, $y_2\le 0$, and $y_3\le 0$
for the three variables, form $10$ intersection points:
\begin{equation}
  \begin{array}{c|c|c|c}\hline
    & (y_1,\,y_2,\,y_3)  & \mbox{objective function} \\\hline
    1 & (1,0,0) & \mbox{infeasible}   & 18\\
    2 & (3,0,0) & \mbox{feasible}   & 54\\
    3 & (0,1/3,0) & \mbox{infeasible}   & 20\\
    4 & (0,3/5,0) & \mbox{feasible}   & 36\\
    5 & (0,0,1) & \mbox{feasible}   & 40\\
    6 & (0,0,3/5) & \mbox{infeasible}   & 24\\
    7 & (0,1/5,2/5) & \mbox{feasible}   & 28\\
    8 & (1/2,0,1/2) & \mbox{feasible}   & 29\\
    9 & (-2,1,0) & \mbox{infeasible}   & 24\\
    10 & (0,0,0) & \mbox{infeasible}   & 0\\\hline
  \end{array}
  \nonumber
\end{equation}

We see that at the feasible point $(0,1/5,2/5)$, the dual function 
$f_d(y_1,y_2,y_3)=18y_1+60y_2+40y_3$ is minimized to be $28$, the same 
as the maximum of the primal function $f_p(x_1,x_2)=2x_1+3x_2$ at $(5,6)$.

\htmladdimg{figures/LPdual1.png}

{\bf Homework:} 

Develop the code to find all $C_{N+M}^N$ intersections formed by $N+M$ 
given hyper-planes in an N-D space in terms of their equations
${\bf A}_{M\times N}{\bf x}_{N\times 1}\le{\bf b}_{M\times 1}$ and 
${\bf x}\ge {\bf 0}$, identify which of them are vertices of the polytope 
surrounded by these hype-planes, and find the vertex corresponding to the 
optimal solution that maximizes $f({\bf x})={\bf c}^T{\bf x}$ for a given 
${\bf c}$.

% {\bf Duality}
% http://theory.stanford.edu/~trevisan/cs261/lecture06.pdf



\subsection*{The Simplex Algorithm}

Instead of exhausting all vertices of the feasible region to find
the optimal solution, the simplex algorithm is an iterative process 
that traverses along a sequence of edges of the polytopic feasible 
region, starting at the origin and through a sequence of vertices 
${\bf x}$ with progressively greater objective value $f({\bf x})$,
until eventually reaching the optimal solution.

To understand the algorithm, we first consider the equality constraint 
${\bf Ax}=[{\bf A}_{M\times N}\;|\;{\bf I}_{M\times M}] {\bf x}={\bf b}$ 
of the standard LP problem. This is an under-determined linear system
of $N+M$ variables but only $M$ equations, only $M$ of its $N+M$ 
columns of the coefficient matrix ${\bf A}$ are independent (assuming 
$rank({\bf A})=M$). Initially we choose the $M$ columns of the 
identity coefficient matrix ${\bf I}$ as the independent columns, but
subsequently we can choose any other $M$ columns of ${\bf A}$ as the 
independent columns and convert them into a standard basis vector by 
\htmladdnormallink{Gauss-Jordan elimination}{../ch0/node5.html},
together with the corresponding variables in ${\bf x}$. The resulting
equation ${\bf Ax}={\bf b}$ remains equivalent to the original one,
i.e., the $M$ equality constraints are always preserved and never 
violated in the process.

For convenience of discussion and without loss of generality, we could
reorder (not actually carried out) the $N+M$ columns in ${\bf A}$, 
together with the corresponding variables in ${\bf x}$, so that the 
constraining equation would always takes the following form:
\begin{equation}
  {\bf A}{\bf x}=[{\bf A}_n\;\;{\bf I}]
  \left[\begin{array}{c}{\bf x}_n\\{\bf x}_b\end{array}\right]
  ={\bf A}_n{\bf x}_n+{\bf I}\;{\bf x}_b={\bf b}
\end{equation}
Now the variables in the M-D vector ${\bf x}_b$ corresponding to the 
$M$ independent columns in ${\bf I}$ are the {\em basic variables}, 
while the N-D vector ${\bf x}_n$ corresponding to the remaining $N$ 
dependent columns, now denoted by ${\bf A}_n$, are the {\em non-basic 
variables}. This equation will always hold if ${\bf x}_n={\bf 0}$ and
${\bf x}_b={\bf b}$. Such a solution is called a {\em basic solution} 
of the linear system ${\bf A}{\bf x}={\bf b}$:
\begin{equation}
  {\bf x}=\left[\begin{array}{c}{\bf x}_n\\{\bf x}_b\end{array}\right]
  =\left[\begin{array}{c}{\bf 0}\\{\bf b}\end{array}\right]
\end{equation}
Initially, ${\bf A}_n$ is the coefficient matrix in the inequality 
constraint ${\bf Ax}\le {\bf b}$, and the corresponding non-basic
variables ${\bf x}_n={\bf 0}$ is actually the origin. But through
the iteration, we will keep converting some other columns of ${\bf A}$
into standard basis vectors $\{{\bf e}_1,\cdots,{\bf e}_M\}$, the 
column vectors of ${\bf I}$ by Gauss-Jordan elimination. The variables
corresponding to the new ${\bf I}$ become the basic variables, while
those corresponding to columns that become non-standard basis vectors 
become non-basis variables.

The basic solutions corresponding the $C_{N+M}^M$ ways to choose $M$ 
of the $N+M$ columns of ${\bf A}$ as independent are actually the
intersections formed by any $N$ of the $N+M$ constraining hyper-planes 
in the N-D space. As noted before, only a subset of these $C_{N+M}^M$ 
basic solutions satisfy the $M$ constraints, and they are called 
{\em basic feasible solutions (BFS)}. The goal of the iterative 
process in the simplex algorithm is to find the optimal basic feasible 
solution that maximizes $f({\bf x})$ without exhausting all $C_{N+M}^M$ 
possibilities. This is done by selecting the columns in such a way 
that the value of the objective function will always be maximally 
increased, until eventually we find the optimal solution ${\bf x}^*$, 
the vertex of the polytopic feasible region at which $f({\bf x}^*)$ 
is maximized.

Specially, the implementation of the simplex algorithm is based on a 
tableau with $N+M+1$ columns and $M+1$ rows, initialized as below:
\begin{itemize}
\item The first $M$ rows are for the coefficients in the equality
  constraints $\sum_{j=1}^N a_{ij}x_j+s_i=b_i$ ($i=1,\cdots,M$). An
  additional $(M+1)st$ row at the bottom is for the coefficients
  in the objective function $f({\bf x})=\sum_{j=1}^N c_jx_j$, which
  is zero initially at the origin $x_1=\cdots=x_N=0$, i.e., 
  $z-c_1x_1-\cdots-c_Nx_N=0$.
\item The first $N+M$ columns contain the coefficients for the $N$ 
  original variables $\{x_1,\cdots,x_N\}$ and the $M$ slack variables 
  $\{s_1,\cdots,s_M\}$, and an additional $(N+M+1)st$ column is for 
  the constants ${\bf b}=[b_1,\cdots,b_M]^T$ on the right-hand side of
  the $M$ constraint equations. The last element of the column is the 
  right-hand side of the objective equation $z=f({\bf x})$, which is 
  initially zero.
\end{itemize}

\begin{equation}
  \begin{array}{c||cccc|cccc||c}\hline
    \mbox{basic variables} & x_1   & x_2   & \cdots &  x_N   & s_1 & s_2 & \cdots & s_M & \mbox{basic solution}   \\\hline
    s_1   & a_{11} & a_{12} & \cdots & a_{1N}  & 1    &  0   & \cdots & 0    & b_1  \\
    s_2   & a_{21} & a_{22} & \cdots & a_{2N}  & 0    &  1   & \cdots & 0    & b_2  \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
    s_M  & a_{M1} & a_{M2} & \cdots & a_{MN}  & 0    &  0   & \cdots & 1    & b_M   \\\hline
    z   & -c_1  & -c_2  & \cdots &  -c_M  & 0   &  0   & \cdots & 0  & 0\\\hline
  \end{array}
\end{equation}

At this initial stage of the iteration, the $N$ original variables 
${\bf x}_n=[x_1,\cdots,x_N]^T$ are the non-basic variables, while the 
$M$ slack variables ${\bf x}_b=[s_1,\cdots,s_M]^T$ are the basic variables 
and their coefficients form an $M\times M$ identity matrix ${\bf I}$, 
composed of the standard basis vectors. The corresponding feasible basic 
solution is simply
\begin{equation}
  {\bf x}_b=[s_1,\cdots,s_M]^T={\bf b},\;\;\;\;\; 
  {\bf x}_n=[x_1,\cdots,x_N]^T={\bf 0}
\end{equation}
that satisfies all the constraints
${\bf Ax}={\bf A}_n{\bf x}_n+{\bf I}{\bf x}_b={\bf b}$. 

In each of the subsequent iterations, we will select one of the non-basic
variables, called the {\em entering variable}, to replace one of the basic
variables, called the {\em leaving variable}, in such a way that the value
of the objective function value $z=f({\bf x})$ in the last row will be 
maximally increased, while all constraints remain satisfied. Here are the 
steps in each iteration:

\begin{itemize}
\item {\bf Selection of the entering variable}

  This section is based on the maximization of $z=f({\bf x})$. We select 
  $x_j$ in the jth column of the tableau if $-c_j$ is most negative, i.e.,
  $x_j$ is most heavily weighted by $c_j=\max\{c_1,\cdots,c_N\}$, so that 
  it will increase $z=\sum_{j=1}^N c_jx_j$ more than any other $x_k\;\;(k\ne j)$. 

\item {\bf Selection of the leaving variable}

  This selection is based on the constraints imposed on the selected 
  entering variable $x_j$. In general, the restriction on $x_j$ set by 
  the kth constraint $\sum_{j=1}^N a_{kj}x_j\le b_k$ is $x_j\le b_k/a_{kj}$
  when $a_{kj}>0$. If $b_i/a_{ij} \le b_k/a_{kj}$ for all $k=1,\cdots,M$,
  then the ith constraint is most restrictive on $x_j$, we will therefore
  select the corresponding basic variable $s_i$ as the leaving variable,
  i.e., it becomes a non-basic variable to be set to zero, so that $x_j$
  can be maximally increased without violating the constraints. If all 
  $a_{kj}<0$ for all $k=1,\cdots,M$, variable $x_j$ is not bounded.

\item {\bf Gauss-Jordan elimination based on Pivoting}
  
  To convert the entering variable $x_j$ to a basic variable to replace the
  leaving variable $s_i$, we need to turn the corresponding jth column into a
  standard basis vector ${\bf e}_i$. This is realized by pivoting on $a_{ij}$
  in the following steps:
  \begin{enumerate}
  \item Divide the ith row by $a_{ij}$: ${\bf r}_i\leftarrow{\bf r}_i/a_{ij}$.
    Now $a_{ij}=1$.
  \item Subtract $a_{kj}$ times the ith row from the kth row:
    ${\bf r}_k\leftarrow{\bf r}_k-a_{kj}{\bf r}_i$. 
    Now $a_{kj}=0$ for all $k=1,\cdots,M$, $k\ne i$.
  \end{enumerate}
  Now the jth column corresponding to the entering variable $x_j$ becomes
  a standard basis vector ${\bf e}_i$, i.e., $x_j$ becomes a basic variable 
  that takes the value of $b_i$, and the column corresponding to the leaving 
  variable $s_i$ is no longer a standard basis vector, and $s_i=0$ as it is
  now a non-basic variable.

\end{itemize}

As Gauss-Jordan elimination converts the linear equations to a set of 
equivalent equations, the constraints remain satisfied through out the 
process. Although the membership of the basic and non-basic variable 
groups keeps changing in the iteration, so long as ${\bf x}_b={\bf b}$ 
and ${\bf x}_n={\bf 0}$, the $M$ constraint equations 
${\bf Ax}={\bf A}_n{\bf x}_n+{\bf I}{\bf x}_b={\bf b}$ always holds.

This iterative process keeps replacing the slack variables 
$\{s_1,\cdots,s_M\}$ (the basic variables initially) by the original 
variables $\{x_1,\cdots,x_N\}$ (the non-basic variables initially), 
one at a time, until all elements in the last row are non-negative.

The final result can be read out directly from the tableau. The variables 
corresponding to the $M$ standard basis vectors ${\bf e}_i$ ($i=1,\cdots,M$) 
are the final basic variables that take the values in ${\bf b}$ in the
right-most column of the tableau. They form the optimal basic solution, 
with the maximum of the objective function $z=f({\bf x})$ given by the last 
element also in the last column. The remaining $n$ variables corresponding 
to non-standard columns are non-basic variables that take the value zero. 
When $M>N$ (more constraints than variables), some of the slack variables 
may remain in the basic variable group taking non-zero values; when $M<N$, 
some of the original variable may be in the non-basic group taking the value
zero.

{\bf Example:}

There are $C_{M+N}^M=C_{10}^3=10$ basic solutions as listed below, 
out of which five are basic feasible solutions with all $M+N=5$
variables taking non-negative values. The remaining solutions 
are not feasible, as some of the variables take negative values.

\begin{equation}
  \begin{array}{c|ccccc|c|c}\hline
    & x_1 & x_2 & s_1 & s_2 & s_3 & & \mbox{objective function (normalized)} \\\hline
    1 & 5 & 6 & 2 & 0 & 0 & \mbox{feasible}   & 28\; (7.77) \\
    2 &6.25 & 5.5 & 0 & -5 & 0 & \mbox{infeasible} & 29\; (8.04) \\
    3 & 7.5 & 3 & 0 & 0 & 10 & \mbox{feasible}   & 24\; (6.66) \\
    4 &20 & 0 & -2 & -60 & 20  & \mbox{infeasible} & 40\; (11.1) \\
    5 &10 & 0 & -2 & 0 & 20 & \mbox{infeasible} & 20\; (5.55) \\
    6 & 9 & 0 & 0 & 6 & 22 & \mbox{feasible}   & 18\; (4.10) \\
    7 & 0 & 8 & 10 & 20 & 0 & \mbox{feasible}   & 24\; (6.66) \\
    8 & 0 & 12 & 6 & 0 & -20 & \mbox{infeasible} & 36\; (9.98) \\
    9 & 0 & 18 & 0 & -30 & -50 & \mbox{infeasible} & 54\; (14.99) \\
    10& 0 & 0 & 18 & 60 & 40 & \mbox{feasible}   &  0\; (0.00) \\\hline
  \end{array}
  \nonumber
\end{equation}

{\bf Example:}

We re-solve the LP problem considered in the previous examples, now 
in the standard form:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize}    & f({\bf x})=2x_1+3x_2 \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}
      2x_1+ x_2+s_1=18\\ 6x_1+5x_2+s_2=60 \\ 2x_1+5x_2+s_3=40 \\ 
      x_1 \ge 0,\; x_2\ge 0,\; s_1\ge 0,\; s_2\ge 0,\; s_3\ge0
    \end{array}\right.
  \end{array}
  \nonumber
\end{equation}
The standard form is further converted to a tableau as shown below. The 
left-most column indicates the basic variables, the next $N=2$ columns 
are for the variables $x$ and $y$,the next $M=3$ columns are for the slack 
variables $u$, $v$, and $w$, the right most column is for the constants 
$b_1$, $b_2$, and $b_3$ on the right-hand side of the equations.

\begin{equation}
  \begin{array}{c|cc|ccc|c}  \hline
    & x_1 & x_2 & s_1 & s_2 & s_3 & b  \\\hline
    s_1 & 2 & 1 & 1 & 0 & 0 & 18 \\
    s_2 & 6 & 5 & 0 & 1 & 0 & 60 \\
    s_3 & 2 & 5 & 0 & 0 & 1 & 40 \\\hline
    z &-2 &-3 & 0 & 0 & 0 & 0  \\\hline
  \end{array}
  \nonumber
\end{equation}

In this initial state, the basic variables are $s_1=18$, $s_2=60$, and 
$s_3=40$, the non-basic variables are $x_1=x_2=0$. The corresponding basic
feasible solution is at the origin.

\begin{itemize}
\item Select column $j=2$ as the pivot column, as $-c_2=-3$ is the most 
  negative coefficient.
\item Select row $i=3$ as the pivot row, as the ratio 
  $b_3/a_{32}=40/5=8$ is the minimum among all $M=3$ ratios $b_k/a_{k2}$
  ($18/1,\;60/5,\;40/5$).
\item Divide pivot row ${\bf r}_i={\bf r}_3=[2,\;5,\;0,\;0,\;1,\;40]$ by 
  the pivot element $a_{ij}=a_{32}=5$ to get ${\bf r}_3=[0.4,\;1,\;0,\;0,\;0.2,\;8]$.
\item Subtract $a_{kj}=a_{k2}$${\bf r}_k$ from row ${\bf r}_k$, so that 
  $a_{kj}=a_{k2}=0, \;(k=1,2)$.
\item Subtract $a_{4j}{\bf r}_i=a_{42}{\bf r}_3$ from ${\bf r}_4$ (last row).
\end{itemize}

\begin{equation}
  \begin{array}{c|cc|ccc|c}     \hline
    & x_1 & x_2 & s_1 & s_2 & s_3  &  b  \\\hline
    s_1 &1.6& 0 & 1 & 0 &-0.2& 10  \\
    s_2 & 4 & 0 & 0 & 1 &-1  & 20  \\
    x_2 &0.4&1.0& 0 & 0 & 0.2&  8  \\\hline
    z &-0.8& 0 & 0 & 0 &0.6& 24  \\\hline
  \end{array}
  \nonumber
\end{equation}

The entering variable $y$ becomes a basic variable to replace the 
leaving variable $w$ which becomes a non-basic variable. The corresponding
basic feasible solution is at $x_1=0$, $x_2=b_3=8$, with $s_1=10$, $s_2=20$,
$s_3=0$. The objective function value is $z=24$.

\begin{itemize}
\item Select column $j=1$ as the pivot column, as $-c_1=-0.8$ is the most 
  (only) negative coefficient.
\item Select row $i=2$ as the pivot row, as the ratio 
  $b_2/a_{21}=20/4=5$ is the minimum among all $M=3$ ratios $b_k/a_{k1}$
  ($10/1.6=6.25,\;20/4=5,\;8/0.4=20$).
\item Divide row ${\bf r}_i={\bf r}_2=[4,\;0,\;0,\;1,\;-1,\;20]$ by the pivot 
  element $a_{21}=4$ to get ${\bf r}_2=[1,\;0,\;0,\;0.25,\;-0.25,\;5]$.
\item Subtract $a_{k1}$${\bf r}_k$ from row ${\bf r}_k$, so that 
  $a_{k1}=0, (k=1,3)$.
\item Subtract $a_{4j}{\bf r}_i=a_{41}{\bf r}_4$ from ${\bf r}_4$ (last row).
\end{itemize}

\begin{equation}
  \begin{array}{c|cc|ccc|c}       \hline
    & x_1 & x_2 & s_1 & s_2  & s_3   & b   \\\hline
    s_1 & 0 & 0 & 1 &-0.4& 0.2 & 2   \\
    x_1 & 1 & 0 & 0 &0.25&-0.25& 5   \\
    x_2 & 0 & 1 & 0 &-0,1& 0.3 & 6   \\\hline
    z & 0 & 0 & 0 & 0.2& 0.4 & 28  \\\hline
  \end{array}
  \nonumber
\end{equation}

The entering variable $x$ becomes a basic variable to replace the leaving 
variable $v$ which becomes a non-basic variable. The corresponding basic 
feasible solution is at $x_1=b_2=5$ and $y_2=b_3=6$, with $s_1=2$, and 
$s_2=s_3=0$. The objective function value is $z=28$.

This problem has $C_{N+M}^M=C_5^3=10$ basic solutions, corresponding
to the same number of intersections, out of which five are feasible,
as previously obtained. The simplex method finds three of these feasible
solutions, starting from $x_1=s_2=0$ at the origin with $z=0$, through 
the vertex at $x_1=0$ and $x_2=8$ with $z=24$, to the optimal solution 
at $x_1=5$ and $x_2=6$ with $z=28$.


\subsection*{Quadratic Programming (QP)}

% http://etd.dtu.dk/thesis/220437/ep08_19.pdf
% https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture19.pdf
% https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture1.pdf
% http://www.solver.com/linear-quadratic-programming#Quadratic Programming (QP) Problems
% https://www.tu-ilmenau.de/fileadmin/media/simulation/Lehre/Vorlesungsskripte/Lecture_materials_Abebe/NLP_Notes.pdf

% https://www.math.uh.edu/~rohop/fall_06/Chapter3.pdf
% http://home.agh.edu.pl/~pba/pdfdoc/Numerical_Optimization.pdf
% https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf


A quadratic programming (QP) problem is to minimize a quadratic function 
subject to some equality and/or inequality constraints:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize}    & 
         f({\bf x})=\frac{1}{2}[{\bf x}-{\bf m}]^T{\bf Q}[{\bf x}-{\bf m}]
         =\frac{1}{2}{\bf x}^T{\bf Q}{\bf x}+{\bf c}^T{\bf x}+c
    \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\
    \mbox{subject to:} & {\bf A}{\bf x}\le{\bf b}
  \end{array}
\end{equation}
where ${\bf Q}={\bf Q}^T$ is a positive definite matrix, ${\bf c}=-{\bf Qm},
\;c={\bf m}^T{\bf Q}{\bf m}/2$. Here the scalar constant $c$ can be dropped
as it does not play any role in the optimization.

We let ${\bf Q}$ be an $N\times N$ matrix, ${\bf A}$ an $m\times N$ matrix,
and ${\bf b}$ is an m-D vector, and ${\bf x}^T{\bf Q}{\bf x}$ is a hyper 
elliptic paraboloid with the minimum at point ${\bf m}$ in the N-D space.

We first consider the special case where the QP problem is only subject to
equality constraints and we assume $m\le N$, i.e., the number of constraints
is smaller than the number of unknowns in ${\bf x}$. Then the solution 
${\bf x}$ has to satisfy ${\bf Ax}-{\bf b}={\bf 0}$, i.e., it has to be on
$m$ hyper planes in the N-D space.

The Lagrangian function of the QP problem is:
\begin{equation}
  L({\bf x},{\bf \lambda})=f({\bf x})+{\bf\lambda}^T({\bf Ax}-{\bf b})
  =\frac{1}{2}{\bf x}^T{\bf Q}{\bf x}+{\bf c}^T{\bf x}
  +{\bf\lambda}^T({\bf Ax}-{\bf b})
\end{equation}
and then set its derivatives with respect to both ${\bf x}$ and
${\bf\lambda}$ to zero:
\begin{eqnarray}
  \bigtriangledown_{\bf x}L({\bf x},{\bf \lambda})
  &=&\bigtriangledown_{\bf x}f({\bf x})
  +\bigtriangledown_{\bf x}{\bf\lambda}^T({\bf A}{\bf x}-{\bf b})
  ={\bf Qx}+{\bf c}+{\bf A}^T{\bf\lambda}={\bf 0}
  \nonumber\\
  \bigtriangledown_{\bf\lambda}L({\bf x},{\bf \lambda})
  &=&{\bf Ax}-{\bf b}={\bf 0}
\end{eqnarray}
These two equations can be combined and expressed in matrix form as:
\begin{equation}
  \left[\begin{array}{cc}{\bf Q}&{\bf A}^T\\{\bf A}&{\bf 0}\end{array}\right]
  \left[\begin{array}{c}{\bf x}\\{\bf\lambda}\end{array}\right]
  =\left[\begin{array}{c}-{\bf c}\\{\bf b}\end{array}\right]
\end{equation}
Solving this system of $m+N$ equations, we get both ${\bf x}^*$ and
${\bf \lambda}^*$.

{\bf Example} 
\begin{equation}
  \mbox{Minimize:}\;\;\;\;\;\;f(x_1,x_2)=x^2_1+x^2_2
  =\frac{1}{2}[x_1,\;x_2]\left[\begin{array}{cc}2&0\\0&2\end{array}\right]
  \left[\begin{array}{c}x_1\\x_2\end{array}\right]
  =\frac{1}{2}{\bf x}^T{\bf Q}{\bf x}
  \nonumber
\end{equation}

\begin{equation}
  \mbox{subject to:}\;\;\;\;\;\;
       {\bf Ax}=[1,\;1]\left[\begin{array}{c}x_1\\x_2\end{array}\right]
       =x_1+x_2={\bf b}=1
  \nonumber
\end{equation}
where
\begin{equation}
  {\bf Q}=\left[\begin{array}{cc}2&0\\0&2\end{array}\right],\;\;\;
  {\bf A}=[1,\;1],\;\;\;\;\;
  {\bf c}=\left[\begin{array}{c}0\\0\end{array}\right],\;\;\;\;\;
  {\bf b}=1;
  \nonumber
\end{equation}

\begin{equation}
  \left[\begin{array}{rrr}2&0&1\\0&2&1\\1&1&0\end{array}\right]
  \left[\begin{array}{r}x_1\\x_2\\\lambda\end{array}\right]
  =\left[\begin{array}{r}0\\0\\1\end{array}\right]
  \nonumber
\end{equation}
Solving this equation we get the solution $x^*_1=x^*_2=0.5$ and
$\lambda^*=-1$, at which the function $f(x^*_1,x^*_2)=0.5$ is 
minimized subject to $x_1+x_2=1$.

If $m=N$, i.e., the number of equality constraints is the same
as the number of variables, then the variable ${\bf x}$ is uniquely
determined by the linear system ${\bf Ax}={\bf b}$, as the intersect
of $m=N$ hyper planes, independent of the objective function 
$f({\bf x})$. Further if $m>N$, i.e., the system ${\bf Ax}={\bf b}$ 
is over constrained, and its solution does not exist in general.
It is therefore more interesting to consider QP problems subject to
both equality and inequality constraints:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize}    & 
    f({\bf x})=\frac{1}{2}{\bf x}^T{\bf Q}{\bf x}+{\bf c}^T{\bf x}\\
    \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\
    \mbox{subject to:} & {\bf Ax}\le{\bf b}
  \end{array}
  \nonumber
\end{equation}

\subsection*{Interior Point Methods}

%http://cepac.cheme.cmu.edu/pasilectures/biegler/ipopt.pdf

Consider the a general constrained optimization problem:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize} & f({\bf x}) \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}
         {\bf h}({\bf x})={\bf 0}\\
         {\bf g}({\bf x})\le{\bf 0}\;\;\;\mbox{or}\;\;\;
         {\bf g}({\bf x})\ge{\bf 0}\\
         {\bf x}\ge{\bf 0} 
    \end{array}\right.\end{array}
\end{equation}
By introducing slack variables ${\bf s}\ge{\bf 0}$, all inequality
constraints can be converted into equality constraints:
\begin{equation}
    {\bf g}({\bf x})\le{\bf 0}\;\Longrightarrow \;
    {\bf g}({\bf x})+{\bf s}={\bf 0},\;\;\;\;\;\;\;
    {\bf g}({\bf x})\ge{\bf 0}\;\Longrightarrow \;
    {\bf g}({\bf x})-{\bf s}={\bf 0} 
\end{equation}
which can then be combined with the equality constraints and still 
denoted by ${\bf h}({\bf x})={\bf 0}$, while the slack variables 
${\bf s}$ are also combined with the original variables and still 
denoted by ${\bf x}$. We assume there are in total $M$ equality 
constraints and $N$ variables. Now the optimization problem can be
reformulated as:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize} & f({\bf x}) \\
    \mbox{subject to:} & 
    \left\{\begin{array}{l}
         {\bf h}({\bf x})={\bf 0}\\
         {\bf x}\ge{\bf 0}
    \end{array}\right.\end{array}
\end{equation}
The Lagrangian is
\begin{equation}
  L({\bf x},{\bf\lambda},{\bf\mu})=f({\bf x})
  +{\bf\lambda}^T{\bf h}({\bf x})-{\bf\mu}^T{\bf x}
\end{equation}
The KKT conditions are
\begin{equation}
\left\{
\begin{array}{ll}
  \bigtriangledown_{\bf x} L({\bf x},{\bf\lambda},{\bf\mu})
  ={\bf g}_f({\bf x})+{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}-{\bf\mu}
  ={\bf 0}&
  \mbox{(stationarity)}\\
  {\bf h}({\bf x})={\bf 0},\;\;\;\;{\bf x}\ge{\bf 0} & 
  \mbox{(primal feasibility)}\\
  {\bf\mu}\ge {\bf 0} & \mbox{(dual feasibility)}\\
  \mu_j x_j=0,\;\;\;\;(j=1,\cdots,N)\;\;\;\;\mbox{or}\;\;\;
  {\bf XM1}={\bf 0} & \mbox{(complementarity)}
  \end{array}\right.
\end{equation}
where ${\bf X}=diag(x_1,\cdots,x_N)$, ${\bf M}=diag(\mu_1,\cdots,\mu_N)$,
and ${\bf J}_{\bf f}({\bf x})$ is the Jacobian matrix of ${\bf h}({\bf x})$:
\begin{equation}
{\bf X}=\left[\begin{array}{ccc}x_1&\cdots&0\\
    \vdots&\ddots&\vdots\\0&\cdots&x_n \end{array}\right]_{N\times N},
  \;\;\;\;\;
{\bf M}=\left[\begin{array}{ccc}\mu_1&\cdots&0\\
    \vdots&\ddots&\vdots\\0&\cdots&\mu_N
    \end{array}\right]_{N\times N},
  \;\;\;\;\;
{\bf J}_{\bf h}({\bf x})=\left[\begin{array}{ccc}
      \frac{\partial h_1}{\partial x_1}&\cdots&
      \frac{\partial h_1}{\partial x_n}\\
      \vdots & \ddots & \vdots\\
      \frac{\partial h_m}{\partial x_1}&\cdots&
      \frac{\partial h_m}{\partial x_n}\end{array}\right]_{M\times N}
\end{equation}
% It is difficult to find the solution ${\bf x}^*$ of the optimization
% problem that satisfies all the inequalities as well as equalities 
% in the KKT conditions. 
To simplify the problem, the non-negativity constraints can be removed 
by introducing an indicator function:
\begin{equation}
  I(x)=\left\{\begin{array}{ll}0 & x\ge 0\\\infty& x<0\end{array}
\right.
\end{equation}
which, when used as a cost function, penalizes any violation of the 
non-negativity constraint $x\ge 0$. Now the optimization problem can
be reformulated as shown below without the non-negative constants 
${\bf x}\ge{\bf 0}$:
\begin{equation}
  \begin{array}{ll}
  \mbox{minimize} & f({\bf x})+\sum_{i=1}^N I(x_i)\\
  \mbox{subject to} & {\bf h}({\bf x})={\bf 0}
  \end{array}
\end{equation}
However, as the indicator function is not smooth and therefore not
differentiable, it is approximated by the {\em logarithmic barrier function} 
which approaches $I(x)$ when the parameter $t>0$ approaches infinity:
\begin{equation}
  -\frac{1}{t}\ln(x)\;\;\stackrel{t\rightarrow\infty}{\Longrightarrow}\;\;I(x)
\end{equation}

\htmladdimg{figures/LogBarrier.png}

Now the optimization problem can be written as:
\begin{equation}
  \begin{array}{ll}
  \mbox{minimize} & f({\bf x})-\frac{1}{t}\sum_{i=1}^N\ln x_i\\
  \mbox{subject to} & {\bf h}({\bf x})={\bf 0}
  \end{array}
\end{equation}
The Lagrangian is:
\begin{equation}
  L({\bf x},{\bf\lambda},{\bf\mu})=f({\bf x})
  +{\bf\lambda}^T{\bf h}({\bf x})-\frac{1}{t}\sum_{j=1}^N\ln x_j\\
\end{equation}
and its gradient is:
\begin{equation}
  \bigtriangledown_{\bf x} L({\bf x},{\bf\lambda},{\bf\mu})
  ={\bf g}_f({\bf x})+{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}
  -\frac{1}{t}{\bf X}^{-1}{\bf 1}
\end{equation}
We define
\begin{equation}
  {\bf\mu}=\frac{\bf x}{t}=\frac{1}{t}{\bf X}^{-1}{\bf 1},
  \;\;\;\;\mbox{i.e.}\;\;\;
  {\bf X\mu}={\bf XM 1}=\frac{\bf 1}{t}
\end{equation}
and combine this equation with the KKT conditions of the new 
optimization problem to get
\begin{equation}
  \left\{
  \begin{array}{ll}
    {\bf g}_f({\bf x})+{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}-\mu={\bf 0} &
    \mbox{(stationarity)}\\
         {\bf h}({\bf x})={\bf 0} & \mbox{(primal feasibility)}\\
         {\bf XM1}-{\bf 1}/t={\bf 0} & \mbox{(complementarity)}
  \end{array}\right.
  \label{KKTconditionsIP}
\end{equation}
The third condition is actually from the definition of ${\bf\mu}$ 
given above, but here it is labeled as complementarity, so that 
these conditions can be compared with the KKT conditions of the 
original problem. We see that the two sets of KKT conditions are
similar to each other, with the following differences:
\begin{itemize}
\item The non-negativity ${\bf x}\ge{\bf 0}$ in the primal feasibility
  of the original KKT is dropped;
\item Consequently the inequality ${\bf\mu}\ge{\bf 0}$ in the dual 
  feasibility of the original KKT conditions is also dropped;
\item There is an extra term ${\bf 1}/t$ in complementarity which
  will vanish when $t\rightarrow\infty$.
\end{itemize}

The optimal solution that minimizes the objective function $f({\bf x})$
can be found by solving the simultaneous equations in the modified
KKT conditions (with no inequalities) given above. To do so, we first
express the equations in the KKT conditions above as a nonlinear
equation system ${\bf F}({\bf x},{\bf\lambda},{\bf\mu})={\bf 0}$, and 
find its Jacobian matrix ${\bf J}_{\bf F}$ composed of the partial 
derivatives of the function ${\bf F}({\bf x},{\bf\lambda},{\bf\mu})$ 
with respect to each of the three variables ${\bf x}$, ${\bf\lambda}$, 
and ${\bf\mu}$:
\begin{equation}
  {\bf F}({\bf x},{\bf\lambda},{\bf\mu})=\left[ \begin{array}{c}
      {\bf g}_f({\bf x})+{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}-\mu\\
      {\bf h}({\bf x})\\  {\bf XM1}-{\bf 1}/t \end{array}\right],
  \;\;\;\;\;\;
  {\bf J}_{\bf F}=\left[\begin{array}{ccc}
      {\bf W}({\bf x}) & {\bf J}^T_{\bf h}({\bf x}) & -{\bf I}\\
      {\bf J}_{\bf h}({\bf x}) & {\bf 0} & {\bf 0} \\
      {\bf M} & {\bf 0} & {\bf X}\end{array}\right]
\end{equation}
where 
\begin{eqnarray}
  {\bf W}({\bf x})&=&\bigtriangledown_{\bf x} 
  \left[{\bf g}_f({\bf x})+{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}
  -{\bf\mu}\right]
  \nonumber\\
  &=&\bigtriangledown_{\bf x} {\bf g}_f({\bf x})
  -\bigtriangledown_{\bf x} \left[\sum_{i=1}^M\lambda_i\frac{\partial h_i}{\partial x_1},
    \cdots,\sum_{i=1}^M\lambda_i\frac{\partial h_i}{\partial x_N}\right]^T
  ={\bf H}_f({\bf x})-\sum_{i=1}^M\lambda_i{\bf H}_{h_i}({\bf x})
\end{eqnarray}
is symmetric as the Hessian matrices ${\bf H}_f$ and ${\bf H}_{h_i},\;
(i=1,\cdots,M)$ are symmetric.

We can now use the 
\htmladdnormallink{Newton-Raphson method}{../ch2/node7.html}
to find the solution of the nonlinear equation 
${\bf F}({\bf x},{\bf\lambda},{\bf\mu})={\bf 0}$ iteratively:
\begin{equation}
  \left[\begin{array}{c}{\bf x}_{n+1}\\{\bf\lambda}_{n+1}\\
      {\bf\mu}_{n+1}\end{array}\right]
  =\left[\begin{array}{c}{\bf x}_n\\{\bf\lambda}_n\\
      {\bf\mu}_n\end{array}\right]
  +\alpha\left[\begin{array}{c}\delta{\bf x}_n\\\delta{\bf\lambda}_n\\
      \delta{\bf\mu}_n\end{array}\right]
\end{equation}
where $\alpha$ is a parameter that controls the step size and 
$[\delta{\bf x}_n,\delta{\bf\lambda}_n,\delta{\bf\mu}_n]^T$ is the
search direction ({\em Newton direction}), which can be found by 
solving the following equation:
\begin{eqnarray}
  &&{\bf J}_{\bf F}({\bf x}_n,{\bf\lambda}_n,{\bf\mu}_n)
  \left[\begin{array}{c}
      \delta{\bf x}_n\\\delta{\bf\lambda}_n\\\delta{\bf\mu}_n
    \end{array}\right]
  =\left[\begin{array}{ccc}
      {\bf W}({\bf x}_n) & {\bf J}^T_{\bf h}({\bf x}_n) & -{\bf I}\\
      {\bf J}_{\bf h}({\bf x}_n) & {\bf 0} & {\bf 0} \\
      {\bf M}_n & {\bf 0} & {\bf X}_n\end{array}\right]
  \left[\begin{array}{c}
      \delta{\bf x}_n\\\delta{\bf\lambda}_n\\\delta{\bf\mu}_n
    \end{array}\right]
  \nonumber\\
  &=&-{\bf F}({\bf x}_n,{\bf\lambda}_n,{\bf\mu}_n)
  =-\left[\begin{array}{c}
      {\bf g}_f({\bf x}_n)+{\bf J}_{\bf h}^T({\bf x}_n){\bf\lambda}_n-\mu_n\\
      {\bf h}({\bf x}_n)\\  {\bf X_nM_n1}-{\bf 1}/t
    \end{array}\right]
\end{eqnarray}
To get the initial values for the iteration, we first find any point 
inside the feasible region ${\bf x}_0$ and then ${\bf\mu}_0={\bf x}_0/t$,
based on which we further find ${\bf\lambda}_0$ by solving the first 
equation in the KKT conditions in Eq. (\ref{KKTconditionsIP}).

Actually this equation system above can be separated into two
subsystems, which are easier to solve. Pre-multiplying ${\bf X}^{-1}$ 
to the third equation:
\begin{equation}
  {\bf M}\delta{\bf x}+{\bf X}\delta{\bf\mu}=-{\bf XM1}+{\bf 1}/t
\end{equation}
we get
\begin{equation}
  {\bf X}^{-1}{\bf M}\delta{\bf x}+\delta{\bf\mu}
  =-{\bf M1}+{\bf X}^{-1}{\bf 1}/t=-{\bf\mu}+{\bf X}^{-1}{\bf 1}/t
\end{equation}
Adding this to the first equation:
\begin{equation}
  {\bf W}\delta{\bf x}+ {\bf J}^T_{\bf h}({\bf x}) \delta{\bf\lambda}
  -\delta{\bf\mu}=-{\bf g}_f({\bf x})-{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}+{\bf\mu}
\end{equation}
we get
\begin{equation}
  ({\bf W}+{\bf X}^{-1}{\bf M})\delta{\bf x}+{\bf J}^T_{\bf h}({\bf x})\delta{\bf\lambda}
  =-{\bf g}_f({\bf x})-{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}+{\bf X}^{-1}{\bf 1}/t
  =-\bigtriangledown_{\bf x}L({\bf x},{\bf\lambda},{\bf\mu})
\end{equation}
Combining this equation with the second one, we get an equation system 
of two vector variables $\delta{\bf x}$ and $\delta{\bf\lambda}$ with
symmetric coefficient matrix:
\begin{equation}
  \left[\begin{array}{ccc}
      {\bf W}+{\bf X}^{-1}{\bf M} & {\bf J}^T_{\bf h}({\bf x})\\
      {\bf J}_{\bf h}({\bf x}) & {\bf 0} \end{array}\right]
  \left[\begin{array}{c}
      \delta{\bf x}\\\delta{\bf\lambda}\end{array}\right]
  =-\left[\begin{array}{c}
      \bigtriangledown_{\bf x}L({\bf x},{\bf\lambda},{\bf\mu})\\
                      {\bf h}({\bf x})
    \end{array}\right]
\end{equation}
Solving this equation system we get $\delta{\bf x}$ and $\delta{\bf\lambda}$,
and we can further find $\delta{\bf\mu}$ by solving the third equation
\begin{equation}
  {\bf M}\delta{\bf x}+{\bf X}\delta\mu=-{\bf XM1}-{\bf 1}/t
\end{equation}
to get
\begin{equation}
  \delta{\bf\mu}={\bf X}^{-1}{\bf 1}/t-{\bf X}^{-1}{\bf M}\delta{\bf x}-{\bf\mu}
\end{equation}

% http://cepac.cheme.cmu.edu/pasilectures/biegler/ipopt.pdf

We now consider the two special cases of linear and quadratic 
programming:
\begin{itemize}
\item {\bf Linear programming}:

\begin{equation}
  \begin{array}{ll}
    \mbox{minimize}  &  f({\bf x})={\bf c}^T{\bf x}\\
    \mbox{subject to:} & {\bf h}({\bf x})={\bf Ax}-{\bf b}={\bf 0},
    \;\;\;\;{\bf x}\ge{\bf 0}
  \end{array}
  \label{LPproblem}
\end{equation}
We have 
\begin{itemize}
\item 
  ${\bf g}_f({\bf x})=\bigtriangledown_{\bf x}f({\bf x})={\bf c}$
\item
  ${\bf J}_{\bf h}({\bf x})=\bigtriangledown_{\bf x}({\bf Ax}-{\bf b})={\bf A}$.
\item 
  ${\bf W}({\bf x})=\bigtriangledown^2_{\bf x} L({\bf x},{\bf\lambda},{\bf\mu})={\bf 0}$.
\end{itemize}
The Lagrangian is
\begin{equation}
  L({\bf x},{\bf\lambda},{\bf\mu})
  ={\bf c}^T{\bf x}+{\bf\lambda}^T({\bf Ax}-{\bf b})-{\bf\mu}^T{\bf x}
\end{equation}
The KKT conditions are:
\begin{equation}
  \left\{
  \begin{array}{ll}
    \bigtriangledown_{\bf x} L({\bf x},{\bf\lambda},{\bf\mu})
    ={\bf g}_f({\bf x})+{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}-\mu
    ={\bf c}+{\bf A}^T{\bf\lambda}-{\bf\mu}={\bf 0} \nonumber\\
    {\bf h}({\bf x})={\bf Ax}-{\bf b}={\bf 0} \nonumber\\
    {\bf XM1}-{\bf 1}/t={\bf 0} 
    \end{array}\right.
\end{equation}
The search direction of the iteration can be found by solving the
this equation system:
\begin{equation}
  \left[\begin{array}{ccc}
      {\bf 0} & {\bf A}^T & -{\bf I}\\
      {\bf A} & {\bf 0} & {\bf 0} \\
      {\bf M} & {\bf 0} & {\bf X}\end{array}\right]
  \left[\begin{array}{c}
      \delta{\bf x}\\\delta{\bf\lambda}\\\delta{\bf\mu}
    \end{array}\right]
  =-\left[\begin{array}{c}
      {\bf c}+{\bf A}^T{\bf\lambda}-{\bf\mu}\\
      {\bf Ax}-{\bf b}\\  {\bf XM1}-{\bf 1}/t
    \end{array}\right]
\end{equation}
By solving the equation ${\bf c}+{\bf A}^T{\bf\lambda}-{\bf\mu}={\bf 0}$
we get the initial value ${\bf\lambda}$. The Matlab code for the interior 
point method for the LP problem is listed below:

\begin{verbatim}
function x=InterierPointLP(A,c,b,x)
    % given A,b,c of the LP problem and inital value of x,
    % find optimal solution x that minimizes c'*x
    [m n]=size(A);      % m constraints, n variables
    z1=zeros(n);
    z2=zeros(m);        % zero matrices in coefficient matrix
    z3=zeros(m,n);
    I=eye(n);           % identity matrix in coefficient matrix
    y=ones(n,1);
    t=9;                % initinal value for parameter t
    alpha=0.3;          % small enough not to exceed boundary
    mu=x./t;
    lambda=pinv(A')*(mu-c); % initial value of lambda
    w=[x; lambda; mu];      % initial values for all three 
    B=[c+A'*lambda-mu; A*x-b; x.*mu-y/t];   
    p=[x(1) x(2)];      % initial guess of solution
    while norm(B)>10^(-7)        
        t=t*9;          % increase parameter t
        X=diag(x);
        M=diag(mu);
        C=[z1 A' -I; A z2 z3; M z3' X];
        B=[c+A'*lambda-mu; A*x-b; x.*mu-y/t]; 
        dw=-inv(C)*B;  	% find search direction
        w=w+alpha*dw;   % step forward
        x=w(1:n);
        lambda=w(n+1:n+m);
        mu=w(n+m+1:length(w));       
        p=[p; x(1), x(2)];
    end
    scatter(p(:,1),p(:,2));  % plot trajectory of solution
end
\end{verbatim}

{\bf Example 1:} A given linear programming problem shown below is first 
converted into the standard form:
\begin{equation}
  \begin{array}{ll}
    \mbox{maximize}    & f(x_1,x_2)=2x_1+3x_2 \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}
      2x_1+ x_2 \le 18\\ 6x_1+5x_2 \le 60 \\ 2x_1+5x_2 \le 40 \\ x_1 \ge 0,\;\; x_2\ge 0
    \end{array}\right.
  \end{array}
  \;\;\;\;\;\Longrightarrow\;\;\;\;\;
  \begin{array}{ll}
    \mbox{minimize}    & f(x_1,x_2,x_3,x_4,x_5)=-2x_1-3x_2 \\
    \mbox{subject to:} & 
    \left\{ \begin{array}{l}
      2x_1+x_2+x_3=18\\ 6x_1+5x_2+x_4=60 \\ 2x_1+5x_2+x_5=40 \\ x_i \ge 0,\;\;(i=1,\cdots,5)
    \end{array}\right.
  \end{array}
  \nonumber
\end{equation}
or in matrix form:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize}    & f({\bf x})={\bf c}^T{\bf x} \\
    \mbox{subject to}  & {\bf h}({\bf x})={\bf A}{\bf x}-{\bf b}={\bf 0},\;\;{\bf x}\ge {\bf 0}
  \end{array}
  \nonumber
\end{equation}
with
\begin{equation}
  {\bf x}=\left[\begin{array}{c}x_1\\x_2\\x_3\\x_4\\x_5\end{array}\right],\;\;\;\;
  {\bf c}=\left[\begin{array}{c}-2\\-3\\0\\0\\0\end{array}\right],\;\;\;\;
  {\bf A}=\left[\begin{array}{ccccc}2&1&1&0&0\\6&5&0&1&0\\2&5&0&0&1\end{array}\right],\;\;\;\;
  {\bf b}=\left[\begin{array}{c}18\\60\\40\end{array}\right]
  \nonumber
\end{equation}
where $x_3,\,x_4,\,x_5$ are the slack variables.

\htmladdimg{figures/InteriorPointLPexample.png}

We choose an initial value ${\bf x}_0=[1,\,2,\,1,\,1,\,1]^T$, and 
an initial parameter $t=9$, which is scaled up by a factor of $9$ 
in each iteration. We also used full Newton step size of $\alpha=1$. 
After 8 iterations, the algorithm converged to the optimal solution 
${\bf x}^*=[5,\;6]^T$ corresponding to the maximum function value 
$f({\bf x}^*)=2x_1+3x_2=28$:
\begin{equation}
  \begin{array}{c|| c c| c | c}\hline
    & (x_1 & x_2) & f({\bf x}) & \mbox{error} \\\hline
    1 & (1.000000e+00 & 2.000000e+00) &  -8.000000	& 52.413951 \\
    2 & (4.654514e+00 & 6.346354e+00) & -28.348090	& 3.080204 \\
    3 & (5.040828e+00 & 5.946973e+00) & -27.922575	& 0.213509 \\
    4 & (4.997282e+00 & 6.001764e+00) & -27.999856	& 0.004262 \\
    5 & (4.999906e+00 & 5.999962e+00) & -27.999697	& 0.000303 \\
    6 & (4.999989e+00 & 5.999996e+00) & -27.999966	& 0.000034 \\
    7 & (4.999999e+00 & 6.000000e+00) & -27.999996	& 0.000004 \\
    8 & (5.000000e+00 & 6.000000e+00) & -28.000000	& 0.000000 \\\hline
  \end{array}
  \nonumber
\end{equation}
At the end of the iteration, we also get the values of the slack 
variables: $x_3=2,\;x_4=x_5=0$.

\item {\bf Quadratic programming:}

\begin{equation}
  \begin{array}{ll}
    \mbox{minimize}    & 
    f({\bf x})=\frac{1}{2}{\bf x}^T{\bf Q}{\bf x}+{\bf c}^T{\bf x}\\
    \mbox{subject to:} & {\bf h}({\bf x})={\bf A}{\bf x}-{\bf b}={\bf 0},
    \;\;\;\;{\bf x}\ge{\bf 0}
  \end{array}
  \label{QPproblem}
\end{equation}
We have
\begin{itemize}
\item
  ${\bf g}_f({\bf x})=\bigtriangledown_{\bf x}f({\bf x})={\bf Qx}+{\bf c}$
\item
  ${\bf J}_{\bf h}({\bf x})=\bigtriangledown_{\bf x}({\bf Ax}-{\bf b})={\bf A}$.
\item 
  ${\bf W}({\bf x})=\bigtriangledown^2_{\bf x} L({\bf x},{\bf\lambda},{\bf\mu})={\bf Q}$.
\end{itemize}
The Lagrangian is 
\begin{equation}
  L({\bf x},{\bf\lambda},{\bf\mu})
  =\frac{1}{2}{\bf x}^T{\bf Qx}+{\bf c}^T{\bf x}
  +{\bf\lambda}^T({\bf Ax}-{\bf b})-{\bf\mu}^T{\bf x}
\end{equation}
The KKT conditions are:
\begin{equation}
  \left\{
  \begin{array}{ll}
    \bigtriangledown_{\bf x} L({\bf x},{\bf\lambda},{\bf\mu})
    ={\bf g}_f({\bf x})+{\bf J}_{\bf h}^T({\bf x}){\bf\lambda}-\mu
    ={\bf Qx}+{\bf c}+{\bf A}^T{\bf\lambda}-{\bf\mu}={\bf 0}
    \nonumber\\
        {\bf h}({\bf x})={\bf Ax}-{\bf b}={\bf 0} \nonumber\\
        {\bf XM1}-{\bf 1}/t={\bf 0} 
    \end{array}
  \right.
\end{equation}
The equation system for the Newton-Raphson method is:
\begin{equation}
  \left[\begin{array}{ccc}
      {\bf Q} & {\bf A}^T & -{\bf I}\\
      {\bf A} & {\bf 0} & {\bf 0} \\
      {\bf M} & {\bf 0} & {\bf X}\end{array}\right]
  \left[\begin{array}{c}
      \delta{\bf x}\\\delta{\bf\lambda}\\\delta{\bf\mu}
    \end{array}\right]
  =-\left[\begin{array}{c}
      {\bf Qx}+{\bf c}+{\bf A}^T{\bf\lambda}-{\bf\mu}\\
      {\bf Ax}-{\bf b}\\{\bf XM1}-{\bf 1}/t
    \end{array}\right]
\end{equation}

\end{itemize}


The Matlab code for the interior point method for the QP problem is
listed below:

\begin{verbatim}
function [x mu lambda]=InteriorPointQP(Q,A,c,b,x)
    n=length(c);        % n variables
    m=length(b);    	% m constraints
    z2=zeros(m);
    z3=zeros(m,n);
    I=eye(n);
    y=ones(n,1);
    t=9;                % initinal value for parameter t
    alpha=0.1;       	% stepsize, small enough not to exceed boundary
    mu=x./t;
    lambda=pinv(A')*(mu-c-Q*x); % initial value of lambda
    w=[x; lambda; mu];   	% initial value
    B=[Q*x+c+A'*lambda-mu; A*x-b; x.*mu-y/t];             
    while norm(B)>10^(-7)   
        t=t*9;        	% increase parameter t
        X=diag(x);
        M=diag(mu);       
        C=[Q A' -I; A z2 z3; M z3' X];
        B=[Q*x+c+A'*lambda-mu; A*x-b; x.*mu-y/t];             
        dw=-inv(C)*B;  	% find search direction
        w=w+alpha*dw;  	% step forward
        x=w(1:n);
        lambda=w(n+1:n+m);
        mu=w(n+m+1:length(w));
    end
end
\end{verbatim}

% http://www.cs.toronto.edu/~robere/paper/interiorpoint.pdf
% http://apmonitor.com/me575/uploads/Main/interior_point_lecture.pdf

{\bf Example 2:} A 2-D quadratic programming problem shown below is to 
minimize four different quadratic functions under the same linear 
constraints as the liniear programming problem in the previous example.
In general, the quadratic function is given in the matrix form:
\begin{equation}
  \begin{array}{ll}
    \mbox{minimize}    & f({\bf x})=[{\bf x}-{\bf m}]^T{\bf Q}[{\bf x}-{\bf m}]
    ={\bf x}^T{\bf Q}{\bf x}+{\bf c}^T{\bf x}+d \\
    \mbox{subject to}  & {\bf h}({\bf x})={\bf A}{\bf x}-{\bf b}={\bf 0},\;\;{\bf x}\ge {\bf 0}
  \end{array}
  \nonumber
\end{equation}
with following four different sets of function parameters:
\begin{equation}
  {\bf Q}=\left[\begin{array}{rr}4 & -1\\-1 & 4\end{array}\right],\;\;\;
  \left[\begin{array}{rr}2 &  1\\ 1 & 2\end{array}\right],\;\;\;
  \left[\begin{array}{rr}2 &  1\\ 1 & 2\end{array}\right],\;\;\;
  \left[\begin{array}{rr}3 & -1\\-1 & 3\end{array}\right]
  \nonumber
\end{equation}

\begin{equation}
  {\bf m}=\left[\begin{array}{r}4\\10\end{array}\right],\;\;\;
  \left[\begin{array}{r}9\\5\end{array}\right],\;\;\;
  \left[\begin{array}{r}7\\2\end{array}\right],\;\;\;
  \left[\begin{array}{r}-1\\6\end{array}\right]
  \nonumber
\end{equation}
As shown in the four panels in the figure below, the iteration of 
the interior point algorithm brings the solution from the same 
initial position at ${\bf x}_0=[2,\;1]^T$ to the final position 
for each of the four objective functions $f({\bf x})$, the optimal
solution, which is on the boundary of the feasible region in all 
cases except the third one, where the optimal solution is inside
the feasible region, i.e., the optimization problem is not constrained.
Also note that the trajectories of the solution during the iteration 
go straightly from the initial guess to the final optimal solution 
by following the negative gradient direction of the quadratic function
in all cases execpt in the last one, where it makes a turn while 
approaching the vertical boundary, due obviously to the significantly
greater value of the log barrier function close to the boundary.

\htmladdimg{figures/InteriorPointQPexample.png}





\end{document}
