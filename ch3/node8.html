
<H2><A ID="SECTION00026000000000000000">
Line minimization</A>
</H2>

<P>
In both Newton's method and the radient descent method, an iteration 
<tex2html_verbatim_mark>#math420#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18786#</SPAN> is carried out to gradually 
reduce the value of the objective function <tex2html_verbatim_mark>#math421#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18788#</SPAN>. Here both the
search direction <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18790#</SPAN> and the step size <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18792#</SPAN> need to be 
determined to maximally reduce the function value <tex2html_verbatim_mark>#math422#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18794#</SPAN>.

<P>
First, we realize that to reduce <tex2html_verbatim_mark>#math423#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18796#</SPAN> the search 
direction <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18798#</SPAN> needs to point away from the gradient <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18800#</SPAN> 
along which <tex2html_verbatim_mark>#math424#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18802#</SPAN> increases most rapidly, i.e., the angle 
<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18804#</SPAN> between <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18806#</SPAN> and <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18808#</SPAN> should be greater than 
<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18810#</SPAN>:
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math425#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18812#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">51</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
so that <tex2html_verbatim_mark>#math426#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18814#</SPAN> will decrease along <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18816#</SPAN>. 
We see that this condition is indeed satisfied in both the gradient
descent method with <tex2html_verbatim_mark>#math427#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18818#</SPAN> and Newton's method with
<tex2html_verbatim_mark>#math428#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18820#</SPAN> (with <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18822#</SPAN> being positive 
definite):
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math429#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18824#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">52</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>

<P>
Second, the optimal step size <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18826#</SPAN> needs to be determined so that 
the function velue at the next step <tex2html_verbatim_mark>#math430#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18828#</SPAN> 
is minimized along the search direction <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18830#</SPAN>. To find such a <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18832#</SPAN>,
we set to zero the derivative of the function with respect to <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18834#</SPAN>, the 
<#1101#><EM>directional derivative</EM><#1101#> along the direction of <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18836#</SPAN>, and get 
(by chain rule):
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math431#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18838#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">53</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
This result indicates that the gradient <tex2html_verbatim_mark>#math432#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18840#</SPAN> 
at <tex2html_verbatim_mark>#math433#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18842#</SPAN> needs to be perpendicular to the previous search 
direction <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18844#</SPAN>. In other words, when traversing along the 
search direction <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18846#</SPAN>, we should stop at the point 
<tex2html_verbatim_mark>#math434#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18848#</SPAN> at which the gradient 
<tex2html_verbatim_mark>#math435#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18850#</SPAN> has zero component along the direction <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18852#</SPAN>, 
and the corresponding <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18854#</SPAN> is the optimal step size.

<P>
Finding the optimal step size <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18856#</SPAN> is a 1-D optimization problem, 
which can be solved by Newton's method. Specifically, we treat
<tex2html_verbatim_mark>#math436#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18858#</SPAN> as a function of the signle
variable <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18860#</SPAN>, and approximate it by the first three terms of its 
Taylor series at <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18862#</SPAN>:
<P></P>
<DIV CLASS="mathdisplay"><A ID="OptimalStepTaylor"><tex2html_anchor_mark></A><tex2html_verbatim_mark>#math437#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18864#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">54</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
where
<BR>
<DIV CLASS="mathdisplay">
<tex2html_verbatim_mark>#math438#
<TABLE CELLPADDING="0" ALIGN="CENTER" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP WIDTH="50%" ALIGN="RIGHT"><tex2html_image_mark>#tex2html_wrap_indisplay18867#</TD>
<TD WIDTH="10" ALIGN="CENTER" NOWRAP><tex2html_image_mark>#tex2html_wrap_indisplay18869#</TD>
<TD ALIGN="LEFT" NOWRAP WIDTH="50%"><tex2html_image_mark>#tex2html_wrap_indisplay18871#</TD>
<TD CLASS="eqno" WIDTH=10 ALIGN="RIGHT">
(<SPAN CLASS="arabic">55</SPAN>)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP WIDTH="50%" ALIGN="RIGHT"><tex2html_image_mark>#tex2html_wrap_indisplay18873#</TD>
<TD WIDTH="10" ALIGN="CENTER" NOWRAP><tex2html_image_mark>#tex2html_wrap_indisplay18875#</TD>
<TD ALIGN="LEFT" NOWRAP WIDTH="50%"><tex2html_image_mark>#tex2html_wrap_indisplay18877#</TD>
<TD CLASS="eqno" WIDTH=10 ALIGN="RIGHT">
(<SPAN CLASS="arabic">56</SPAN>)</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP WIDTH="50%" ALIGN="RIGHT"><tex2html_image_mark>#tex2html_wrap_indisplay18879#</TD>
<TD WIDTH="10" ALIGN="CENTER" NOWRAP><tex2html_image_mark>#tex2html_wrap_indisplay18881#</TD>
<TD ALIGN="LEFT" NOWRAP WIDTH="50%"><tex2html_image_mark>#tex2html_wrap_indisplay18883#</TD>
<TD CLASS="eqno" WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP WIDTH="50%" ALIGN="RIGHT">&nbsp;</TD>
<TD WIDTH="10" ALIGN="CENTER" NOWRAP><tex2html_image_mark>#tex2html_wrap_indisplay18885#</TD>
<TD ALIGN="LEFT" NOWRAP WIDTH="50%"><tex2html_image_mark>#tex2html_wrap_indisplay18887#</TD>
<TD CLASS="eqno" WIDTH=10 ALIGN="RIGHT">
(<SPAN CLASS="arabic">57</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL">

where <tex2html_verbatim_mark>#math439#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18889#</SPAN> is the Hessian matrix of <tex2html_verbatim_mark>#math440#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18891#</SPAN>
at <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18893#</SPAN>. Substituting these back into the Taylor series above
we get:
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math441#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18895#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">58</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
To find <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18897#</SPAN> that minimizes <tex2html_verbatim_mark>#math442#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18899#</SPAN>, we 
set its derivative with respect to <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18901#</SPAN> to zero:
<P></P>
<DIV CLASS="mathdisplay"><A ID="OptimalDelta1"><tex2html_anchor_mark></A><tex2html_verbatim_mark>#math443#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18903#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">59</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
and solve the resulting equation for <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18905#</SPAN> to get the optimal 
step size based on both <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18907#</SPAN> and <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18909#</SPAN>:
<P></P>
<DIV CLASS="mathdisplay"><A ID="OptimalDelta"><tex2html_anchor_mark></A><tex2html_verbatim_mark>#math444#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18911#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">60</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>

<P>
For example, consider the following two methods for minimization:

<P>

<UL>
<LI><#1260#><B>Newton's method:</B><#1260#> 

<P>
The search direction is <tex2html_verbatim_mark>#math445#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18913#</SPAN>, and the 
  optimal step size is
  <P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math446#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18915#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">61</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
  This is the iteration obtained before, which is indeed optimal:
  <P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math447#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18917#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">62</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>

<P>
</LI>
<LI><#1304#><B>Gradient descent method:</B><#1304#> 

<P>
The search direction is <tex2html_verbatim_mark>#math448#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18919#</SPAN>, we have
  <P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math449#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18921#<#1#>i.e.,<#1#><tex2html_image_mark>#tex2html_wrap_indisplay18922#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">63</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
  i.e., the search direction <tex2html_verbatim_mark>#math450#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18924#</SPAN> is always
  perpendicular to the previous one <tex2html_verbatim_mark>#math451#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18926#</SPAN>, i.e., the
  iteration follows a zigzag path composed of a sequence of segments 
  from the initial guess to the final solution. The optimal step size 
  is
  <P></P>
<DIV CLASS="mathdisplay"><A ID="GradientStepSizeOptimal"><tex2html_anchor_mark></A><tex2html_verbatim_mark>#math452#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18928#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">64</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
</LI>
</UL>

<P>
However, as the Hessian <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18930#</SPAN> is not assumed to be available in 
the gradient descent method, the optimal step size above cannot actually 
be computed. We can instead approximate <tex2html_verbatim_mark>#math453#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18932#</SPAN> in 
the third term of Eq. (<A HREF=<tex2html_cr_mark>#OptimalStepTaylor#1345><tex2html_cr_mark></A>) at two nearby points 
at <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18934#</SPAN> and <tex2html_verbatim_mark>#math454#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18936#</SPAN>, where <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18938#</SPAN> is a small value:
<BR>
<DIV CLASS="mathdisplay">
<tex2html_verbatim_mark>#math455#
<TABLE CELLPADDING="0" ALIGN="CENTER" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP WIDTH="50%" ALIGN="RIGHT"><tex2html_image_mark>#tex2html_wrap_indisplay18941#</TD>
<TD WIDTH="10" ALIGN="CENTER" NOWRAP><tex2html_image_mark>#tex2html_wrap_indisplay18943#</TD>
<TD ALIGN="LEFT" NOWRAP WIDTH="50%"><tex2html_image_mark>#tex2html_wrap_indisplay18945#</TD>
<TD CLASS="eqno" WIDTH=10 ALIGN="RIGHT">
&nbsp;</TD></TR>
<TR VALIGN="MIDDLE"><TD NOWRAP WIDTH="50%" ALIGN="RIGHT">&nbsp;</TD>
<TD WIDTH="10" ALIGN="CENTER" NOWRAP><tex2html_image_mark>#tex2html_wrap_indisplay18947#</TD>
<TD ALIGN="LEFT" NOWRAP WIDTH="50%"><tex2html_image_mark>#tex2html_wrap_indisplay18949#</TD>
<TD CLASS="eqno" WIDTH=10 ALIGN="RIGHT">
(<SPAN CLASS="arabic">65</SPAN>)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL">

where <tex2html_verbatim_mark>#math456#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18951#</SPAN> and <tex2html_verbatim_mark>#math457#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18953#</SPAN>.
This approximation can be used to replace <tex2html_verbatim_mark>#math458#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18955#</SPAN> 
in Eq. (<A HREF=<tex2html_cr_mark>#OptimalDelta1#1386><tex2html_cr_mark></A>) abve:
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math459#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18957#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">66</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
Solving for <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18959#</SPAN> we get the estimated optimal step size:
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math460#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18961#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">67</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
For example, in the gradient descent method, the search 
direction is <tex2html_verbatim_mark>#math461#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18963#</SPAN>, and the optimal step size
<tex2html_verbatim_mark>#math462#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18965#</SPAN> depends on
<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18967#</SPAN>, which is not assumed to be available. Therefore the 
secant method can be used to approximate the optimal step size:  
<P></P>
<DIV CLASS="mathdisplay"><A ID="GradientStepSizeSecant"><tex2html_anchor_mark></A><tex2html_verbatim_mark>#math463#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18969#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">68</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
The iteration becomes
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math464#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18971#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">69</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>

<P>
<#1457#><B>Example: </B><#1457#> The gradient descent method applied to solve the same
three-variable equation system previously solved by Newton's method:
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math465#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18973#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
;SPMnbsp;;SPMnbsp;;SPMnbsp;</TD></TR>
</TABLE></DIV>
<P></P>

<P>
The step size <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18975#</SPAN> is determined by the secant method with 
<tex2html_verbatim_mark>#math466#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18977#</SPAN>. The iteration from an initial guess <tex2html_verbatim_mark>#math467#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18979#</SPAN> 
is shown below: 
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math468#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#displaymath18981#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
;SPMnbsp;;SPMnbsp;;SPMnbsp;</TD></TR>
</TABLE></DIV>
<P></P>
We see that after the first 100 iterations the error is reduced
to about <tex2html_verbatim_mark>#math469#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18983#</SPAN>, and
With 500 additional iterations the algorithm converges to the 
following approximated solution with accuracy of 
<tex2html_verbatim_mark>#math470#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18985#</SPAN>:
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math471#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18987#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">70</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
Although the gradient descent method requires many more iterations
than Newton's method to converge, the computational cost in each 
iteration is much reduced, as no more matrix inversion is needed.

<P>
When it is difficult or too computationally costly to find the 
optimal step size along the search direction, some suboptimal 
step size may be acceptable, such as in the 
<A ID="tex2html9"
  HREF="node8.html"><EM>quasi-Newton methods</EM></A>
for minimization problems. In this case, although the step size 
<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18989#</SPAN> is no longer required to be such that the function value 
at the new position <tex2html_verbatim_mark>#math472#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18991#</SPAN> 
is minimized along the search direction <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18993#</SPAN>, the step size 
<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline18995#</SPAN> still has to satisfy the following
<A ID="tex2html10"
  HREF="https://en.wikipedia.org/wiki/Wolfe_conditions"><EM>Wolfe conditions</EM></A>:

<UL>
<LI><#1496#><EM>Sufficient decrease (Armijo rule):</EM><#1496#>
  <P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math473#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18997#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">71</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
</LI>
<LI><#1506#><EM>Curvature condition:</EM><#1506#>
  <P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math474#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay18999#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">72</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
  As <tex2html_verbatim_mark>#math475#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19001#</SPAN>, this condition can also be written 
  in the following alternative form:
  <P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math476#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay19003#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">73</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
</LI>
</UL>
Here the two constants <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19005#</SPAN> and <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19007#</SPAN> above satisfy <tex2html_verbatim_mark>#math477#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19009#</SPAN>. 

<P>
In general, these conditions are motivated by the desired effect 
that after each iterative step, the function should have a shallower 
slope along <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19011#</SPAN>, as well as a lower value, so that eventually 
the solution can be approached where <tex2html_verbatim_mark>#math478#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19013#</SPAN> is minimum and 
the gradient is zero.

<P>
Specifically, to understand the first condition above, we represent
the function to be minimized as a single-variable function of the 
step size <tex2html_verbatim_mark>#math479#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19015#</SPAN>, and its 
tangent line at the point <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19017#</SPAN> as a linear function 
<tex2html_verbatim_mark>#math480#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19019#</SPAN>, where the intercept <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19021#</SPAN> can be found
as <tex2html_verbatim_mark>#math481#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19023#</SPAN>, and the
slope <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19025#</SPAN> can be found as the derivative of 
<tex2html_verbatim_mark>#math482#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19027#</SPAN> at <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19029#</SPAN>:
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math483#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay19031#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">74</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
which is required to be negative for the function value to be
reduced, <tex2html_verbatim_mark>#math484#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19033#</SPAN>. Now the
function of the tangent line can be written as
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math485#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay19035#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">75</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
Comparing this with a constant line <tex2html_verbatim_mark>#math486#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19037#</SPAN> of
slope zero, we see that any straight line between <tex2html_verbatim_mark>#math487#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19039#</SPAN>
and <tex2html_verbatim_mark>#math488#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19041#</SPAN> can be described by 
<tex2html_verbatim_mark>#math489#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19043#</SPAN> with 
<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19045#</SPAN>, with a slope <tex2html_verbatim_mark>#math490#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19047#</SPAN>.
The Armijo rule is to find any <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19049#</SPAN> that satisfies
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math491#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay19051#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">76</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
We see that the value of <tex2html_verbatim_mark>#math492#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19053#</SPAN> is guaranteed to be reduced.

<P>
The second condition requires that at the new position 
<tex2html_verbatim_mark>#math493#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19055#</SPAN> the slope of the gradient <tex2html_verbatim_mark>#math494#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19057#</SPAN> along 
the search direction <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19059#</SPAN> be sufficiently reduced to be 
less than a specified value (determined by <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19061#</SPAN>), in comparison 
to the slope at the old position <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19063#</SPAN>. 

<P>
<IMG STYLE="" SRC="figures/GradientDescent.png"
 ALT="GradientDescent.png">

<P>
The reason why <tex2html_verbatim_mark>#math495#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19065#</SPAN> can also be explained geometrically.
As shown in the figure the gradient vectors at various points along the direction 
of <tex2html_verbatim_mark>#math496#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19067#</SPAN> are shown by the blue arrows, and their projections onto the direction
represent the slopes of the function <tex2html_verbatim_mark>#math497#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19069#</SPAN>. 
Obviously at <tex2html_verbatim_mark>#math498#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19071#</SPAN> where <tex2html_verbatim_mark>#math499#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19073#</SPAN> reaches its minimum, its slope 
is zero. In other words, the projection of the gradient <tex2html_verbatim_mark>#math500#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19075#</SPAN> onto the 
direction of <tex2html_verbatim_mark>#math501#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19077#</SPAN> is zero, i.e., <tex2html_verbatim_mark>#math502#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19079#</SPAN> or
<tex2html_verbatim_mark>#math503#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19081#</SPAN>.

<P>
The gradient descent method gradually approaches a solution <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19083#</SPAN> of an N-D
minimization problem by moving from the initial guess <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19085#</SPAN> along a zigzag 
path composed of a set of segments with any two consecutive segments
perpendicular to each other. The number of steps depends greatly on the initial 
guess. As illustrated in the example below in an N=2 dimensional case, the best 
possible case is that the solution happens to be on the gradient direction of the
initial guess, which could be reached in a single step, while the worst possible 
case is that the gradient direction of the initial guess happens to be 45 degrees
off from the gradient direction of the optimal case, and it takes many zigzag 
steps to go around the optimal path to reach the solution. Many of the steps 
are in the same direction as some of the previous steps. 

<P>
To improve the performance of the gradient descent method we can include in
the iteration a momentum term representing the search direction previously
traversed:
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math504#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay19087#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
(<SPAN CLASS="arabic">77</SPAN>)</TD></TR>
</TABLE></DIV>
<P></P>
Now two consecutive search directions are no longer perpendicular to each 
other and the resulting search path is smoother than the zigzag path  without
the momentum term. The parameter <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19089#</SPAN> controls how much momentum is to 
be added.

<P>
Obviously it is most desirable not to repeat any of the previous directions 
traveled so that the solution can be reached in N steps, each in a unique 
direction in the N-D space. In other words, the subsequent steps are independent 
of each other, never interfering with the results achieved in the previous steps.
Such a method will be discussed in the next section.

<P>
<IMG STYLE="" SRC="figures/GradientDescent1.png"
 ALT="GradientDescent1.png">

<P>
<#1604#><B>Example:</B><#1604#> The Rosenbrock function 
<P></P>
<DIV CLASS="mathdisplay"><tex2html_verbatim_mark>#math505#<TABLE CLASS="equation" >
<TR>
<TD  style="text-align:center;"><SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_indisplay19091#<tex2html_image_mark>#tex2html_wrap_indisplay19092#</SPAN></TD>
<TD  CLASS="eqno" style="text-align:right">
;SPMnbsp;;SPMnbsp;;SPMnbsp;</TD></TR>
</TABLE></DIV>
<P></P>
is a two-variable non-convex function with a global minimum 
<tex2html_verbatim_mark>#math506#<SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19094#</SPAN> at the point <SPAN CLASS="MATH"><tex2html_image_mark>#tex2html_wrap_inline19096#</SPAN>, which is inside a long parabolic 
shaped valley as shown in the figure below. As the slope along the 
valley is very shallow, it is difficult for an algorithm to converge 
quickly to the minimum. For this reason, the Rosenbrock function is 
often used to test various minimization algorithms.

<P>
<IMG STYLE="" SRC="figures/Rosenbrock.png"
 ALT="Rosenbrock.png">

<P>
The figure below shows the search path of the gradient 
descent method based on the optimal step size given in Eq. 
(<A HREF=<tex2html_cr_mark>#GradientStepSizeOptimal#1609><tex2html_cr_mark></A>). We see that the search path 
is composed a long sequence of 90 degree turns between consecutive 
segments.

<P>
<IMG STYLE="" SRC="figures/RosenbrockGradient0.png"
 ALT="RosenbrockGradient0.png">

<P>
<IMG STYLE="" SRC="figures/RosenbrockGradient01.png"
 ALT="RosenbrockGradient01.png">

<P>
When the Newton's method is applied to this minimization problem,
it takes only four iterations for the algorithm to converge to the
minimum, as shown in the figure below:

<P>
<IMG STYLE="" SRC="figures/RosenbrockNewton.png"
 ALT="RosenbrockNewton.png">

<P>
